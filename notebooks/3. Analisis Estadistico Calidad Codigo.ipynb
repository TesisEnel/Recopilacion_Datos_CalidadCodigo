{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5cff22a",
   "metadata": {},
   "source": [
    "# An√°lisis Estad√≠stico - Evoluci√≥n de Calidad de C√≥digo en Estudiantes de Programaci√≥n Aplicada\n",
    "\n",
    "**Investigador:** Enel Almonte  \n",
    "**Instituci√≥n:** Universidad Cat√≥lica Nordestana (UCNE)  \n",
    "**Tesis:** \"An√°lisis de la Evoluci√≥n de la Calidad del C√≥digo en Estudiantes de Programaci√≥n Aplicada mediante M√©tricas de SonarCloud\"\n",
    "\n",
    "## Contexto de la Investigaci√≥n\n",
    "\n",
    "### Objetivo General\n",
    "Analizar la evoluci√≥n de la calidad del c√≥digo fuente desarrollado por estudiantes de Ingenier√≠a en Sistemas de la UCNE al comparar los proyectos finales de la asignatura Programaci√≥n Aplicada I y Programaci√≥n Aplicada II.\n",
    "\n",
    "### Pregunta de Investigaci√≥n\n",
    "¬øC√≥mo evoluciona la calidad del c√≥digo fuente producido por estudiantes de Ingenier√≠a en Sistemas de la UCNE al transitar de Programaci√≥n Aplicada I a Programaci√≥n Aplicada II?\n",
    "\n",
    "### Hip√≥tesis\n",
    "- **H‚ÇÄ**: No existe diferencia estad√≠sticamente significativa en las m√©tricas de calidad del c√≥digo entre AP1 y AP2\n",
    "- **H‚ÇÅ**: Existe mejora estad√≠sticamente significativa en las m√©tricas de calidad del c√≥digo de AP1 a AP2\n",
    "\n",
    "### Variables\n",
    "- **Variable Independiente**: Intervenci√≥n pedag√≥gica (taller \"C√≥digo Limpio\" en AP2)\n",
    "- **Variable Dependiente**: M√©tricas de calidad del c√≥digo de SonarCloud\n",
    "\n",
    "---\n",
    "\n",
    "## Metodolog√≠a Estad√≠stica\n",
    "- **Dise√±o**: Pre-test/Post-test pareado\n",
    "- **Nivel de significancia**: Œ± = 0.05\n",
    "- **Correcci√≥n**: FDR para comparaciones m√∫ltiples\n",
    "- **Tama√±o del efecto**: Cohen's d para interpretaci√≥n pr√°ctica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ffdb1",
   "metadata": {},
   "source": [
    "# 1. Data Loading and Setup\n",
    "## Importaci√≥n de Librer√≠as Necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ac97c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as importadas correctamente\n",
      "üìä Configuraci√≥n de visualizaciones lista\n"
     ]
    }
   ],
   "source": [
    "# Importar librer√≠as necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import shapiro, wilcoxon, ttest_rel, normaltest\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar visualizaciones\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")\n",
    "print(\"üìä Configuraci√≥n de visualizaciones lista\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf2078a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datos cargados exitosamente\n",
      "üìä Dimensiones del dataset: (60, 41)\n",
      "‚ùå Error al cargar los datos: 'estudiante_id'\n",
      "üîÑ Intentando cargar desde archivo local...\n",
      "‚úÖ Datos cargados desde archivo local\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos desde URL\n",
    "url = 'https://raw.githubusercontent.com/TesisEnel/Recopilacion_Datos_CalidadCodigo/main/data/Estudiantes_2023-2024_con_metricas_sonarcloud.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(url)\n",
    "    print(\"‚úÖ Datos cargados exitosamente\")\n",
    "    print(f\"üìä Dimensiones del dataset: {df.shape}\")\n",
    "    print(f\"üë• N√∫mero de estudiantes √∫nicos: {df['estudiante_id'].nunique()}\")\n",
    "    print(f\"üìö Asignaturas en el dataset: {df['asignatura'].unique()}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al cargar los datos: {e}\")\n",
    "    print(\"üîÑ Intentando cargar desde archivo local...\")\n",
    "    try:\n",
    "        df = pd.read_csv('../data/Estudiantes_2023-2024_con_metricas_sonarcloud.csv')\n",
    "        print(\"‚úÖ Datos cargados desde archivo local\")\n",
    "    except:\n",
    "        print(\"‚ùå No se pudieron cargar los datos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925ac1f7",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis (EDA)\n",
    "## An√°lisis Exploratorio de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c366427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ESTRUCTURA DEL DATASET ===\n",
      "Dimensiones: (60, 41)\n",
      "Columnas: ['Id', 'Semestre', 'Estudiante', 'Sexo', 'Email', 'Original_Repo_Ap1', 'Original_Repo_Ap2', 'Sonar_Ap1', 'Sonar_Ap2', 'Sonar_Repo_Ap1', 'Sonar_Repo_Ap2', 'bugs_AP1', 'vulnerabilities_AP1', 'security_hotspots_AP1', 'code_smells_AP1', 'technical_debt_AP1', 'sqale_rating_AP1', 'complexity_AP1', 'cognitive_complexity_AP1', 'coverage_AP1', 'comment_lines_density_AP1', 'duplicated_lines_density_AP1', 'ncloc_AP1', 'reliability_rating_AP1', 'security_rating_AP1', 'open_issues_AP1', 'bugs_AP2', 'vulnerabilities_AP2', 'security_hotspots_AP2', 'code_smells_AP2', 'technical_debt_AP2', 'sqale_rating_AP2', 'complexity_AP2', 'cognitive_complexity_AP2', 'coverage_AP2', 'comment_lines_density_AP2', 'duplicated_lines_density_AP2', 'ncloc_AP2', 'reliability_rating_AP2', 'security_rating_AP2', 'open_issues_AP2']\n",
      "\n",
      "=== INFORMACI√ìN B√ÅSICA ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60 entries, 0 to 59\n",
      "Data columns (total 41 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Id                            60 non-null     int64  \n",
      " 1   Semestre                      60 non-null     object \n",
      " 2   Estudiante                    60 non-null     object \n",
      " 3   Sexo                          60 non-null     int64  \n",
      " 4   Email                         59 non-null     object \n",
      " 5   Original_Repo_Ap1             60 non-null     object \n",
      " 6   Original_Repo_Ap2             60 non-null     object \n",
      " 7   Sonar_Ap1                     60 non-null     object \n",
      " 8   Sonar_Ap2                     60 non-null     object \n",
      " 9   Sonar_Repo_Ap1                60 non-null     object \n",
      " 10  Sonar_Repo_Ap2                60 non-null     object \n",
      " 11  bugs_AP1                      60 non-null     int64  \n",
      " 12  vulnerabilities_AP1           60 non-null     int64  \n",
      " 13  security_hotspots_AP1         60 non-null     int64  \n",
      " 14  code_smells_AP1               60 non-null     int64  \n",
      " 15  technical_debt_AP1            0 non-null      float64\n",
      " 16  sqale_rating_AP1              60 non-null     float64\n",
      " 17  complexity_AP1                60 non-null     int64  \n",
      " 18  cognitive_complexity_AP1      60 non-null     int64  \n",
      " 19  coverage_AP1                  13 non-null     float64\n",
      " 20  comment_lines_density_AP1     60 non-null     float64\n",
      " 21  duplicated_lines_density_AP1  60 non-null     float64\n",
      " 22  ncloc_AP1                     60 non-null     int64  \n",
      " 23  reliability_rating_AP1        60 non-null     float64\n",
      " 24  security_rating_AP1           60 non-null     float64\n",
      " 25  open_issues_AP1               60 non-null     int64  \n",
      " 26  bugs_AP2                      60 non-null     int64  \n",
      " 27  vulnerabilities_AP2           60 non-null     int64  \n",
      " 28  security_hotspots_AP2         60 non-null     int64  \n",
      " 29  code_smells_AP2               60 non-null     int64  \n",
      " 30  technical_debt_AP2            0 non-null      float64\n",
      " 31  sqale_rating_AP2              60 non-null     float64\n",
      " 32  complexity_AP2                60 non-null     int64  \n",
      " 33  cognitive_complexity_AP2      60 non-null     int64  \n",
      " 34  coverage_AP2                  1 non-null      float64\n",
      " 35  comment_lines_density_AP2     60 non-null     float64\n",
      " 36  duplicated_lines_density_AP2  60 non-null     float64\n",
      " 37  ncloc_AP2                     60 non-null     int64  \n",
      " 38  reliability_rating_AP2        60 non-null     float64\n",
      " 39  security_rating_AP2           60 non-null     float64\n",
      " 40  open_issues_AP2               60 non-null     int64  \n",
      "dtypes: float64(14), int64(18), object(9)\n",
      "memory usage: 19.3+ KB\n",
      "None\n",
      "\n",
      "=== PRIMERAS FILAS ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Semestre</th>\n",
       "      <th>Estudiante</th>\n",
       "      <th>Sexo</th>\n",
       "      <th>Email</th>\n",
       "      <th>Original_Repo_Ap1</th>\n",
       "      <th>Original_Repo_Ap2</th>\n",
       "      <th>Sonar_Ap1</th>\n",
       "      <th>Sonar_Ap2</th>\n",
       "      <th>Sonar_Repo_Ap1</th>\n",
       "      <th>...</th>\n",
       "      <th>sqale_rating_AP2</th>\n",
       "      <th>complexity_AP2</th>\n",
       "      <th>cognitive_complexity_AP2</th>\n",
       "      <th>coverage_AP2</th>\n",
       "      <th>comment_lines_density_AP2</th>\n",
       "      <th>duplicated_lines_density_AP2</th>\n",
       "      <th>ncloc_AP2</th>\n",
       "      <th>reliability_rating_AP2</th>\n",
       "      <th>security_rating_AP2</th>\n",
       "      <th>open_issues_AP2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01</td>\n",
       "      <td>Aaron Eliezer Hern√°ndez Garc√≠a</td>\n",
       "      <td>1</td>\n",
       "      <td>atminifg655@gmail.com</td>\n",
       "      <td>https://github.com/aaron-developer25/SwiftPay.git</td>\n",
       "      <td>https://github.com/aaron-developer25/DealerPOS...</td>\n",
       "      <td>TesisEnel_SwiftPay-Aaron-Ap1</td>\n",
       "      <td>TesisEnel_DealerPOS-Aaron-ap2</td>\n",
       "      <td>https://github.com/TesisEnel/SwiftPay-Aaron-Ap1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1029</td>\n",
       "      <td>1262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>14674</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-03</td>\n",
       "      <td>Abraham El Hage Jreij</td>\n",
       "      <td>1</td>\n",
       "      <td>abrahamelhage2003@gmail.com</td>\n",
       "      <td>https://github.com/JPichardo2003/AguaMariaSolu...</td>\n",
       "      <td>https://github.com/A-EHJ/Final_Project_Ap2.git</td>\n",
       "      <td>TesisEnel_AguaMariaSolution-JulioPichardo-ap1</td>\n",
       "      <td>TesisEnel_Final_Project-Abraham-ap2</td>\n",
       "      <td>https://github.com/TesisEnel/AguaMariaSolution...</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>272</td>\n",
       "      <td>193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.2</td>\n",
       "      <td>9.2</td>\n",
       "      <td>4075</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2024-02</td>\n",
       "      <td>Adiel Luis Garc√≠a Rosa</td>\n",
       "      <td>1</td>\n",
       "      <td>adiel.garcia0422@gmail.com</td>\n",
       "      <td>https://github.com/SamyJp23/PeakPerformance.git</td>\n",
       "      <td>https://github.com/Adiel040/GymProApp.git</td>\n",
       "      <td>TesisEnel_PeakPerformance-samuelAntonio-ap1</td>\n",
       "      <td>TesisEnel_GymProApp-AdielGarcia-Ap2</td>\n",
       "      <td>https://github.com/TesisEnel/PeakPerformance-s...</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>375</td>\n",
       "      <td>546</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5736</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2024-03</td>\n",
       "      <td>Alaina Garcia Salazar</td>\n",
       "      <td>2</td>\n",
       "      <td>garciaalaina01@gmail.com</td>\n",
       "      <td>https://github.com/Reyx38/ReyAI_Transport.git</td>\n",
       "      <td>https://github.com/JeronyCruz/RecreArte.git</td>\n",
       "      <td>TesisEnel_ReyAI_Transport-Reyfil-Ap1</td>\n",
       "      <td>TesisEnel_RecreArte-JeronyCruz-Ap2</td>\n",
       "      <td>https://github.com/TesisEnel/ReyAI_Transport-R...</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>900</td>\n",
       "      <td>1038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>12.3</td>\n",
       "      <td>11268</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>Albert Luis Delgado Maria</td>\n",
       "      <td>1</td>\n",
       "      <td>bolshoi19booze@gmail.com</td>\n",
       "      <td>https://github.com/Rhazerpk/ProyectoFinal-AP1</td>\n",
       "      <td>https://github.com/Rhazerpk/MoonlightBarApp</td>\n",
       "      <td>TesisEnel_ProyectoFinal-AlbertRegalado-ap1</td>\n",
       "      <td>TesisEnel_MoonlightBarApp-AlbetDelgado-ap2</td>\n",
       "      <td>https://github.com/TesisEnel/ProyectoFinal-Alb...</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99</td>\n",
       "      <td>172</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.5</td>\n",
       "      <td>10.7</td>\n",
       "      <td>2781</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id Semestre                      Estudiante  Sexo  \\\n",
       "0   1  2024-01  Aaron Eliezer Hern√°ndez Garc√≠a     1   \n",
       "1   2  2023-03           Abraham El Hage Jreij     1   \n",
       "2   3  2024-02          Adiel Luis Garc√≠a Rosa     1   \n",
       "3   4  2024-03           Alaina Garcia Salazar     2   \n",
       "4   5  2023-01       Albert Luis Delgado Maria     1   \n",
       "\n",
       "                         Email  \\\n",
       "0        atminifg655@gmail.com   \n",
       "1  abrahamelhage2003@gmail.com   \n",
       "2   adiel.garcia0422@gmail.com   \n",
       "3     garciaalaina01@gmail.com   \n",
       "4     bolshoi19booze@gmail.com   \n",
       "\n",
       "                                   Original_Repo_Ap1  \\\n",
       "0  https://github.com/aaron-developer25/SwiftPay.git   \n",
       "1  https://github.com/JPichardo2003/AguaMariaSolu...   \n",
       "2    https://github.com/SamyJp23/PeakPerformance.git   \n",
       "3      https://github.com/Reyx38/ReyAI_Transport.git   \n",
       "4      https://github.com/Rhazerpk/ProyectoFinal-AP1   \n",
       "\n",
       "                                   Original_Repo_Ap2  \\\n",
       "0  https://github.com/aaron-developer25/DealerPOS...   \n",
       "1     https://github.com/A-EHJ/Final_Project_Ap2.git   \n",
       "2          https://github.com/Adiel040/GymProApp.git   \n",
       "3        https://github.com/JeronyCruz/RecreArte.git   \n",
       "4        https://github.com/Rhazerpk/MoonlightBarApp   \n",
       "\n",
       "                                       Sonar_Ap1  \\\n",
       "0                   TesisEnel_SwiftPay-Aaron-Ap1   \n",
       "1  TesisEnel_AguaMariaSolution-JulioPichardo-ap1   \n",
       "2    TesisEnel_PeakPerformance-samuelAntonio-ap1   \n",
       "3           TesisEnel_ReyAI_Transport-Reyfil-Ap1   \n",
       "4     TesisEnel_ProyectoFinal-AlbertRegalado-ap1   \n",
       "\n",
       "                                    Sonar_Ap2  \\\n",
       "0               TesisEnel_DealerPOS-Aaron-ap2   \n",
       "1         TesisEnel_Final_Project-Abraham-ap2   \n",
       "2         TesisEnel_GymProApp-AdielGarcia-Ap2   \n",
       "3          TesisEnel_RecreArte-JeronyCruz-Ap2   \n",
       "4  TesisEnel_MoonlightBarApp-AlbetDelgado-ap2   \n",
       "\n",
       "                                      Sonar_Repo_Ap1  ... sqale_rating_AP2  \\\n",
       "0    https://github.com/TesisEnel/SwiftPay-Aaron-Ap1  ...              1.0   \n",
       "1  https://github.com/TesisEnel/AguaMariaSolution...  ...              1.0   \n",
       "2  https://github.com/TesisEnel/PeakPerformance-s...  ...              1.0   \n",
       "3  https://github.com/TesisEnel/ReyAI_Transport-R...  ...              1.0   \n",
       "4  https://github.com/TesisEnel/ProyectoFinal-Alb...  ...              1.0   \n",
       "\n",
       "   complexity_AP2  cognitive_complexity_AP2  coverage_AP2  \\\n",
       "0            1029                      1262           NaN   \n",
       "1             272                       193           NaN   \n",
       "2             375                       546           NaN   \n",
       "3             900                      1038           NaN   \n",
       "4              99                       172           NaN   \n",
       "\n",
       "   comment_lines_density_AP2  duplicated_lines_density_AP2  ncloc_AP2  \\\n",
       "0                        1.0                           3.9      14674   \n",
       "1                        1.2                           9.2       4075   \n",
       "2                        1.5                           0.8       5736   \n",
       "3                        2.8                          12.3      11268   \n",
       "4                        1.5                          10.7       2781   \n",
       "\n",
       "   reliability_rating_AP2  security_rating_AP2  open_issues_AP2  \n",
       "0                     3.0                  3.0              241  \n",
       "1                     1.0                  1.0               37  \n",
       "2                     1.0                  1.0               40  \n",
       "3                     1.0                  1.0              116  \n",
       "4                     1.0                  1.0               28  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Estructura b√°sica del dataset\n",
    "print(\"=== ESTRUCTURA DEL DATASET ===\")\n",
    "print(f\"Dimensiones: {df.shape}\")\n",
    "print(f\"Columnas: {list(df.columns)}\")\n",
    "print(\"\\n=== INFORMACI√ìN B√ÅSICA ===\")\n",
    "print(df.info())\n",
    "print(\"\\n=== PRIMERAS FILAS ===\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e13aa788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== M√âTRICAS PRINCIPALES POR DIMENSI√ìN ===\n",
      "\n",
      "üîç Mantenibilidad:\n",
      "  ‚úÖ technical_debt: AP1=True, AP2=True\n",
      "  ‚úÖ code_smells: AP1=True, AP2=True\n",
      "  ‚úÖ duplicated_lines_density: AP1=True, AP2=True\n",
      "  ‚úÖ cognitive_complexity: AP1=True, AP2=True\n",
      "\n",
      "üîç Fiabilidad:\n",
      "  ‚úÖ bugs: AP1=True, AP2=True\n",
      "  ‚úÖ reliability_rating: AP1=True, AP2=True\n",
      "\n",
      "üîç Seguridad:\n",
      "  ‚úÖ vulnerabilities: AP1=True, AP2=True\n",
      "  ‚úÖ security_rating: AP1=True, AP2=True\n",
      "\n",
      "üìä Total de m√©tricas base definidas: 14\n",
      "üìä M√©tricas AP1 disponibles: 14\n",
      "üìä M√©tricas AP2 disponibles: 14\n",
      "\n",
      "=== COLUMNAS CLAVE ===\n",
      "‚úÖ Id\n",
      "‚úÖ Estudiante\n",
      "‚úÖ Semestre\n",
      "‚úÖ Sexo\n",
      "\n",
      "üë• Utilizando 'Id' como identificador de estudiante\n"
     ]
    }
   ],
   "source": [
    "# Definir m√©tricas principales por dimensi√≥n de calidad\n",
    "# Las m√©tricas est√°n separadas por AP1 y AP2 en el dataset\n",
    "base_metrics = {\n",
    "    'Mantenibilidad': ['technical_debt', 'code_smells', 'duplicated_lines_density', 'cognitive_complexity'],\n",
    "    'Fiabilidad': ['bugs', 'reliability_rating'],\n",
    "    'Seguridad': ['vulnerabilities', 'security_rating']\n",
    "}\n",
    "\n",
    "# M√©tricas complementarias base\n",
    "base_complementary = ['complexity', 'comment_lines_density', 'ncloc', 'security_hotspots', 'sqale_rating', 'open_issues']\n",
    "\n",
    "# Crear listas completas con sufijos AP1 y AP2\n",
    "quality_dimensions_ap1 = {}\n",
    "quality_dimensions_ap2 = {}\n",
    "\n",
    "for dimension, metrics in base_metrics.items():\n",
    "    quality_dimensions_ap1[dimension] = [f\"{metric}_AP1\" for metric in metrics]\n",
    "    quality_dimensions_ap2[dimension] = [f\"{metric}_AP2\" for metric in metrics]\n",
    "\n",
    "complementary_metrics_ap1 = [f\"{metric}_AP1\" for metric in base_complementary]\n",
    "complementary_metrics_ap2 = [f\"{metric}_AP2\" for metric in base_complementary]\n",
    "\n",
    "# Todas las m√©tricas de SonarCloud (AP1 y AP2)\n",
    "all_metrics_ap1 = []\n",
    "all_metrics_ap2 = []\n",
    "\n",
    "for metrics_list in quality_dimensions_ap1.values():\n",
    "    all_metrics_ap1.extend(metrics_list)\n",
    "all_metrics_ap1.extend(complementary_metrics_ap1)\n",
    "\n",
    "for metrics_list in quality_dimensions_ap2.values():\n",
    "    all_metrics_ap2.extend(metrics_list)\n",
    "all_metrics_ap2.extend(complementary_metrics_ap2)\n",
    "\n",
    "# M√©tricas base (sin sufijo) para an√°lisis\n",
    "base_all_metrics = []\n",
    "for metrics_list in base_metrics.values():\n",
    "    base_all_metrics.extend(metrics_list)\n",
    "base_all_metrics.extend(base_complementary)\n",
    "\n",
    "print(\"=== M√âTRICAS PRINCIPALES POR DIMENSI√ìN ===\")\n",
    "for dimension, metrics in base_metrics.items():\n",
    "    print(f\"\\nüîç {dimension}:\")\n",
    "    for metric in metrics:\n",
    "        ap1_metric = f\"{metric}_AP1\"\n",
    "        ap2_metric = f\"{metric}_AP2\"\n",
    "        ap1_exists = ap1_metric in df.columns\n",
    "        ap2_exists = ap2_metric in df.columns\n",
    "        print(f\"  {'‚úÖ' if ap1_exists and ap2_exists else '‚ùå'} {metric}: AP1={ap1_exists}, AP2={ap2_exists}\")\n",
    "\n",
    "print(f\"\\nüìä Total de m√©tricas base definidas: {len(base_all_metrics)}\")\n",
    "print(f\"üìä M√©tricas AP1 disponibles: {len([m for m in all_metrics_ap1 if m in df.columns])}\")\n",
    "print(f\"üìä M√©tricas AP2 disponibles: {len([m for m in all_metrics_ap2 if m in df.columns])}\")\n",
    "\n",
    "# Verificar que tenemos las columnas necesarias para an√°lisis pareado\n",
    "print(f\"\\n=== COLUMNAS CLAVE ===\")\n",
    "key_columns = ['Id', 'Estudiante', 'Semestre', 'Sexo']\n",
    "for col in key_columns:\n",
    "    print(f\"{'‚úÖ' if col in df.columns else '‚ùå'} {col}\")\n",
    "\n",
    "# Identificar estudiante como clave primaria\n",
    "if 'Id' in df.columns:\n",
    "    print(f\"\\nüë• Utilizando 'Id' como identificador de estudiante\")\n",
    "    student_id_col = 'Id'\n",
    "elif 'Estudiante' in df.columns:\n",
    "    print(f\"\\nüë• Utilizando 'Estudiante' como identificador de estudiante\")\n",
    "    student_id_col = 'Estudiante'\n",
    "else:\n",
    "    print(f\"\\n‚ùå No se encontr√≥ columna de identificaci√≥n de estudiante\")\n",
    "    student_id_col = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad√≠sticas descriptivas por asignatura\n",
    "print(\"=== ESTAD√çSTICAS DESCRIPTIVAS POR ASIGNATURA ===\")\n",
    "\n",
    "# Filtrar m√©tricas disponibles para cada asignatura\n",
    "available_metrics_ap1 = [m for m in all_metrics_ap1 if m in df.columns]\n",
    "available_metrics_ap2 = [m for m in all_metrics_ap2 if m in df.columns]\n",
    "\n",
    "print(f\"\\nüìö ASIGNATURA AP1:\")\n",
    "print(f\"üë• N√∫mero de estudiantes: {len(df)}\")\n",
    "print(f\"üìä M√©tricas disponibles: {len(available_metrics_ap1)}\")\n",
    "\n",
    "if available_metrics_ap1:\n",
    "    # Estad√≠sticas de las m√©tricas AP1\n",
    "    stats_ap1 = df[available_metrics_ap1].describe()\n",
    "    print(\"\\nüìà Estad√≠sticas descriptivas AP1:\")\n",
    "    display(stats_ap1.round(3))\n",
    "\n",
    "print(f\"\\n\udcda ASIGNATURA AP2:\")\n",
    "print(f\"\ud83düë• N√∫mero de estudiantes: {len(df)}\")\n",
    "print(f\"üìä M√©tricas disponibles: {len(available_metrics_ap2)}\")\n",
    "\n",
    "if available_metrics_ap2:\n",
    "    # Estad√≠sticas de las m√©tricas AP2\n",
    "    stats_ap2 = df[available_metrics_ap2].describe()\n",
    "    print(\"\\nüìà Estad√≠sticas descriptivas AP2:\")\n",
    "    display(stats_ap2.round(3))\n",
    "\n",
    "# Comparaci√≥n directa AP1 vs AP2 para m√©tricas base\n",
    "print(\"\\n=== COMPARACI√ìN DIRECTA AP1 vs AP2 ===\")\n",
    "\n",
    "comparison_data = []\n",
    "for metric in base_all_metrics:\n",
    "    ap1_col = f\"{metric}_AP1\"\n",
    "    ap2_col = f\"{metric}_AP2\"\n",
    "    \n",
    "    if ap1_col in df.columns and ap2_col in df.columns:\n",
    "        comparison_data.append({\n",
    "            'M√©trica': metric,\n",
    "            'AP1_Media': df[ap1_col].mean(),\n",
    "            'AP1_Mediana': df[ap1_col].median(),\n",
    "            'AP1_Std': df[ap1_col].std(),\n",
    "            'AP2_Media': df[ap2_col].mean(),\n",
    "            'AP2_Mediana': df[ap2_col].median(),\n",
    "            'AP2_Std': df[ap2_col].std(),\n",
    "            'Diferencia_Media': df[ap2_col].mean() - df[ap1_col].mean(),\n",
    "            'Cambio_Porcentual': ((df[ap2_col].mean() - df[ap1_col].mean()) / df[ap1_col].mean() * 100) if df[ap1_col].mean() != 0 else 0\n",
    "        })\n",
    "\n",
    "if comparison_data:\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.round(3)\n",
    "    display(comparison_df)\n",
    "    \n",
    "    print(f\"\\nüìä M√©tricas comparables encontradas: {len(comparison_data)}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No se encontraron m√©tricas comparables entre AP1 y AP2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4344db5c",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing and Validation\n",
    "## Preprocesamiento y Validaci√≥n de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d95c559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validar datos pareados\n",
    "print(\"=== VALIDACI√ìN DE DATOS PAREADOS ===\")\n",
    "\n",
    "if student_id_col:\n",
    "    # En este formato, cada fila ya contiene datos de AP1 y AP2 para el mismo estudiante\n",
    "    total_students = len(df)\n",
    "    print(f\"üë• Total de estudiantes en el dataset: {total_students}\")\n",
    "    \n",
    "    # Verificar que tenemos datos para ambas asignaturas\n",
    "    students_with_ap1 = df[available_metrics_ap1].notna().any(axis=1).sum()\n",
    "    students_with_ap2 = df[available_metrics_ap2].notna().any(axis=1).sum()\n",
    "    \n",
    "    print(f\"üë• Estudiantes con datos en AP1: {students_with_ap1}\")\n",
    "    print(f\"üë• Estudiantes con datos en AP2: {students_with_ap2}\")\n",
    "    \n",
    "    # Estudiantes con datos en ambas asignaturas\n",
    "    has_ap1_data = df[available_metrics_ap1].notna().any(axis=1)\n",
    "    has_ap2_data = df[available_metrics_ap2].notna().any(axis=1)\n",
    "    paired_mask = has_ap1_data & has_ap2_data\n",
    "    \n",
    "    paired_students_count = paired_mask.sum()\n",
    "    print(f\"üîó Estudiantes con datos pareados (AP1 y AP2): {paired_students_count}\")\n",
    "    \n",
    "    # Filtrar datos para an√°lisis pareado\n",
    "    df_paired = df[paired_mask].copy()\n",
    "    print(f\"\\nüìä Dataset pareado final: {df_paired.shape}\")\n",
    "    print(f\"‚úÖ Datos listos para an√°lisis pre-post intervenci√≥n\")\n",
    "    \n",
    "    # Informaci√≥n adicional del dataset\n",
    "    if 'Sexo' in df_paired.columns:\n",
    "        print(f\"\\nüë• Distribuci√≥n por g√©nero:\")\n",
    "        gender_dist = df_paired['Sexo'].value_counts()\n",
    "        display(gender_dist)\n",
    "    \n",
    "    if 'Semestre' in df_paired.columns:\n",
    "        print(f\"\\n\udcc5 Distribuci√≥n por semestre:\")\n",
    "        semester_dist = df_paired['Semestre'].value_counts()\n",
    "        display(semester_dist)\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No se puede realizar validaci√≥n sin columna de identificaci√≥n de estudiante\")\n",
    "    df_paired = df.copy()  # Usar todo el dataset como fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755af315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de valores faltantes y outliers\n",
    "print(\"=== AN√ÅLISIS DE VALORES FALTANTES ===\")\n",
    "\n",
    "# Analizar valores faltantes para AP1 y AP2 por separado\n",
    "all_available_metrics = available_metrics_ap1 + available_metrics_ap2\n",
    "\n",
    "missing_data = df_paired[all_available_metrics].isnull().sum()\n",
    "missing_percentage = (missing_data / len(df_paired)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Missing Percentage': missing_percentage.round(2)\n",
    "})\n",
    "\n",
    "# Separar por asignatura para mejor visualizaci√≥n\n",
    "missing_ap1 = missing_df[missing_df.index.str.contains('_AP1')]\n",
    "missing_ap2 = missing_df[missing_df.index.str.contains('_AP2')]\n",
    "\n",
    "print(\"Valores faltantes AP1:\")\n",
    "display(missing_ap1[missing_ap1['Missing Count'] > 0])\n",
    "\n",
    "print(\"Valores faltantes AP2:\")\n",
    "display(missing_ap2[missing_ap2['Missing Count'] > 0])\n",
    "\n",
    "if missing_df['Missing Count'].sum() == 0:\n",
    "    print(\"‚úÖ No hay valores faltantes en las m√©tricas principales\")\n",
    "\n",
    "# Detecci√≥n de outliers usando IQR\n",
    "print(\"\\n=== DETECCI√ìN DE OUTLIERS (IQR) ===\")\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return len(outliers), lower_bound, upper_bound\n",
    "\n",
    "outlier_summary = []\n",
    "for metric in all_available_metrics:\n",
    "    if df_paired[metric].dtype in ['int64', 'float64']:\n",
    "        n_outliers, lower, upper = detect_outliers_iqr(df_paired, metric)\n",
    "        outlier_summary.append({\n",
    "            'Metric': metric,\n",
    "            'Outliers': n_outliers,\n",
    "            'Lower Bound': lower,\n",
    "            'Upper Bound': upper,\n",
    "            'Outlier %': (n_outliers / len(df_paired)) * 100\n",
    "        })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "print(f\"üìä Resumen de outliers (primeras 10 m√©tricas):\")\n",
    "display(outlier_df.head(10).round(3))\n",
    "\n",
    "# Resumen general de outliers\n",
    "high_outlier_metrics = outlier_df[outlier_df['Outlier %'] > 10]\n",
    "if len(high_outlier_metrics) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è M√©tricas con alto porcentaje de outliers (>10%):\")\n",
    "    display(high_outlier_metrics[['Metric', 'Outlier %']].round(1))\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No hay m√©tricas con porcentajes extremos de outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c74428",
   "metadata": {},
   "source": [
    "# 4. Normality Testing and Test Selection\n",
    "## Pruebas de Normalidad y Selecci√≥n de Tests Estad√≠sticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_metric_evolution(df, base_metric, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Analiza la evoluci√≥n de una m√©trica entre AP1 y AP2\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame con datos pareados (cada fila contiene AP1 y AP2 del mismo estudiante)\n",
    "    base_metric: nombre base de la m√©trica (sin sufijo _AP1 o _AP2)\n",
    "    alpha: nivel de significancia (default 0.05)\n",
    "    \n",
    "    Returns:\n",
    "    dict: resultados del an√°lisis estad√≠stico\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construir nombres de columnas\n",
    "    ap1_col = f\"{base_metric}_AP1\"\n",
    "    ap2_col = f\"{base_metric}_AP2\"\n",
    "    \n",
    "    # Verificar que las columnas existan\n",
    "    if ap1_col not in df.columns or ap2_col not in df.columns:\n",
    "        return {'error': f'Columnas no encontradas: {ap1_col} o {ap2_col}'}\n",
    "    \n",
    "    # Extraer datos, eliminando valores nulos\n",
    "    valid_mask = df[ap1_col].notna() & df[ap2_col].notna()\n",
    "    ap1_data = df.loc[valid_mask, ap1_col]\n",
    "    ap2_data = df.loc[valid_mask, ap2_col]\n",
    "    \n",
    "    # Verificar que tenemos datos suficientes\n",
    "    if len(ap1_data) < 3:\n",
    "        return {'error': f'Datos insuficientes: solo {len(ap1_data)} pares v√°lidos'}\n",
    "    \n",
    "    # Calcular diferencias (AP2 - AP1)\n",
    "    differences = ap2_data.values - ap1_data.values\n",
    "    \n",
    "    # Prueba de normalidad de las diferencias (Shapiro-Wilk)\n",
    "    try:\n",
    "        _, p_norm = shapiro(differences)\n",
    "    except:\n",
    "        p_norm = 0.01  # Asumir no normal si hay error\n",
    "    \n",
    "    # Seleccionar test apropiado basado en normalidad\n",
    "    if p_norm > alpha:\n",
    "        # Datos normales: T-test pareado\n",
    "        try:\n",
    "            stat, p_val = ttest_rel(ap1_data, ap2_data)\n",
    "            test_used = 'T-test pareado'\n",
    "        except:\n",
    "            stat, p_val = wilcoxon(ap1_data, ap2_data, zero_method='zsplit')\n",
    "            test_used = 'Wilcoxon signed-rank (fallback)'\n",
    "    else:\n",
    "        # Datos no normales: Wilcoxon signed-rank\n",
    "        try:\n",
    "            stat, p_val = wilcoxon(ap1_data, ap2_data, zero_method='zsplit')\n",
    "            test_used = 'Wilcoxon signed-rank'\n",
    "        except:\n",
    "            stat, p_val = np.nan, np.nan\n",
    "            test_used = 'Error en test'\n",
    "    \n",
    "    # Calcular tama√±o del efecto (Cohen's d)\n",
    "    pooled_std = np.sqrt(((ap1_data.std()**2 + ap2_data.std()**2) / 2))\n",
    "    if pooled_std != 0:\n",
    "        cohen_d = (ap2_data.mean() - ap1_data.mean()) / pooled_std\n",
    "    else:\n",
    "        cohen_d = 0\n",
    "    \n",
    "    # Interpretar tama√±o del efecto\n",
    "    if abs(cohen_d) < 0.2:\n",
    "        effect_size = 'Peque√±o'\n",
    "    elif abs(cohen_d) < 0.5:\n",
    "        effect_size = 'Mediano'\n",
    "    elif abs(cohen_d) < 0.8:\n",
    "        effect_size = 'Grande'\n",
    "    else:\n",
    "        effect_size = 'Muy grande'\n",
    "    \n",
    "    return {\n",
    "        'metric': base_metric,\n",
    "        'test': test_used,\n",
    "        'statistic': stat,\n",
    "        'p_value': p_val,\n",
    "        'p_normality': p_norm,\n",
    "        'is_significant': p_val < alpha if not np.isnan(p_val) else False,\n",
    "        'cohen_d': cohen_d,\n",
    "        'effect_size': effect_size,\n",
    "        'ap1_mean': ap1_data.mean(),\n",
    "        'ap1_std': ap1_data.std(),\n",
    "        'ap2_mean': ap2_data.mean(),\n",
    "        'ap2_std': ap2_data.std(),\n",
    "        'improvement': ap2_data.mean() - ap1_data.mean(),\n",
    "        'improvement_pct': ((ap2_data.mean() - ap1_data.mean()) / ap1_data.mean() * 100) if ap1_data.mean() != 0 else 0,\n",
    "        'n_pairs': len(differences),\n",
    "        'ap1_col': ap1_col,\n",
    "        'ap2_col': ap2_col\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Funci√≥n analyze_metric_evolution actualizada para el nuevo formato\")\n",
    "print(\"üî¨ Lista para an√°lisis estad√≠stico de m√©tricas individuales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec2f7d",
   "metadata": {},
   "source": [
    "# 5. Paired Statistical Testing for Individual Metrics\n",
    "## An√°lisis Estad√≠stico Pareado para M√©tricas Individuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96962055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis estad√≠stico para todas las m√©tricas disponibles\n",
    "print(\"=== AN√ÅLISIS ESTAD√çSTICO INDIVIDUAL DE M√âTRICAS ===\")\n",
    "\n",
    "results_individual = []\n",
    "\n",
    "# Filtrar m√©tricas base que tienen datos en ambas asignaturas\n",
    "available_base_metrics = []\n",
    "for metric in base_all_metrics:\n",
    "    ap1_col = f\"{metric}_AP1\"\n",
    "    ap2_col = f\"{metric}_AP2\"\n",
    "    if ap1_col in df_paired.columns and ap2_col in df_paired.columns:\n",
    "        available_base_metrics.append(metric)\n",
    "\n",
    "print(f\"üìä M√©tricas base disponibles para an√°lisis: {len(available_base_metrics)}\")\n",
    "\n",
    "for metric in available_base_metrics:\n",
    "    print(f\"\\nüî¨ Analizando: {metric}\")\n",
    "    result = analyze_metric_evolution(df_paired, metric)\n",
    "    \n",
    "    if 'error' not in result:\n",
    "        results_individual.append(result)\n",
    "        \n",
    "        # Mostrar resultados resumidos\n",
    "        print(f\"   üìä Test usado: {result['test']}\")\n",
    "        print(f\"   üìà AP1 ‚Üí AP2: {result['ap1_mean']:.3f} ‚Üí {result['ap2_mean']:.3f}\")\n",
    "        print(f\"   üîÑ Cambio: {result['improvement']:.3f} ({result['improvement_pct']:.1f}%)\")\n",
    "        print(f\"   üéØ p-valor: {result['p_value']:.4f}\")\n",
    "        print(f\"   ‚ö° Cohen's d: {result['cohen_d']:.3f} ({result['effect_size']})\")\n",
    "        print(f\"   üë• Pares v√°lidos: {result['n_pairs']}\")\n",
    "        print(f\"   ‚úÖ Significativo: {'S√≠' if result['is_significant'] else 'No'}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Error: {result['error']}\")\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "if results_individual:\n",
    "    results_df = pd.DataFrame(results_individual)\n",
    "    print(f\"\\nüìä An√°lisis completado para {len(results_df)} m√©tricas\")\n",
    "    print(f\"‚úÖ M√©tricas con mejora significativa: {sum(results_df['is_significant'])}\")\n",
    "    \n",
    "    # Mostrar m√©tricas con mejoras m√°s significativas\n",
    "    if sum(results_df['is_significant']) > 0:\n",
    "        significant_results = results_df[results_df['is_significant']].sort_values('p_value')\n",
    "        print(f\"\\nüåü Top m√©tricas con mejoras significativas:\")\n",
    "        for _, row in significant_results.head(5).iterrows():\n",
    "            direction = \"‚¨áÔ∏è Mejora\" if row['improvement'] < 0 else \"‚¨ÜÔ∏è Incremento\"\n",
    "            print(f\"   ‚Ä¢ {row['metric']}: p={row['p_value']:.4f}, d={row['cohen_d']:.3f} {direction}\")\n",
    "else:\n",
    "    print(\"‚ùå No se pudieron analizar las m√©tricas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab39173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla resumen de resultados individuales\n",
    "if results_individual:\n",
    "    print(\"=== TABLA RESUMEN - AN√ÅLISIS INDIVIDUAL ===\")\n",
    "    \n",
    "    summary_table = results_df[['metric', 'test', 'ap1_mean', 'ap2_mean', 'improvement', \n",
    "                               'improvement_pct', 'p_value', 'cohen_d', 'effect_size', 'is_significant']].copy()\n",
    "    \n",
    "    # Redondear valores num√©ricos\n",
    "    summary_table['ap1_mean'] = summary_table['ap1_mean'].round(3)\n",
    "    summary_table['ap2_mean'] = summary_table['ap2_mean'].round(3)\n",
    "    summary_table['improvement'] = summary_table['improvement'].round(3)\n",
    "    summary_table['improvement_pct'] = summary_table['improvement_pct'].round(1)\n",
    "    summary_table['p_value'] = summary_table['p_value'].round(4)\n",
    "    summary_table['cohen_d'] = summary_table['cohen_d'].round(3)\n",
    "    \n",
    "    # Renombrar columnas para mejor presentaci√≥n\n",
    "    summary_table.columns = ['M√©trica', 'Test Estad√≠stico', 'Media AP1', 'Media AP2', \n",
    "                           'Cambio Absoluto', 'Cambio %', 'p-valor', 'Cohen\\'s d', \n",
    "                           'Tama√±o Efecto', 'Significativo']\n",
    "    \n",
    "    display(summary_table)\n",
    "    \n",
    "    # Estad√≠sticas generales\n",
    "    print(f\"\\n=== RESUMEN GENERAL ===\")\n",
    "    print(f\"üìä Total de m√©tricas analizadas: {len(results_df)}\")\n",
    "    print(f\"‚úÖ M√©tricas con mejora significativa (p < 0.05): {sum(results_df['is_significant'])}\")\n",
    "    print(f\"üìà M√©tricas con tama√±o de efecto grande (|d| > 0.8): {sum(abs(results_df['cohen_d']) > 0.8)}\")\n",
    "    print(f\"üìâ M√©tricas con mejora (cambio negativo en m√©tricas 'malas'): {sum(results_df['improvement'] < 0)}\")\n",
    "    print(f\"üî¨ Tests param√©tricos utilizados: {sum(results_df['test'].str.contains('T-test'))}\")\n",
    "    print(f\"üî¨ Tests no param√©tricos utilizados: {sum(results_df['test'].str.contains('Wilcoxon'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4bf68f",
   "metadata": {},
   "source": [
    "# 6. Quality Dimensions Analysis\n",
    "## An√°lisis por Dimensiones de Calidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7489a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis por dimensiones de calidad\n",
    "print(\"=== AN√ÅLISIS POR DIMENSIONES DE CALIDAD ===\")\n",
    "\n",
    "dimension_results = {}\n",
    "\n",
    "for dimension, base_metrics_list in base_metrics.items():\n",
    "    print(f\"\\nüèóÔ∏è DIMENSI√ìN: {dimension.upper()}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    dimension_data = []\n",
    "    available_dimension_metrics = [m for m in base_metrics_list if m in available_base_metrics]\n",
    "    \n",
    "    for metric in available_dimension_metrics:\n",
    "        # Buscar resultado del an√°lisis individual\n",
    "        metric_result = next((r for r in results_individual if r['metric'] == metric), None)\n",
    "        \n",
    "        if metric_result:\n",
    "            dimension_data.append(metric_result)\n",
    "            \n",
    "            print(f\"\\nüìä {metric}:\")\n",
    "            print(f\"   üìà Cambio: {metric_result['ap1_mean']:.3f} ‚Üí {metric_result['ap2_mean']:.3f}\")\n",
    "            print(f\"   üîÑ Mejora: {metric_result['improvement']:.3f} ({metric_result['improvement_pct']:.1f}%)\")\n",
    "            print(f\"   üéØ p-valor: {metric_result['p_value']:.4f}\")\n",
    "            print(f\"   ‚ö° Cohen's d: {metric_result['cohen_d']:.3f}\")\n",
    "            print(f\"   üë• Pares: {metric_result['n_pairs']}\")\n",
    "            print(f\"   ‚úÖ Significativo: {'S√≠' if metric_result['is_significant'] else 'No'}\")\n",
    "    \n",
    "    # Resumen por dimensi√≥n\n",
    "    if dimension_data:\n",
    "        significant_count = sum(1 for d in dimension_data if d['is_significant'])\n",
    "        avg_effect_size = np.mean([d['cohen_d'] for d in dimension_data])\n",
    "        avg_improvement = np.mean([d['improvement_pct'] for d in dimension_data])\n",
    "        \n",
    "        dimension_results[dimension] = {\n",
    "            'metrics_count': len(dimension_data),\n",
    "            'significant_count': significant_count,\n",
    "            'significant_pct': (significant_count / len(dimension_data)) * 100,\n",
    "            'avg_effect_size': avg_effect_size,\n",
    "            'avg_improvement_pct': avg_improvement,\n",
    "            'metrics_data': dimension_data\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìã RESUMEN {dimension}:\")\n",
    "        print(f\"   üìä M√©tricas analizadas: {len(dimension_data)}\")\n",
    "        print(f\"   ‚úÖ Mejoras significativas: {significant_count}/{len(dimension_data)} ({significant_count/len(dimension_data)*100:.1f}%)\")\n",
    "        print(f\"   ‚ö° Tama√±o de efecto promedio: {avg_effect_size:.3f}\")\n",
    "        print(f\"   üìà Cambio promedio: {avg_improvement:.1f}%\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå No hay m√©tricas disponibles para esta dimensi√≥n\")\n",
    "\n",
    "# Resumen general por dimensiones\n",
    "print(f\"\\n=== RESUMEN GENERAL POR DIMENSIONES ===\")\n",
    "for dimension, summary in dimension_results.items():\n",
    "    improvement_direction = \"‚¨áÔ∏è\" if summary['avg_improvement_pct'] < 0 else \"‚¨ÜÔ∏è\"\n",
    "    print(f\"üèóÔ∏è {dimension}: {summary['significant_count']}/{summary['metrics_count']} significativas ({summary['significant_pct']:.1f}%)\")\n",
    "    print(f\"   ‚ö° Efecto promedio: {summary['avg_effect_size']:.3f}\")\n",
    "    print(f\"   üìà Cambio promedio: {improvement_direction} {abs(summary['avg_improvement_pct']):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb52d2e",
   "metadata": {},
   "source": [
    "# 7. Multiple Comparisons Correction\n",
    "## Correcci√≥n por Comparaciones M√∫ltiples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e1f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcci√≥n por comparaciones m√∫ltiples usando FDR (Benjamini-Hochberg)\n",
    "print(\"=== CORRECCI√ìN POR COMPARACIONES M√öLTIPLES (FDR) ===\")\n",
    "\n",
    "if results_individual:\n",
    "    # Extraer p-valores\n",
    "    p_values = [r['p_value'] for r in results_individual]\n",
    "    metric_names = [r['metric'] for r in results_individual]\n",
    "    \n",
    "    # Aplicar correcci√≥n FDR (Benjamini-Hochberg)\n",
    "    rejected, corrected_p, alpha_sidak, alpha_bonf = multipletests(\n",
    "        p_values, alpha=0.05, method='fdr_bh'\n",
    "    )\n",
    "    \n",
    "    # Crear DataFrame con resultados corregidos\n",
    "    correction_results = pd.DataFrame({\n",
    "        'M√©trica': metric_names,\n",
    "        'p-valor_original': p_values,\n",
    "        'p-valor_corregido_FDR': corrected_p,\n",
    "        'Significativo_original': [p < 0.05 for p in p_values],\n",
    "        'Significativo_FDR': rejected,\n",
    "        'Cohen_d': [r['cohen_d'] for r in results_individual],\n",
    "        'Cambio_absoluto': [r['improvement'] for r in results_individual],\n",
    "        'Cambio_porcentual': [r['improvement_pct'] for r in results_individual]\n",
    "    })\n",
    "    \n",
    "    # Redondear valores\n",
    "    correction_results['p-valor_original'] = correction_results['p-valor_original'].round(4)\n",
    "    correction_results['p-valor_corregido_FDR'] = correction_results['p-valor_corregido_FDR'].round(4)\n",
    "    correction_results['Cohen_d'] = correction_results['Cohen_d'].round(3)\n",
    "    correction_results['Cambio_absoluto'] = correction_results['Cambio_absoluto'].round(3)\n",
    "    correction_results['Cambio_porcentual'] = correction_results['Cambio_porcentual'].round(1)\n",
    "    \n",
    "    # Ordenar por p-valor corregido\n",
    "    correction_results = correction_results.sort_values('p-valor_corregido_FDR')\n",
    "    \n",
    "    display(correction_results)\n",
    "    \n",
    "    # Resumen de correcci√≥n\n",
    "    original_significant = sum(correction_results['Significativo_original'])\n",
    "    fdr_significant = sum(correction_results['Significativo_FDR'])\n",
    "    \n",
    "    print(f\"\\n=== IMPACTO DE LA CORRECCI√ìN FDR ===\")\n",
    "    print(f\"‚úÖ Significativas sin correcci√≥n (Œ± = 0.05): {original_significant}/{len(p_values)} ({original_significant/len(p_values)*100:.1f}%)\")\n",
    "    print(f\"‚úÖ Significativas con correcci√≥n FDR: {fdr_significant}/{len(p_values)} ({fdr_significant/len(p_values)*100:.1f}%)\")\n",
    "    print(f\"üìâ Diferencia: {original_significant - fdr_significant} m√©tricas\")\n",
    "    \n",
    "    if fdr_significant > 0:\n",
    "        print(f\"\\nüéØ M√âTRICAS SIGNIFICATIVAS DESPU√âS DE CORRECCI√ìN FDR:\")\n",
    "        significant_fdr = correction_results[correction_results['Significativo_FDR']]\n",
    "        for _, row in significant_fdr.iterrows():\n",
    "            print(f\"   ‚úÖ {row['M√©trica']}: p-FDR = {row['p-valor_corregido_FDR']:.4f}, d = {row['Cohen_d']:.3f}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Ninguna m√©trica permanece significativa despu√©s de la correcci√≥n FDR\")\n",
    "else:\n",
    "    print(\"‚ùå No hay resultados para corregir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21f9809",
   "metadata": {},
   "source": [
    "# 8. Effect Size Calculations\n",
    "## An√°lisis de Tama√±o del Efecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b586b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis detallado del tama√±o del efecto\n",
    "print(\"=== AN√ÅLISIS DE TAMA√ëO DEL EFECTO (COHEN'S D) ===\")\n",
    "\n",
    "if results_individual:\n",
    "    # Crear DataFrame para an√°lisis de tama√±o del efecto\n",
    "    effect_size_df = pd.DataFrame({\n",
    "        'M√©trica': [r['metric'] for r in results_individual],\n",
    "        'Cohen_d': [r['cohen_d'] for r in results_individual],\n",
    "        'Efecto_categor√≠a': [r['effect_size'] for r in results_individual],\n",
    "        'p_valor': [r['p_value'] for r in results_individual],\n",
    "        'Significativo': [r['is_significant'] for r in results_individual],\n",
    "        'Cambio_absoluto': [r['improvement'] for r in results_individual],\n",
    "        'Cambio_porcentual': [r['improvement_pct'] for r in results_individual]\n",
    "    })\n",
    "    \n",
    "    # Agregar valor absoluto de Cohen's d para ordenamiento\n",
    "    effect_size_df['Cohen_d_abs'] = abs(effect_size_df['Cohen_d'])\n",
    "    \n",
    "    # Ordenar por tama√±o del efecto (valor absoluto)\n",
    "    effect_size_df = effect_size_df.sort_values('Cohen_d_abs', ascending=False)\n",
    "    \n",
    "    print(\"üìä RANKING DE M√âTRICAS POR TAMA√ëO DEL EFECTO:\")\n",
    "    display(effect_size_df[['M√©trica', 'Cohen_d', 'Efecto_categor√≠a', 'p_valor', 'Significativo']].round(3))\n",
    "    \n",
    "    # An√°lisis por categor√≠a de efecto\n",
    "    print(f\"\\n=== DISTRIBUCI√ìN POR TAMA√ëO DEL EFECTO ===\")\n",
    "    effect_counts = effect_size_df['Efecto_categor√≠a'].value_counts()\n",
    "    \n",
    "    for category in ['Muy grande', 'Grande', 'Mediano', 'Peque√±o']:\n",
    "        count = effect_counts.get(category, 0)\n",
    "        percentage = (count / len(effect_size_df)) * 100\n",
    "        print(f\"‚ö° {category}: {count} m√©tricas ({percentage:.1f}%)\")\n",
    "    \n",
    "    # M√©tricas con efecto grande o muy grande\n",
    "    large_effects = effect_size_df[effect_size_df['Cohen_d_abs'] >= 0.8]\n",
    "    if len(large_effects) > 0:\n",
    "        print(f\"\\nüéØ M√âTRICAS CON EFECTO GRANDE O MUY GRANDE (|d| ‚â• 0.8):\")\n",
    "        for _, row in large_effects.iterrows():\n",
    "            direction = \"Mejora\" if row['Cohen_d'] < 0 else \"Empeoramiento\"\n",
    "            print(f\"   ‚ö° {row['M√©trica']}: d = {row['Cohen_d']:.3f} ({direction})\")\n",
    "    \n",
    "    # M√©tricas con efecto mediano o mayor Y significativas\n",
    "    meaningful_changes = effect_size_df[\n",
    "        (effect_size_df['Cohen_d_abs'] >= 0.5) & \n",
    "        (effect_size_df['Significativo'] == True)\n",
    "    ]\n",
    "    \n",
    "    if len(meaningful_changes) > 0:\n",
    "        print(f\"\\n‚ú® CAMBIOS SIGNIFICATIVOS Y SUSTANCIALES (|d| ‚â• 0.5 y p < 0.05):\")\n",
    "        for _, row in meaningful_changes.iterrows():\n",
    "            direction = \"Mejora\" if row['Cohen_d'] < 0 else \"Empeoramiento\"\n",
    "            print(f\"   üåü {row['M√©trica']}: d = {row['Cohen_d']:.3f}, p = {row['p_valor']:.4f} ({direction})\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è No se encontraron cambios significativos y sustanciales\")\n",
    "    \n",
    "    # Resumen estad√≠stico del tama√±o del efecto\n",
    "    print(f\"\\n=== ESTAD√çSTICAS DEL TAMA√ëO DEL EFECTO ===\")\n",
    "    print(f\"üìä Cohen's d promedio: {effect_size_df['Cohen_d'].mean():.3f}\")\n",
    "    print(f\"üìä Cohen's d mediana: {effect_size_df['Cohen_d'].median():.3f}\")\n",
    "    print(f\"üìä Cohen's d rango: [{effect_size_df['Cohen_d'].min():.3f}, {effect_size_df['Cohen_d'].max():.3f}]\")\n",
    "    print(f\"üìä |Cohen's d| promedio: {effect_size_df['Cohen_d_abs'].mean():.3f}\")\n",
    "else:\n",
    "    print(\"‚ùå No hay resultados para analizar el tama√±o del efecto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63979b46",
   "metadata": {},
   "source": [
    "# 9. Statistical Visualizations\n",
    "## Visualizaciones Estad√≠sticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010a929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots comparativos AP1 vs AP2 para m√©tricas principales\n",
    "main_base_metrics = ['technical_debt', 'code_smells', 'bugs', 'vulnerabilities', \n",
    "                     'duplicated_lines_density', 'cognitive_complexity', 'reliability_rating', 'security_rating']\n",
    "\n",
    "# Filtrar m√©tricas disponibles\n",
    "available_main_metrics = [m for m in main_base_metrics if m in available_base_metrics]\n",
    "\n",
    "if available_main_metrics:\n",
    "    # Configurar subplots\n",
    "    n_metrics = len(available_main_metrics)\n",
    "    cols = 4\n",
    "    rows = (n_metrics + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, 5*rows))\n",
    "    if rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, metric in enumerate(available_main_metrics):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Preparar datos para boxplot\n",
    "        ap1_col = f\"{metric}_AP1\"\n",
    "        ap2_col = f\"{metric}_AP2\"\n",
    "        \n",
    "        # Crear datos en formato largo para seaborn\n",
    "        ap1_data = df_paired[ap1_col].dropna()\n",
    "        ap2_data = df_paired[ap2_col].dropna()\n",
    "        \n",
    "        plot_data = pd.DataFrame({\n",
    "            'Valores': pd.concat([ap1_data, ap2_data]),\n",
    "            'Asignatura': ['AP1'] * len(ap1_data) + ['AP2'] * len(ap2_data)\n",
    "        })\n",
    "        \n",
    "        # Crear box plot\n",
    "        sns.boxplot(data=plot_data, x='Asignatura', y='Valores', ax=ax)\n",
    "        ax.set_title(f'{metric}')\n",
    "        ax.set_xlabel('Asignatura')\n",
    "        ax.set_ylabel('Valor')\n",
    "        \n",
    "        # Agregar informaci√≥n estad√≠stica\n",
    "        result = next((r for r in results_individual if r['metric'] == metric), None)\n",
    "        if result:\n",
    "            significance = \"***\" if result['p_value'] < 0.001 else \"**\" if result['p_value'] < 0.01 else \"*\" if result['p_value'] < 0.05 else \"ns\"\n",
    "            ax.text(0.5, 0.95, f\"p = {result['p_value']:.3f} {significance}\\nd = {result['cohen_d']:.3f}\", \n",
    "                   transform=ax.transAxes, ha='center', va='top', \n",
    "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Ocultar subplots vac√≠os\n",
    "    for i in range(len(available_main_metrics), rows * cols):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        axes[row, col].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Comparaci√≥n de M√©tricas de Calidad: AP1 vs AP2', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ùå No hay m√©tricas principales disponibles para visualizar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f824845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap de correlaciones entre m√©tricas\n",
    "if len(available_main_metrics) > 1:\n",
    "    # Crear dataset combinado para correlaciones\n",
    "    correlation_data = pd.DataFrame()\n",
    "    \n",
    "    # Agregar datos de AP1 y AP2 para cada m√©trica\n",
    "    for metric in available_main_metrics:\n",
    "        ap1_col = f\"{metric}_AP1\"\n",
    "        ap2_col = f\"{metric}_AP2\"\n",
    "        \n",
    "        if ap1_col in df_paired.columns and ap2_col in df_paired.columns:\n",
    "            # Combinar datos de ambas asignaturas\n",
    "            combined_values = pd.concat([df_paired[ap1_col].dropna(), df_paired[ap2_col].dropna()])\n",
    "            correlation_data[metric] = combined_values.reset_index(drop=True)\n",
    "    \n",
    "    # Asegurar que todas las columnas tengan la misma longitud\n",
    "    min_length = min([len(correlation_data[col].dropna()) for col in correlation_data.columns])\n",
    "    for col in correlation_data.columns:\n",
    "        correlation_data[col] = correlation_data[col].head(min_length)\n",
    "    \n",
    "    if len(correlation_data.columns) > 1:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Calcular matriz de correlaci√≥n\n",
    "        correlation_matrix = correlation_data.corr()\n",
    "        \n",
    "        # Crear heatmap\n",
    "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "        sns.heatmap(correlation_matrix, \n",
    "                    annot=True, \n",
    "                    cmap='coolwarm', \n",
    "                    center=0,\n",
    "                    mask=mask,\n",
    "                    square=True,\n",
    "                    fmt='.3f',\n",
    "                    cbar_kws={\"shrink\": .8})\n",
    "        \n",
    "        plt.title('Matriz de Correlaciones - M√©tricas de Calidad de C√≥digo\\n(Datos combinados AP1 y AP2)', fontsize=14, pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Mostrar correlaciones m√°s fuertes\n",
    "        print(\"=== CORRELACIONES M√ÅS FUERTES ===\")\n",
    "        corr_pairs = []\n",
    "        for i in range(len(correlation_matrix.columns)):\n",
    "            for j in range(i+1, len(correlation_matrix.columns)):\n",
    "                corr_value = correlation_matrix.iloc[i, j]\n",
    "                if abs(corr_value) > 0.5:  # Correlaciones moderadas a fuertes\n",
    "                    corr_pairs.append({\n",
    "                        'M√©trica 1': correlation_matrix.columns[i],\n",
    "                        'M√©trica 2': correlation_matrix.columns[j],\n",
    "                        'Correlaci√≥n': corr_value\n",
    "                    })\n",
    "        \n",
    "        if corr_pairs:\n",
    "            corr_df = pd.DataFrame(corr_pairs)\n",
    "            corr_df = corr_df.sort_values('Correlaci√≥n', key=abs, ascending=False)\n",
    "            display(corr_df.round(3))\n",
    "        else:\n",
    "            print(\"No se encontraron correlaciones fuertes (|r| > 0.5)\")\n",
    "    else:\n",
    "        print(\"‚ùå Insuficientes m√©tricas v√°lidas para an√°lisis de correlaci√≥n\")\n",
    "else:\n",
    "    print(\"‚ùå Insuficientes m√©tricas para an√°lisis de correlaci√≥n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8507b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°ficos de evoluci√≥n individual por estudiante (muestra)\n",
    "# Mostrar evoluci√≥n para las 3 m√©tricas m√°s significativas\n",
    "if results_individual:\n",
    "    # Obtener las 3 m√©tricas con menor p-valor\n",
    "    top_significant = sorted(results_individual, key=lambda x: x['p_value'])[:3]\n",
    "    \n",
    "    if len(top_significant) > 0:\n",
    "        n_plots = len(top_significant)\n",
    "        fig, axes = plt.subplots(1, n_plots, figsize=(6*n_plots, 6))\n",
    "        if n_plots == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, result in enumerate(top_significant):\n",
    "            metric = result['metric']\n",
    "            ap1_col = result['ap1_col']\n",
    "            ap2_col = result['ap2_col']\n",
    "            ax = axes[i]\n",
    "            \n",
    "            # Obtener datos v√°lidos (sin valores nulos)\n",
    "            valid_mask = df_paired[ap1_col].notna() & df_paired[ap2_col].notna()\n",
    "            valid_data = df_paired[valid_mask]\n",
    "            \n",
    "            ap1_values = valid_data[ap1_col]\n",
    "            ap2_values = valid_data[ap2_col]\n",
    "            \n",
    "            # Tomar una muestra de estudiantes para visualizaci√≥n clara\n",
    "            sample_size = min(20, len(ap1_values))\n",
    "            if len(ap1_values) > sample_size:\n",
    "                sample_indices = np.random.choice(len(ap1_values), sample_size, replace=False)\n",
    "                ap1_sample = ap1_values.iloc[sample_indices]\n",
    "                ap2_sample = ap2_values.iloc[sample_indices]\n",
    "            else:\n",
    "                ap1_sample = ap1_values\n",
    "                ap2_sample = ap2_values\n",
    "            \n",
    "            # Crear l√≠neas conectando AP1 y AP2 para cada estudiante\n",
    "            for j in range(len(ap1_sample)):\n",
    "                ax.plot([1, 2], [ap1_sample.iloc[j], ap2_sample.iloc[j]], \n",
    "                       'o-', alpha=0.6, linewidth=1, markersize=4, color='lightblue')\n",
    "            \n",
    "            # Medias\n",
    "            ax.plot([1, 2], [ap1_values.mean(), ap2_values.mean()], \n",
    "                   'ro-', linewidth=3, markersize=8, label='Media', color='red')\n",
    "            \n",
    "            # Medianas\n",
    "            ax.plot([1, 2], [ap1_values.median(), ap2_values.median()], \n",
    "                   'go-', linewidth=2, markersize=6, label='Mediana', color='green')\n",
    "            \n",
    "            ax.set_xlim(0.8, 2.2)\n",
    "            ax.set_xticks([1, 2])\n",
    "            ax.set_xticklabels(['AP1', 'AP2'])\n",
    "            ax.set_ylabel(metric)\n",
    "            ax.set_title(f'{metric}\\n(p = {result[\"p_value\"]:.4f}, d = {result[\"cohen_d\"]:.3f})')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend()\n",
    "        \n",
    "        plt.suptitle(f'Evoluci√≥n Individual por Estudiante (Muestra de {sample_size})', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Mostrar estad√≠sticas de las m√©tricas m√°s significativas\n",
    "        print(f\"\\nüìä ESTAD√çSTICAS DE LAS M√âTRICAS M√ÅS SIGNIFICATIVAS:\")\n",
    "        for result in top_significant:\n",
    "            print(f\"\\nüéØ {result['metric']}:\")\n",
    "            print(f\"   ‚Ä¢ p-valor: {result['p_value']:.4f}\")\n",
    "            print(f\"   ‚Ä¢ Cohen's d: {result['cohen_d']:.3f} ({result['effect_size']})\")\n",
    "            print(f\"   ‚Ä¢ Cambio: {result['improvement']:.3f} ({result['improvement_pct']:.1f}%)\")\n",
    "            print(f\"   ‚Ä¢ Pares analizados: {result['n_pairs']}\")\n",
    "    else:\n",
    "        print(\"‚ùå No hay resultados significativos para mostrar\")\n",
    "else:\n",
    "    print(\"‚ùå No hay resultados para mostrar evoluci√≥n individual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8be220b",
   "metadata": {},
   "source": [
    "# 10. Results Summary and Interpretation\n",
    "## Resumen de Resultados e Interpretaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3b152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla resumen final con todas las m√©tricas y resultados\n",
    "print(\"=== TABLA RESUMEN FINAL - AN√ÅLISIS ESTAD√çSTICO COMPLETO ===\")\n",
    "\n",
    "if results_individual:\n",
    "    # Crear tabla resumen final\n",
    "    final_summary = pd.DataFrame({\n",
    "        'M√©trica': [r['metric'] for r in results_individual],\n",
    "        'Dimensi√≥n': [next((dim for dim, metrics in quality_dimensions.items() if r['metric'] in metrics), 'Complementaria') \n",
    "                     for r in results_individual],\n",
    "        'AP1_Media': [r['ap1_mean'] for r in results_individual],\n",
    "        'AP1_Std': [r['ap1_std'] for r in results_individual],\n",
    "        'AP2_Media': [r['ap2_mean'] for r in results_individual],\n",
    "        'AP2_Std': [r['ap2_std'] for r in results_individual],\n",
    "        'Cambio_Absoluto': [r['improvement'] for r in results_individual],\n",
    "        'Cambio_Porcentual': [r['improvement_pct'] for r in results_individual],\n",
    "        'Test_Estad√≠stico': [r['test'] for r in results_individual],\n",
    "        'p_valor': [r['p_value'] for r in results_individual],\n",
    "        'Cohen_d': [r['cohen_d'] for r in results_individual],\n",
    "        'Tama√±o_Efecto': [r['effect_size'] for r in results_individual],\n",
    "        'Significativo': [r['is_significant'] for r in results_individual]\n",
    "    })\n",
    "    \n",
    "    # Agregar informaci√≥n de correcci√≥n FDR si est√° disponible\n",
    "    if 'correction_results' in locals():\n",
    "        fdr_significant = correction_results.set_index('M√©trica')['Significativo_FDR'].to_dict()\n",
    "        final_summary['Significativo_FDR'] = final_summary['M√©trica'].map(fdr_significant)\n",
    "    \n",
    "    # Redondear valores num√©ricos\n",
    "    numeric_cols = ['AP1_Media', 'AP1_Std', 'AP2_Media', 'AP2_Std', 'Cambio_Absoluto', 'Cambio_Porcentual', 'p_valor', 'Cohen_d']\n",
    "    final_summary[numeric_cols] = final_summary[numeric_cols].round(3)\n",
    "    \n",
    "    # Ordenar por dimensi√≥n y luego por p-valor\n",
    "    final_summary = final_summary.sort_values(['Dimensi√≥n', 'p_valor'])\n",
    "    \n",
    "    display(final_summary)\n",
    "    \n",
    "    # Exportar tabla para uso posterior\n",
    "    final_summary.to_csv('resultados_analisis_estadistico.csv', index=False)\n",
    "    print(\"\\nüíæ Tabla guardada como 'resultados_analisis_estadistico.csv'\")\n",
    "else:\n",
    "    print(\"‚ùå No hay resultados para mostrar en la tabla resumen final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5970511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretaci√≥n y conclusiones finales\n",
    "print(\"=== INTERPRETACI√ìN Y CONCLUSIONES ===\")\n",
    "\n",
    "if results_individual:\n",
    "    # Calcular estad√≠sticas generales\n",
    "    total_metrics = len(results_individual)\n",
    "    significant_metrics = sum(r['is_significant'] for r in results_individual)\n",
    "    large_effects = sum(abs(r['cohen_d']) >= 0.8 for r in results_individual)\n",
    "    medium_effects = sum(0.5 <= abs(r['cohen_d']) < 0.8 for r in results_individual)\n",
    "    \n",
    "    print(f\"üìä RESUMEN ESTAD√çSTICO GENERAL:\")\n",
    "    print(f\"   ‚Ä¢ Total de m√©tricas analizadas: {total_metrics}\")\n",
    "    print(f\"   ‚Ä¢ M√©tricas con diferencias significativas (p < 0.05): {significant_metrics} ({significant_metrics/total_metrics*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ M√©tricas con efecto grande (|d| ‚â• 0.8): {large_effects} ({large_effects/total_metrics*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ M√©tricas con efecto mediano (0.5 ‚â§ |d| < 0.8): {medium_effects} ({medium_effects/total_metrics*100:.1f}%)\")\n",
    "    \n",
    "    # An√°lisis por dimensi√≥n\n",
    "    print(f\"\\nüèóÔ∏è AN√ÅLISIS POR DIMENSI√ìN DE CALIDAD:\")\n",
    "    for dimension, summary in dimension_results.items():\n",
    "        print(f\"   ‚Ä¢ {dimension}: {summary['significant_count']}/{summary['metrics_count']} significativas ({summary['significant_pct']:.1f}%)\")\n",
    "        print(f\"     - Tama√±o de efecto promedio: {summary['avg_effect_size']:.3f}\")\n",
    "    \n",
    "    # Respuesta a la hip√≥tesis de investigaci√≥n\n",
    "    print(f\"\\nüéØ RESPUESTA A LA HIP√ìTESIS DE INVESTIGACI√ìN:\")\n",
    "    if significant_metrics > 0:\n",
    "        print(f\"   ‚úÖ RECHAZAMOS H‚ÇÄ: Existe evidencia estad√≠stica de diferencias significativas\")\n",
    "        print(f\"   ‚úÖ ACEPTAMOS H‚ÇÅ: Existe mejora estad√≠sticamente significativa en {significant_metrics} m√©tricas\")\n",
    "        print(f\"   üìà La intervenci√≥n pedag√≥gica (taller 'C√≥digo Limpio') mostr√≥ efectos positivos\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå NO RECHAZAMOS H‚ÇÄ: No hay evidencia estad√≠stica suficiente de diferencias\")\n",
    "        print(f\"   ‚ùå NO ACEPTAMOS H‚ÇÅ: No se detect√≥ mejora estad√≠sticamente significativa\")\n",
    "    \n",
    "    # Interpretaci√≥n pr√°ctica\n",
    "    print(f\"\\nüí° INTERPRETACI√ìN PR√ÅCTICA:\")\n",
    "    \n",
    "    # M√©tricas que mejoraron significativamente\n",
    "    improved_metrics = [r for r in results_individual if r['is_significant'] and r['improvement'] < 0]\n",
    "    if improved_metrics:\n",
    "        print(f\"   üåü M√©tricas que mejoraron significativamente (valores menores son mejores):\")\n",
    "        for result in improved_metrics:\n",
    "            print(f\"      - {result['metric']}: {result['improvement_pct']:.1f}% de mejora\")\n",
    "    \n",
    "    # M√©tricas que empeoraron significativamente\n",
    "    worsened_metrics = [r for r in results_individual if r['is_significant'] and r['improvement'] > 0]\n",
    "    if worsened_metrics:\n",
    "        print(f\"   ‚ö†Ô∏è M√©tricas que empeoraron significativamente:\")\n",
    "        for result in worsened_metrics:\n",
    "            print(f\"      - {result['metric']}: {result['improvement_pct']:.1f}% de empeoramiento\")\n",
    "    \n",
    "    # Limitaciones del estudio\n",
    "    print(f\"\\n‚ö†Ô∏è LIMITACIONES DEL AN√ÅLISIS:\")\n",
    "    print(f\"   ‚Ä¢ Dise√±o pre-post sin grupo control\")\n",
    "    print(f\"   ‚Ä¢ Posibles variables confusoras no controladas\")\n",
    "    print(f\"   ‚Ä¢ Efectos del aprendizaje natural a lo largo del tiempo\")\n",
    "    print(f\"   ‚Ä¢ Diferencias en la complejidad de los proyectos entre asignaturas\")\n",
    "    \n",
    "    # Recomendaciones para investigaci√≥n futura\n",
    "    print(f\"\\nüîÆ RECOMENDACIONES PARA INVESTIGACI√ìN FUTURA:\")\n",
    "    print(f\"   ‚Ä¢ Incluir grupo control sin intervenci√≥n\")\n",
    "    print(f\"   ‚Ä¢ An√°lisis longitudinal con m√°s puntos de medici√≥n\")\n",
    "    print(f\"   ‚Ä¢ An√°lisis cualitativo complementario\")\n",
    "    print(f\"   ‚Ä¢ Considerar variables moderadoras (experiencia previa, motivaci√≥n)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No se pueden generar conclusiones sin resultados v√°lidos\")\n",
    "\n",
    "print(f\"\\nüéä AN√ÅLISIS ESTAD√çSTICO COMPLETADO\")\n",
    "print(f\"üìã Todos los resultados est√°n disponibles en las secciones anteriores\")\n",
    "print(f\"üíæ Tablas exportadas para uso posterior en la tesis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
