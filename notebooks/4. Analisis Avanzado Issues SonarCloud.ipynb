{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a26723ff",
   "metadata": {},
   "source": [
    "# üîç An√°lisis Avanzado de Issues de SonarCloud en Proyectos Estudiantiles\n",
    "\n",
    "## üéØ Objetivo\n",
    "An√°lisis exhaustivo y multidimensional de **7,844 issues detallados** extra√≠dos de SonarCloud de **60 estudiantes** en dos asignaturas: **\"Programaci√≥n Aplicada I\" (AP1)** y **\"Programaci√≥n Aplicada II\" (AP2)\"**.\n",
    "\n",
    "## üìä Dataset\n",
    "- **7,844 issues detallados** de SonarCloud\n",
    "- **60 estudiantes** de programaci√≥n aplicada\n",
    "- **Dos asignaturas consecutivas**: AP1 ‚Üí AP2\n",
    "- **M√∫ltiples m√©tricas**: tipo, severidad, deuda t√©cnica, ubicaci√≥n, reglas\n",
    "\n",
    "## üî¨ Enfoque de An√°lisis\n",
    "Este notebook combina:\n",
    "- **An√°lisis estad√≠stico riguroso** para identificar patrones significativos\n",
    "- **Miner√≠a de datos educativos** para insights pedag√≥gicos\n",
    "- **Visualizaciones interactivas** para exploraci√≥n din√°mica\n",
    "- **Modelado predictivo** para sistemas de alerta temprana\n",
    "- **Recomendaciones accionables** basadas en evidencia\n",
    "\n",
    "---\n",
    "\n",
    "## üìã √çndice de Contenido\n",
    "\n",
    "1. **Data Loading & Preprocessing** - Carga y limpieza de datos\n",
    "2. **Exploratory Data Analysis** - An√°lisis exploratorio integral\n",
    "3. **Rule Analysis & Categorization** - An√°lisis de reglas violadas\n",
    "4. **Technical Debt Analysis** - An√°lisis de deuda t√©cnica\n",
    "5. **Issue Type Analysis** - An√°lisis por tipos de issues\n",
    "6. **Student Profiling & Clustering** - Perfilado y clustering de estudiantes\n",
    "7. **Temporal & Evolution Analysis** - An√°lisis temporal y evolutivo\n",
    "8. **Location & Context Analysis** - An√°lisis de ubicaci√≥n y contexto\n",
    "9. **Message Text Mining Analysis** - Miner√≠a de texto en mensajes\n",
    "10. **Advanced Statistical Analysis** - An√°lisis estad√≠stico avanzado\n",
    "11. **Specialized Visualizations** - Visualizaciones especializadas\n",
    "12. **Predictive Modeling** - Modelado predictivo\n",
    "13. **Educational Insights & Recommendations** - Insights educativos y recomendaciones\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b3e68",
   "metadata": {},
   "source": [
    "# 1. üì• Data Loading and Preprocessing\n",
    "\n",
    "En esta secci√≥n cargamos y preprocesamos el dataset de issues de SonarCloud, realizamos limpieza de datos y creamos caracter√≠sticas derivadas para el an√°lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44437c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# IMPORTS Y CONFIGURACI√ìN INICIAL\n",
    "# ===================================================================\n",
    "\n",
    "# Librer√≠as b√°sicas para manejo de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Librer√≠as para an√°lisis estad√≠stico\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, mannwhitneyu, wilcoxon, shapiro\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "\n",
    "# Librer√≠as para machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Librer√≠as para procesamiento de texto\n",
    "import re\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Librer√≠as para an√°lisis de redes\n",
    "import networkx as nx\n",
    "\n",
    "# Configuraci√≥n de visualizaciones\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Configuraci√≥n de warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de plotly para notebooks\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas exitosamente\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"üî¢ NumPy version: {np.__version__}\")\n",
    "print(f\"üìà Plotly disponible para visualizaciones interactivas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b1e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CARGA DE DATOS\n",
    "# ===================================================================\n",
    "\n",
    "# Cargar el dataset de issues detallados\n",
    "print(\"üîÑ Cargando dataset de issues...\")\n",
    "issues_df = pd.read_csv('../data/issues_detallados_latest.csv')\n",
    "\n",
    "print(f\"‚úÖ Dataset cargado exitosamente\")\n",
    "print(f\"üìä Dimensiones: {issues_df.shape[0]:,} issues √ó {issues_df.shape[1]} columnas\")\n",
    "print(f\"üë• Estudiantes √∫nicos: {issues_df['student_id'].nunique()}\")\n",
    "print(f\"üìö Asignaturas: {', '.join(issues_df['assignment'].unique())}\")\n",
    "\n",
    "# Mostrar informaci√≥n b√°sica del dataset\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã INFORMACI√ìN GENERAL DEL DATASET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total de issues: {len(issues_df):,}\")\n",
    "print(f\"Estudiantes √∫nicos: {issues_df['student_id'].nunique()}\")\n",
    "print(f\"Proyectos √∫nicos: {issues_df['project_key'].nunique()}\")\n",
    "print(f\"Reglas √∫nicas violadas: {issues_df['rule'].nunique()}\")\n",
    "print(f\"Tipos de issues: {', '.join(issues_df['type'].unique())}\")\n",
    "print(f\"Niveles de severidad: {', '.join(issues_df['severity'].unique())}\")\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç MUESTRA DE DATOS\")\n",
    "print(\"=\"*60)\n",
    "issues_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af1889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# AN√ÅLISIS DE CALIDAD DE DATOS Y LIMPIEZA\n",
    "# ===================================================================\n",
    "\n",
    "print(\"üîç AN√ÅLISIS DE CALIDAD DE DATOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verificar valores faltantes\n",
    "missing_values = issues_df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(issues_df)) * 100\n",
    "\n",
    "print(\"üìä Valores faltantes por columna:\")\n",
    "for col, missing in missing_values.items():\n",
    "    if missing > 0:\n",
    "        print(f\"   {col}: {missing:,} ({missing_percentage[col]:.2f}%)\")\n",
    "\n",
    "# Informaci√≥n detallada de las columnas\n",
    "print(f\"\\nüìã INFORMACI√ìN DETALLADA DE COLUMNAS\")\n",
    "print(\"=\"*60)\n",
    "print(issues_df.info())\n",
    "\n",
    "# An√°lisis de duplicados\n",
    "print(f\"\\nüîÑ AN√ÅLISIS DE DUPLICADOS\")\n",
    "print(\"=\"*60)\n",
    "duplicates = issues_df.duplicated().sum()\n",
    "print(f\"Issues duplicados: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"Eliminando duplicados...\")\n",
    "    issues_df = issues_df.drop_duplicates()\n",
    "    print(f\"Dataset despu√©s de eliminar duplicados: {len(issues_df):,} issues\")\n",
    "\n",
    "# Convertir tipos de datos apropiados\n",
    "print(f\"\\nüîß CONVERSI√ìN DE TIPOS DE DATOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convertir fechas\n",
    "date_columns = ['creation_date', 'update_date']\n",
    "for col in date_columns:\n",
    "    if col in issues_df.columns:\n",
    "        issues_df[col] = pd.to_datetime(issues_df[col], errors='coerce')\n",
    "        print(f\"‚úÖ {col} convertido a datetime\")\n",
    "\n",
    "# Convertir effort y debt a minutos num√©ricos\n",
    "for col in ['effort', 'debt']:\n",
    "    if col in issues_df.columns:\n",
    "        # Extraer n√∫meros de strings como \"5min\", \"10min\", etc.\n",
    "        issues_df[f'{col}_minutes'] = issues_df[col].str.extract('(\\d+)').astype(float)\n",
    "        issues_df[f'{col}_minutes'] = issues_df[f'{col}_minutes'].fillna(0)\n",
    "        print(f\"‚úÖ {col} convertido a minutos num√©ricos\")\n",
    "\n",
    "# Verificar rangos de datos\n",
    "print(f\"\\nüìä RANGOS DE DATOS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Rango de fechas de creaci√≥n: {issues_df['creation_date'].min()} a {issues_df['creation_date'].max()}\")\n",
    "print(f\"Rango de l√≠neas de c√≥digo: {issues_df['line'].min()} a {issues_df['line'].max()}\")\n",
    "print(f\"Effort range: {issues_df['effort_minutes'].min()} a {issues_df['effort_minutes'].max()} minutos\")\n",
    "print(f\"Debt range: {issues_df['debt_minutes'].min()} a {issues_df['debt_minutes'].max()} minutos\")\n",
    "\n",
    "print(\"\\n‚úÖ Limpieza de datos completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c14b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CREACI√ìN DE CARACTER√çSTICAS DERIVADAS\n",
    "# ===================================================================\n",
    "\n",
    "print(\"üîß CREACI√ìN DE CARACTER√çSTICAS DERIVADAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Extraer tipo de archivo de component\n",
    "issues_df['file_extension'] = issues_df['component'].str.extract(r'\\.([^.]+)$')\n",
    "issues_df['file_extension'] = issues_df['file_extension'].fillna('unknown')\n",
    "\n",
    "# 2. Extraer namespace/carpeta principal\n",
    "issues_df['namespace'] = issues_df['component'].str.extract(r':([^/]+)')\n",
    "issues_df['folder_level_1'] = issues_df['component'].str.extract(r':([^/]+/[^/]+)')\n",
    "\n",
    "# 3. Categorizar familias de reglas\n",
    "def categorize_rule_family(rule):\n",
    "    if pd.isna(rule):\n",
    "        return 'unknown'\n",
    "    elif rule.startswith('external_roslyn:CS'):\n",
    "        return 'roslyn_csharp'\n",
    "    elif rule.startswith('csharpsquid:S'):\n",
    "        return 'sonarqube_csharp'\n",
    "    elif rule.startswith('css:S'):\n",
    "        return 'css'\n",
    "    elif rule.startswith('typescript:S'):\n",
    "        return 'typescript'\n",
    "    elif rule.startswith('javascript:S'):\n",
    "        return 'javascript'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "issues_df['rule_family'] = issues_df['rule'].apply(categorize_rule_family)\n",
    "\n",
    "# 4. Mapear severidad a valores num√©ricos para an√°lisis\n",
    "severity_mapping = {\n",
    "    'CRITICAL': 4,\n",
    "    'MAJOR': 3,\n",
    "    'MINOR': 2,\n",
    "    'INFO': 1\n",
    "}\n",
    "issues_df['severity_numeric'] = issues_df['severity'].map(severity_mapping)\n",
    "\n",
    "# 5. Crear indicadores binarios para tipos de issues\n",
    "for issue_type in issues_df['type'].unique():\n",
    "    issues_df[f'is_{issue_type.lower()}'] = (issues_df['type'] == issue_type).astype(int)\n",
    "\n",
    "# 6. Extraer a√±o y mes de creaci√≥n\n",
    "issues_df['creation_year'] = issues_df['creation_date'].dt.year\n",
    "issues_df['creation_month'] = issues_df['creation_date'].dt.month\n",
    "\n",
    "# 7. Calcular m√©tricas agregadas por estudiante\n",
    "print(\"üìä Calculando m√©tricas por estudiante...\")\n",
    "\n",
    "student_metrics = issues_df.groupby(['student_id', 'assignment']).agg({\n",
    "    'issue_key': 'count',\n",
    "    'severity_numeric': ['mean', 'sum'],\n",
    "    'effort_minutes': 'sum',\n",
    "    'debt_minutes': 'sum',\n",
    "    'type': lambda x: x.value_counts().to_dict(),\n",
    "    'rule_family': lambda x: x.value_counts().to_dict(),\n",
    "    'file_extension': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "student_metrics.columns = [f'{col[0]}_{col[1]}' if col[1] else col[0] for col in student_metrics.columns]\n",
    "student_metrics = student_metrics.rename(columns={\n",
    "    'issue_key_count': 'total_issues',\n",
    "    'severity_numeric_mean': 'avg_severity',\n",
    "    'severity_numeric_sum': 'total_severity_score',\n",
    "    'effort_minutes_sum': 'total_effort_minutes',\n",
    "    'debt_minutes_sum': 'total_debt_minutes',\n",
    "    'file_extension_nunique': 'files_with_issues'\n",
    "})\n",
    "\n",
    "# 8. Crear dataset de m√©tricas por estudiante en formato amplio\n",
    "student_summary = issues_df.pivot_table(\n",
    "    index=['student_id', 'nombre', 'assignment'],\n",
    "    values='issue_key',\n",
    "    columns='type',\n",
    "    aggfunc='count',\n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "# Aplanar nombres de columnas\n",
    "student_summary.columns.name = None\n",
    "\n",
    "print(\"‚úÖ Caracter√≠sticas derivadas creadas:\")\n",
    "print(f\"   üìÅ Extensiones de archivo: {issues_df['file_extension'].nunique()} tipos\")\n",
    "print(f\"   üèóÔ∏è Familias de reglas: {issues_df['rule_family'].nunique()} familias\")\n",
    "print(f\"   üìä M√©tricas por estudiante calculadas para {student_summary.shape[0]} registros\")\n",
    "\n",
    "# Mostrar ejemplo de m√©tricas por estudiante\n",
    "print(f\"\\nüìã EJEMPLO DE M√âTRICAS POR ESTUDIANTE\")\n",
    "print(\"=\"*60)\n",
    "student_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8be8f6",
   "metadata": {},
   "source": [
    "# 2. üìä Exploratory Data Analysis (EDA)\n",
    "\n",
    "En esta secci√≥n realizamos un an√°lisis exploratorio comprehensivo de los datos de issues, examinando distribuciones, patrones temporales y caracter√≠sticas principales del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ece131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# DISTRIBUCI√ìN GENERAL DE ISSUES\n",
    "# ===================================================================\n",
    "\n",
    "print(\"üìä AN√ÅLISIS DE DISTRIBUCI√ìN GENERAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Estad√≠sticas por asignatura\n",
    "assignment_stats = issues_df.groupby('assignment').agg({\n",
    "    'issue_key': 'count',\n",
    "    'student_id': 'nunique',\n",
    "    'severity_numeric': 'mean',\n",
    "    'effort_minutes': 'sum',\n",
    "    'debt_minutes': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "print(\"Estad√≠sticas por asignatura:\")\n",
    "print(assignment_stats)\n",
    "\n",
    "# Distribuci√≥n por tipo de issue\n",
    "print(f\"\\nüìã DISTRIBUCI√ìN POR TIPO DE ISSUE\")\n",
    "type_dist = issues_df['type'].value_counts()\n",
    "type_pct = issues_df['type'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Conteo y porcentaje por tipo:\")\n",
    "for issue_type in type_dist.index:\n",
    "    print(f\"   {issue_type}: {type_dist[issue_type]:,} ({type_pct[issue_type]:.1f}%)\")\n",
    "\n",
    "# Distribuci√≥n por severidad\n",
    "print(f\"\\n‚ö†Ô∏è DISTRIBUCI√ìN POR SEVERIDAD\")\n",
    "severity_dist = issues_df['severity'].value_counts()\n",
    "severity_pct = issues_df['severity'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Conteo y porcentaje por severidad:\")\n",
    "for severity in ['CRITICAL', 'MAJOR', 'MINOR', 'INFO']:\n",
    "    if severity in severity_dist.index:\n",
    "        print(f\"   {severity}: {severity_dist[severity]:,} ({severity_pct[severity]:.1f}%)\")\n",
    "\n",
    "# Crear visualizaci√≥n de distribuciones\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('üìä Distribuci√≥n General de Issues', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Issues por asignatura\n",
    "assignment_counts = issues_df['assignment'].value_counts()\n",
    "axes[0,0].bar(assignment_counts.index, assignment_counts.values, color=['#1f77b4', '#ff7f0e'])\n",
    "axes[0,0].set_title('Issues por Asignatura')\n",
    "axes[0,0].set_ylabel('N√∫mero de Issues')\n",
    "for i, v in enumerate(assignment_counts.values):\n",
    "    axes[0,0].text(i, v + 50, f'{v:,}', ha='center', fontweight='bold')\n",
    "\n",
    "# 2. Issues por tipo\n",
    "type_counts = issues_df['type'].value_counts()\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(type_counts)))\n",
    "axes[0,1].bar(range(len(type_counts)), type_counts.values, color=colors)\n",
    "axes[0,1].set_title('Issues por Tipo')\n",
    "axes[0,1].set_ylabel('N√∫mero de Issues')\n",
    "axes[0,1].set_xticks(range(len(type_counts)))\n",
    "axes[0,1].set_xticklabels(type_counts.index, rotation=45, ha='right')\n",
    "for i, v in enumerate(type_counts.values):\n",
    "    axes[0,1].text(i, v + 50, f'{v:,}', ha='center', fontweight='bold')\n",
    "\n",
    "# 3. Issues por severidad\n",
    "severity_order = ['CRITICAL', 'MAJOR', 'MINOR', 'INFO']\n",
    "severity_counts = issues_df['severity'].value_counts().reindex(severity_order)\n",
    "severity_colors = ['#d62728', '#ff7f0e', '#ffbb78', '#2ca02c']\n",
    "axes[1,0].bar(severity_counts.index, severity_counts.values, color=severity_colors)\n",
    "axes[1,0].set_title('Issues por Severidad')\n",
    "axes[1,0].set_ylabel('N√∫mero de Issues')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(severity_counts.values):\n",
    "    axes[1,0].text(i, v + 50, f'{v:,}', ha='center', fontweight='bold')\n",
    "\n",
    "# 4. Issues por estudiante (distribuci√≥n)\n",
    "issues_per_student = issues_df.groupby('student_id').size()\n",
    "axes[1,1].hist(issues_per_student, bins=20, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "axes[1,1].set_title('Distribuci√≥n de Issues por Estudiante')\n",
    "axes[1,1].set_xlabel('N√∫mero de Issues por Estudiante')\n",
    "axes[1,1].set_ylabel('N√∫mero de Estudiantes')\n",
    "axes[1,1].axvline(issues_per_student.mean(), color='red', linestyle='--', \n",
    "                  label=f'Media: {issues_per_student.mean():.1f}')\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estad√≠sticas descriptivas\n",
    "print(f\"\\nüìà ESTAD√çSTICAS DESCRIPTIVAS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Issues por estudiante:\")\n",
    "print(f\"   Media: {issues_per_student.mean():.1f}\")\n",
    "print(f\"   Mediana: {issues_per_student.median():.1f}\")\n",
    "print(f\"   Desviaci√≥n est√°ndar: {issues_per_student.std():.1f}\")\n",
    "print(f\"   M√≠nimo: {issues_per_student.min()}\")\n",
    "print(f\"   M√°ximo: {issues_per_student.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36e1c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# AN√ÅLISIS TEMPORAL DE ISSUES\n",
    "# ===================================================================\n",
    "\n",
    "print(\"üìÖ AN√ÅLISIS TEMPORAL DE ISSUES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# An√°lisis por fecha de creaci√≥n\n",
    "if 'creation_date' in issues_df.columns:\n",
    "    issues_df['creation_date_only'] = issues_df['creation_date'].dt.date\n",
    "    \n",
    "    # Issues por d√≠a\n",
    "    daily_issues = issues_df.groupby(['creation_date_only', 'assignment']).size().reset_index(name='count')\n",
    "    \n",
    "    # Crear visualizaci√≥n temporal\n",
    "    fig = px.line(daily_issues, x='creation_date_only', y='count', color='assignment',\n",
    "                  title='üìÖ Evoluci√≥n Temporal de Issues por Asignatura',\n",
    "                  labels={'creation_date_only': 'Fecha de Creaci√≥n', 'count': 'N√∫mero de Issues'})\n",
    "    fig.update_layout(height=500)\n",
    "    fig.show()\n",
    "    \n",
    "    # Estad√≠sticas temporales por asignatura\n",
    "    temporal_stats = issues_df.groupby('assignment')['creation_date'].agg(['min', 'max', 'count'])\n",
    "    print(\"Estad√≠sticas temporales por asignatura:\")\n",
    "    print(temporal_stats)\n",
    "    \n",
    "    # An√°lisis por mes\n",
    "    issues_df['year_month'] = issues_df['creation_date'].dt.to_period('M')\n",
    "    monthly_issues = issues_df.groupby(['year_month', 'assignment']).size().reset_index(name='count')\n",
    "    monthly_issues['year_month_str'] = monthly_issues['year_month'].astype(str)\n",
    "    \n",
    "    print(f\"\\nIssues por mes y asignatura:\")\n",
    "    monthly_pivot = monthly_issues.pivot(index='year_month_str', columns='assignment', values='count').fillna(0)\n",
    "    print(monthly_pivot)\n",
    "\n",
    "# An√°lisis de status de issues\n",
    "print(f\"\\nüîÑ AN√ÅLISIS DE STATUS DE ISSUES\")\n",
    "print(\"=\"*60)\n",
    "status_dist = issues_df['status'].value_counts()\n",
    "status_pct = issues_df['status'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Distribuci√≥n de status:\")\n",
    "for status in status_dist.index:\n",
    "    print(f\"   {status}: {status_dist[status]:,} ({status_pct[status]:.1f}%)\")\n",
    "\n",
    "# Cross-tabulation de assignment vs tipo de issue\n",
    "print(f\"\\nüìä CROSSTAB: ASIGNATURA vs TIPO DE ISSUE\")\n",
    "print(\"=\"*60)\n",
    "crosstab_assignment_type = pd.crosstab(issues_df['assignment'], issues_df['type'], margins=True)\n",
    "print(crosstab_assignment_type)\n",
    "\n",
    "# Porcentajes por fila\n",
    "print(f\"\\nPorcentajes por asignatura:\")\n",
    "crosstab_pct = pd.crosstab(issues_df['assignment'], issues_df['type'], normalize='index') * 100\n",
    "print(crosstab_pct.round(1))\n",
    "\n",
    "# Visualizaci√≥n de heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(crosstab_pct, annot=True, fmt='.1f', cmap='YlOrRd', \n",
    "            cbar_kws={'label': 'Porcentaje'})\n",
    "plt.title('üî• Heatmap: Distribuci√≥n de Tipos de Issues por Asignatura (%)')\n",
    "plt.xlabel('Tipo de Issue')\n",
    "plt.ylabel('Asignatura')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
