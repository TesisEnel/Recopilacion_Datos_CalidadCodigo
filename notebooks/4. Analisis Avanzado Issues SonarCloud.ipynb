{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a26723ff",
   "metadata": {},
   "source": [
    "# 🔍 Análisis Avanzado de Issues de SonarCloud en Proyectos Estudiantiles\n",
    "\n",
    "## 🎯 Objetivo\n",
    "Análisis exhaustivo y multidimensional de **7,844 issues detallados** extraídos de SonarCloud de **60 estudiantes** en dos asignaturas: **\"Programación Aplicada I\" (AP1)** y **\"Programación Aplicada II\" (AP2)\"**.\n",
    "\n",
    "## 📊 Dataset\n",
    "- **7,844 issues detallados** de SonarCloud\n",
    "- **60 estudiantes** de programación aplicada\n",
    "- **Dos asignaturas consecutivas**: AP1 → AP2\n",
    "- **Múltiples métricas**: tipo, severidad, deuda técnica, ubicación, reglas\n",
    "\n",
    "## 🔬 Enfoque de Análisis\n",
    "Este notebook combina:\n",
    "- **Análisis estadístico riguroso** para identificar patrones significativos\n",
    "- **Minería de datos educativos** para insights pedagógicos\n",
    "- **Visualizaciones interactivas** para exploración dinámica\n",
    "- **Modelado predictivo** para sistemas de alerta temprana\n",
    "- **Recomendaciones accionables** basadas en evidencia\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 Índice de Contenido\n",
    "\n",
    "1. **Data Loading & Preprocessing** - Carga y limpieza de datos\n",
    "2. **Exploratory Data Analysis** - Análisis exploratorio integral\n",
    "3. **Rule Analysis & Categorization** - Análisis de reglas violadas\n",
    "4. **Technical Debt Analysis** - Análisis de deuda técnica\n",
    "5. **Issue Type Analysis** - Análisis por tipos de issues\n",
    "6. **Student Profiling & Clustering** - Perfilado y clustering de estudiantes\n",
    "7. **Temporal & Evolution Analysis** - Análisis temporal y evolutivo\n",
    "8. **Location & Context Analysis** - Análisis de ubicación y contexto\n",
    "9. **Message Text Mining Analysis** - Minería de texto en mensajes\n",
    "10. **Advanced Statistical Analysis** - Análisis estadístico avanzado\n",
    "11. **Specialized Visualizations** - Visualizaciones especializadas\n",
    "12. **Predictive Modeling** - Modelado predictivo\n",
    "13. **Educational Insights & Recommendations** - Insights educativos y recomendaciones\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b3e68",
   "metadata": {},
   "source": [
    "# 1. 📥 Data Loading and Preprocessing\n",
    "\n",
    "En esta sección cargamos y preprocesamos el dataset de issues de SonarCloud, realizamos limpieza de datos y creamos características derivadas para el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44437c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# IMPORTS Y CONFIGURACIÓN INICIAL\n",
    "# ===================================================================\n",
    "\n",
    "# Librerías básicas para manejo de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Librerías para análisis estadístico\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, mannwhitneyu, wilcoxon, shapiro\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "\n",
    "# Librerías para machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Librerías para procesamiento de texto\n",
    "import re\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Librerías para análisis de redes\n",
    "import networkx as nx\n",
    "\n",
    "# Configuración de visualizaciones\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Configuración de warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de plotly para notebooks\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "print(\"✅ Librerías importadas exitosamente\")\n",
    "print(f\"📊 Pandas version: {pd.__version__}\")\n",
    "print(f\"🔢 NumPy version: {np.__version__}\")\n",
    "print(f\"📈 Plotly disponible para visualizaciones interactivas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b1e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CARGA DE DATOS\n",
    "# ===================================================================\n",
    "\n",
    "# Cargar el dataset de issues detallados\n",
    "print(\"🔄 Cargando dataset de issues...\")\n",
    "issues_df = pd.read_csv('../data/issues_detallados_latest.csv')\n",
    "\n",
    "print(f\"✅ Dataset cargado exitosamente\")\n",
    "print(f\"📊 Dimensiones: {issues_df.shape[0]:,} issues × {issues_df.shape[1]} columnas\")\n",
    "print(f\"👥 Estudiantes únicos: {issues_df['student_id'].nunique()}\")\n",
    "print(f\"📚 Asignaturas: {', '.join(issues_df['assignment'].unique())}\")\n",
    "\n",
    "# Mostrar información básica del dataset\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📋 INFORMACIÓN GENERAL DEL DATASET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total de issues: {len(issues_df):,}\")\n",
    "print(f\"Estudiantes únicos: {issues_df['student_id'].nunique()}\")\n",
    "print(f\"Proyectos únicos: {issues_df['project_key'].nunique()}\")\n",
    "print(f\"Reglas únicas violadas: {issues_df['rule'].nunique()}\")\n",
    "print(f\"Tipos de issues: {', '.join(issues_df['type'].unique())}\")\n",
    "print(f\"Niveles de severidad: {', '.join(issues_df['severity'].unique())}\")\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🔍 MUESTRA DE DATOS\")\n",
    "print(\"=\"*60)\n",
    "issues_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af1889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# ANÁLISIS DE CALIDAD DE DATOS Y LIMPIEZA\n",
    "# ===================================================================\n",
    "\n",
    "print(\"🔍 ANÁLISIS DE CALIDAD DE DATOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verificar valores faltantes\n",
    "missing_values = issues_df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(issues_df)) * 100\n",
    "\n",
    "print(\"📊 Valores faltantes por columna:\")\n",
    "for col, missing in missing_values.items():\n",
    "    if missing > 0:\n",
    "        print(f\"   {col}: {missing:,} ({missing_percentage[col]:.2f}%)\")\n",
    "\n",
    "# Información detallada de las columnas\n",
    "print(f\"\\n📋 INFORMACIÓN DETALLADA DE COLUMNAS\")\n",
    "print(\"=\"*60)\n",
    "print(issues_df.info())\n",
    "\n",
    "# Análisis de duplicados\n",
    "print(f\"\\n🔄 ANÁLISIS DE DUPLICADOS\")\n",
    "print(\"=\"*60)\n",
    "duplicates = issues_df.duplicated().sum()\n",
    "print(f\"Issues duplicados: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"Eliminando duplicados...\")\n",
    "    issues_df = issues_df.drop_duplicates()\n",
    "    print(f\"Dataset después de eliminar duplicados: {len(issues_df):,} issues\")\n",
    "\n",
    "# Convertir tipos de datos apropiados\n",
    "print(f\"\\n🔧 CONVERSIÓN DE TIPOS DE DATOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convertir fechas\n",
    "date_columns = ['creation_date', 'update_date']\n",
    "for col in date_columns:\n",
    "    if col in issues_df.columns:\n",
    "        issues_df[col] = pd.to_datetime(issues_df[col], errors='coerce')\n",
    "        print(f\"✅ {col} convertido a datetime\")\n",
    "\n",
    "# Convertir effort y debt a minutos numéricos\n",
    "for col in ['effort', 'debt']:\n",
    "    if col in issues_df.columns:\n",
    "        # Extraer números de strings como \"5min\", \"10min\", etc.\n",
    "        issues_df[f'{col}_minutes'] = issues_df[col].str.extract('(\\d+)').astype(float)\n",
    "        issues_df[f'{col}_minutes'] = issues_df[f'{col}_minutes'].fillna(0)\n",
    "        print(f\"✅ {col} convertido a minutos numéricos\")\n",
    "\n",
    "# Verificar rangos de datos\n",
    "print(f\"\\n📊 RANGOS DE DATOS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Rango de fechas de creación: {issues_df['creation_date'].min()} a {issues_df['creation_date'].max()}\")\n",
    "print(f\"Rango de líneas de código: {issues_df['line'].min()} a {issues_df['line'].max()}\")\n",
    "print(f\"Effort range: {issues_df['effort_minutes'].min()} a {issues_df['effort_minutes'].max()} minutos\")\n",
    "print(f\"Debt range: {issues_df['debt_minutes'].min()} a {issues_df['debt_minutes'].max()} minutos\")\n",
    "\n",
    "print(\"\\n✅ Limpieza de datos completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c14b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CREACIÓN DE CARACTERÍSTICAS DERIVADAS\n",
    "# ===================================================================\n",
    "\n",
    "print(\"🔧 CREACIÓN DE CARACTERÍSTICAS DERIVADAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Extraer tipo de archivo de component\n",
    "issues_df['file_extension'] = issues_df['component'].str.extract(r'\\.([^.]+)$')\n",
    "issues_df['file_extension'] = issues_df['file_extension'].fillna('unknown')\n",
    "\n",
    "# 2. Extraer namespace/carpeta principal\n",
    "issues_df['namespace'] = issues_df['component'].str.extract(r':([^/]+)')\n",
    "issues_df['folder_level_1'] = issues_df['component'].str.extract(r':([^/]+/[^/]+)')\n",
    "\n",
    "# 3. Categorizar familias de reglas\n",
    "def categorize_rule_family(rule):\n",
    "    if pd.isna(rule):\n",
    "        return 'unknown'\n",
    "    elif rule.startswith('external_roslyn:CS'):\n",
    "        return 'roslyn_csharp'\n",
    "    elif rule.startswith('csharpsquid:S'):\n",
    "        return 'sonarqube_csharp'\n",
    "    elif rule.startswith('css:S'):\n",
    "        return 'css'\n",
    "    elif rule.startswith('typescript:S'):\n",
    "        return 'typescript'\n",
    "    elif rule.startswith('javascript:S'):\n",
    "        return 'javascript'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "issues_df['rule_family'] = issues_df['rule'].apply(categorize_rule_family)\n",
    "\n",
    "# 4. Mapear severidad a valores numéricos para análisis\n",
    "severity_mapping = {\n",
    "    'CRITICAL': 4,\n",
    "    'MAJOR': 3,\n",
    "    'MINOR': 2,\n",
    "    'INFO': 1\n",
    "}\n",
    "issues_df['severity_numeric'] = issues_df['severity'].map(severity_mapping)\n",
    "\n",
    "# 5. Crear indicadores binarios para tipos de issues\n",
    "for issue_type in issues_df['type'].unique():\n",
    "    issues_df[f'is_{issue_type.lower()}'] = (issues_df['type'] == issue_type).astype(int)\n",
    "\n",
    "# 6. Extraer año y mes de creación\n",
    "issues_df['creation_year'] = issues_df['creation_date'].dt.year\n",
    "issues_df['creation_month'] = issues_df['creation_date'].dt.month\n",
    "\n",
    "# 7. Calcular métricas agregadas por estudiante\n",
    "print(\"📊 Calculando métricas por estudiante...\")\n",
    "\n",
    "student_metrics = issues_df.groupby(['student_id', 'assignment']).agg({\n",
    "    'issue_key': 'count',\n",
    "    'severity_numeric': ['mean', 'sum'],\n",
    "    'effort_minutes': 'sum',\n",
    "    'debt_minutes': 'sum',\n",
    "    'type': lambda x: x.value_counts().to_dict(),\n",
    "    'rule_family': lambda x: x.value_counts().to_dict(),\n",
    "    'file_extension': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "student_metrics.columns = [f'{col[0]}_{col[1]}' if col[1] else col[0] for col in student_metrics.columns]\n",
    "student_metrics = student_metrics.rename(columns={\n",
    "    'issue_key_count': 'total_issues',\n",
    "    'severity_numeric_mean': 'avg_severity',\n",
    "    'severity_numeric_sum': 'total_severity_score',\n",
    "    'effort_minutes_sum': 'total_effort_minutes',\n",
    "    'debt_minutes_sum': 'total_debt_minutes',\n",
    "    'file_extension_nunique': 'files_with_issues'\n",
    "})\n",
    "\n",
    "# 8. Crear dataset de métricas por estudiante en formato amplio\n",
    "student_summary = issues_df.pivot_table(\n",
    "    index=['student_id', 'nombre', 'assignment'],\n",
    "    values='issue_key',\n",
    "    columns='type',\n",
    "    aggfunc='count',\n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "# Aplanar nombres de columnas\n",
    "student_summary.columns.name = None\n",
    "\n",
    "print(\"✅ Características derivadas creadas:\")\n",
    "print(f\"   📁 Extensiones de archivo: {issues_df['file_extension'].nunique()} tipos\")\n",
    "print(f\"   🏗️ Familias de reglas: {issues_df['rule_family'].nunique()} familias\")\n",
    "print(f\"   📊 Métricas por estudiante calculadas para {student_summary.shape[0]} registros\")\n",
    "\n",
    "# Mostrar ejemplo de métricas por estudiante\n",
    "print(f\"\\n📋 EJEMPLO DE MÉTRICAS POR ESTUDIANTE\")\n",
    "print(\"=\"*60)\n",
    "student_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8be8f6",
   "metadata": {},
   "source": [
    "# 2. 📊 Exploratory Data Analysis (EDA)\n",
    "\n",
    "En esta sección realizamos un análisis exploratorio comprehensivo de los datos de issues, examinando distribuciones, patrones temporales y características principales del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ece131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# DISTRIBUCIÓN GENERAL DE ISSUES\n",
    "# ===================================================================\n",
    "\n",
    "print(\"📊 ANÁLISIS DE DISTRIBUCIÓN GENERAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Estadísticas por asignatura\n",
    "assignment_stats = issues_df.groupby('assignment').agg({\n",
    "    'issue_key': 'count',\n",
    "    'student_id': 'nunique',\n",
    "    'severity_numeric': 'mean',\n",
    "    'effort_minutes': 'sum',\n",
    "    'debt_minutes': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "print(\"Estadísticas por asignatura:\")\n",
    "print(assignment_stats)\n",
    "\n",
    "# Distribución por tipo de issue\n",
    "print(f\"\\n📋 DISTRIBUCIÓN POR TIPO DE ISSUE\")\n",
    "type_dist = issues_df['type'].value_counts()\n",
    "type_pct = issues_df['type'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Conteo y porcentaje por tipo:\")\n",
    "for issue_type in type_dist.index:\n",
    "    print(f\"   {issue_type}: {type_dist[issue_type]:,} ({type_pct[issue_type]:.1f}%)\")\n",
    "\n",
    "# Distribución por severidad\n",
    "print(f\"\\n⚠️ DISTRIBUCIÓN POR SEVERIDAD\")\n",
    "severity_dist = issues_df['severity'].value_counts()\n",
    "severity_pct = issues_df['severity'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Conteo y porcentaje por severidad:\")\n",
    "for severity in ['CRITICAL', 'MAJOR', 'MINOR', 'INFO']:\n",
    "    if severity in severity_dist.index:\n",
    "        print(f\"   {severity}: {severity_dist[severity]:,} ({severity_pct[severity]:.1f}%)\")\n",
    "\n",
    "# Crear visualización de distribuciones\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('📊 Distribución General de Issues', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Issues por asignatura\n",
    "assignment_counts = issues_df['assignment'].value_counts()\n",
    "axes[0,0].bar(assignment_counts.index, assignment_counts.values, color=['#1f77b4', '#ff7f0e'])\n",
    "axes[0,0].set_title('Issues por Asignatura')\n",
    "axes[0,0].set_ylabel('Número de Issues')\n",
    "for i, v in enumerate(assignment_counts.values):\n",
    "    axes[0,0].text(i, v + 50, f'{v:,}', ha='center', fontweight='bold')\n",
    "\n",
    "# 2. Issues por tipo\n",
    "type_counts = issues_df['type'].value_counts()\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(type_counts)))\n",
    "axes[0,1].bar(range(len(type_counts)), type_counts.values, color=colors)\n",
    "axes[0,1].set_title('Issues por Tipo')\n",
    "axes[0,1].set_ylabel('Número de Issues')\n",
    "axes[0,1].set_xticks(range(len(type_counts)))\n",
    "axes[0,1].set_xticklabels(type_counts.index, rotation=45, ha='right')\n",
    "for i, v in enumerate(type_counts.values):\n",
    "    axes[0,1].text(i, v + 50, f'{v:,}', ha='center', fontweight='bold')\n",
    "\n",
    "# 3. Issues por severidad\n",
    "severity_order = ['CRITICAL', 'MAJOR', 'MINOR', 'INFO']\n",
    "severity_counts = issues_df['severity'].value_counts().reindex(severity_order)\n",
    "severity_colors = ['#d62728', '#ff7f0e', '#ffbb78', '#2ca02c']\n",
    "axes[1,0].bar(severity_counts.index, severity_counts.values, color=severity_colors)\n",
    "axes[1,0].set_title('Issues por Severidad')\n",
    "axes[1,0].set_ylabel('Número de Issues')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(severity_counts.values):\n",
    "    axes[1,0].text(i, v + 50, f'{v:,}', ha='center', fontweight='bold')\n",
    "\n",
    "# 4. Issues por estudiante (distribución)\n",
    "issues_per_student = issues_df.groupby('student_id').size()\n",
    "axes[1,1].hist(issues_per_student, bins=20, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "axes[1,1].set_title('Distribución de Issues por Estudiante')\n",
    "axes[1,1].set_xlabel('Número de Issues por Estudiante')\n",
    "axes[1,1].set_ylabel('Número de Estudiantes')\n",
    "axes[1,1].axvline(issues_per_student.mean(), color='red', linestyle='--', \n",
    "                  label=f'Media: {issues_per_student.mean():.1f}')\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "print(f\"\\n📈 ESTADÍSTICAS DESCRIPTIVAS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Issues por estudiante:\")\n",
    "print(f\"   Media: {issues_per_student.mean():.1f}\")\n",
    "print(f\"   Mediana: {issues_per_student.median():.1f}\")\n",
    "print(f\"   Desviación estándar: {issues_per_student.std():.1f}\")\n",
    "print(f\"   Mínimo: {issues_per_student.min()}\")\n",
    "print(f\"   Máximo: {issues_per_student.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36e1c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# ANÁLISIS TEMPORAL DE ISSUES\n",
    "# ===================================================================\n",
    "\n",
    "print(\"📅 ANÁLISIS TEMPORAL DE ISSUES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Análisis por fecha de creación\n",
    "if 'creation_date' in issues_df.columns:\n",
    "    issues_df['creation_date_only'] = issues_df['creation_date'].dt.date\n",
    "    \n",
    "    # Issues por día\n",
    "    daily_issues = issues_df.groupby(['creation_date_only', 'assignment']).size().reset_index(name='count')\n",
    "    \n",
    "    # Crear visualización temporal\n",
    "    fig = px.line(daily_issues, x='creation_date_only', y='count', color='assignment',\n",
    "                  title='📅 Evolución Temporal de Issues por Asignatura',\n",
    "                  labels={'creation_date_only': 'Fecha de Creación', 'count': 'Número de Issues'})\n",
    "    fig.update_layout(height=500)\n",
    "    fig.show()\n",
    "    \n",
    "    # Estadísticas temporales por asignatura\n",
    "    temporal_stats = issues_df.groupby('assignment')['creation_date'].agg(['min', 'max', 'count'])\n",
    "    print(\"Estadísticas temporales por asignatura:\")\n",
    "    print(temporal_stats)\n",
    "    \n",
    "    # Análisis por mes\n",
    "    issues_df['year_month'] = issues_df['creation_date'].dt.to_period('M')\n",
    "    monthly_issues = issues_df.groupby(['year_month', 'assignment']).size().reset_index(name='count')\n",
    "    monthly_issues['year_month_str'] = monthly_issues['year_month'].astype(str)\n",
    "    \n",
    "    print(f\"\\nIssues por mes y asignatura:\")\n",
    "    monthly_pivot = monthly_issues.pivot(index='year_month_str', columns='assignment', values='count').fillna(0)\n",
    "    print(monthly_pivot)\n",
    "\n",
    "# Análisis de status de issues\n",
    "print(f\"\\n🔄 ANÁLISIS DE STATUS DE ISSUES\")\n",
    "print(\"=\"*60)\n",
    "status_dist = issues_df['status'].value_counts()\n",
    "status_pct = issues_df['status'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Distribución de status:\")\n",
    "for status in status_dist.index:\n",
    "    print(f\"   {status}: {status_dist[status]:,} ({status_pct[status]:.1f}%)\")\n",
    "\n",
    "# Cross-tabulation de assignment vs tipo de issue\n",
    "print(f\"\\n📊 CROSSTAB: ASIGNATURA vs TIPO DE ISSUE\")\n",
    "print(\"=\"*60)\n",
    "crosstab_assignment_type = pd.crosstab(issues_df['assignment'], issues_df['type'], margins=True)\n",
    "print(crosstab_assignment_type)\n",
    "\n",
    "# Porcentajes por fila\n",
    "print(f\"\\nPorcentajes por asignatura:\")\n",
    "crosstab_pct = pd.crosstab(issues_df['assignment'], issues_df['type'], normalize='index') * 100\n",
    "print(crosstab_pct.round(1))\n",
    "\n",
    "# Visualización de heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(crosstab_pct, annot=True, fmt='.1f', cmap='YlOrRd', \n",
    "            cbar_kws={'label': 'Porcentaje'})\n",
    "plt.title('🔥 Heatmap: Distribución de Tipos de Issues por Asignatura (%)')\n",
    "plt.xlabel('Tipo de Issue')\n",
    "plt.ylabel('Asignatura')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
