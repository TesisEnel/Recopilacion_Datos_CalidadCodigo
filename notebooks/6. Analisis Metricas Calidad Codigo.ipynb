{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2ecbd5e",
   "metadata": {},
   "source": [
    "# Jupyter Notebook para Análisis Estadístico de Métricas de Calidad de Código (AP1 vs AP2)\n",
    "\n",
    "Este notebook está diseñado para realizar un análisis estadístico pareado de la evolución de las métricas de calidad del código entre dos asignaturas del programa (AP1 y AP2).\n",
    "\n",
    "## Objetivos:\n",
    "\n",
    "1. **Cargar datos de métricas de calidad** extraídas de SonarCloud para estudiantes en AP1 y AP2\n",
    "2. **Realizar análisis estadístico pareado** utilizando pruebas t de Student o Wilcoxon según normalidad\n",
    "3. **Calcular tamaños de efecto** (Cohen's d) para cuantificar la magnitud de los cambios\n",
    "4. **Aplicar corrección FDR** (False Discovery Rate) para comparaciones múltiples\n",
    "5. **Generar visualizaciones** comparativas: boxplots, gráficos de evolución pareada, matriz de correlaciones\n",
    "6. **Producir reportes** en formato Markdown con interpretación de resultados\n",
    "\n",
    "## Contenido:\n",
    "- Pruebas estadísticas pareadas (t de Student o Wilcoxon según normalidad)\n",
    "- Cálculo de tamaños de efecto (Cohen's d)\n",
    "- Corrección por comparaciones múltiples (FDR Benjamini-Hochberg)\n",
    "- Visualizaciones: boxplots, gráficos de evolución pareada, matriz de correlaciones\n",
    "- Generación de reportes en Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072d8418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from glob import glob\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa49134",
   "metadata": {},
   "source": [
    "## 2. Configuración de Métricas y Parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e10e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas a analizar\n",
    "METRICS_BASE = [\n",
    "    \"code_smells\",\"bugs\",\"vulnerabilities\",\"security_hotspots\",\n",
    "    \"cognitive_complexity\",\"complexity\",\"ncloc\",\"reliability_rating\",\n",
    "    \"security_rating\",\"open_issues\",\n",
    "]\n",
    "\n",
    "# Clasificación de métricas\n",
    "LOWER_IS_BETTER = {\n",
    "    \"code_smells\",\"bugs\",\"vulnerabilities\",\"security_hotspots\",\n",
    "    \"cognitive_complexity\",\"complexity\",\"reliability_rating\",\n",
    "    \"security_rating\",\"open_issues\"\n",
    "}\n",
    "NEUTRAL = set(METRICS_BASE) - LOWER_IS_BETTER\n",
    "\n",
    "# Parámetros de entrada\n",
    "CSV_PATH = \"https://raw.githubusercontent.com/TesisEnel/Recopilacion_Datos_CalidadCodigo/refs/heads/main/data/Estudiantes_2023-2024_con_metricas_sonarcloud.csv\"\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "\n",
    "print(f\"Métricas a analizar: {len(METRICS_BASE)}\")\n",
    "print(f\"Métricas donde menor es mejor: {len(LOWER_IS_BETTER)}\")\n",
    "print(f\"Métricas neutrales: {NEUTRAL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0558c8e0",
   "metadata": {},
   "source": [
    "## 3. Definición de Clases y Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37246929",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MetricResult:\n",
    "    metric: str\n",
    "    test_used: str\n",
    "    n_paired: int\n",
    "    mean_ap1: float\n",
    "    mean_ap2: float\n",
    "    delta: float\n",
    "    pct_change: float | None\n",
    "    p_value: float\n",
    "    effect_size_d: float | None\n",
    "    effect_magnitude: str | None\n",
    "    direction: str\n",
    "    improved: str\n",
    "    normality_p: float | None\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"metric\": self.metric,\n",
    "            \"test_used\": self.test_used,\n",
    "            \"n_paired\": self.n_paired,\n",
    "            \"mean_ap1\": self.mean_ap1,\n",
    "            \"mean_ap2\": self.mean_ap2,\n",
    "            \"delta_ap2_minus_ap1\": self.delta,\n",
    "            \"pct_change\": self.pct_change,\n",
    "            \"p_value\": self.p_value,\n",
    "            \"effect_size_d\": self.effect_size_d,\n",
    "            \"effect_magnitude\": self.effect_magnitude,\n",
    "            \"direction\": self.direction,\n",
    "            \"improved\": self.improved,\n",
    "            \"normality_p\": self.normality_p\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6be91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohen_d_paired(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"Calcula el tamaño de efecto Cohen's d para datos pareados\"\"\"\n",
    "    diff = b - a\n",
    "    sd = diff.std(ddof=1)\n",
    "    return 0.0 if sd == 0 else diff.mean() / sd\n",
    "\n",
    "def classify_effect_size(d: float) -> str:\n",
    "    \"\"\"Clasifica el tamaño de efecto según Cohen\"\"\"\n",
    "    ad = abs(d)\n",
    "    if ad < 0.2:\n",
    "        return \"trivial\"\n",
    "    elif ad < 0.5:\n",
    "        return \"pequeño\"\n",
    "    elif ad < 0.8:\n",
    "        return \"mediano\"\n",
    "    else:\n",
    "        return \"grande\"\n",
    "\n",
    "def infer_direction(metric: str) -> str:\n",
    "    \"\"\"Determina la dirección de mejora para una métrica\"\"\"\n",
    "    if metric in LOWER_IS_BETTER:\n",
    "        return \"lower_better\"\n",
    "    return \"neutral\"\n",
    "\n",
    "def compute_improvement(metric: str, mean_ap1: float, mean_ap2: float) -> str:\n",
    "    \"\"\"Determina si hubo mejora según la dirección de la métrica\"\"\"\n",
    "    d = infer_direction(metric)\n",
    "    if d == \"lower_better\" and mean_ap2 < mean_ap1:\n",
    "        return \"Yes\"\n",
    "    elif d == \"neutral\":\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"No\"\n",
    "\n",
    "def safe_pct_change(ap1: float, ap2: float) -> float | None:\n",
    "    \"\"\"Calcula cambio porcentual de forma segura\"\"\"\n",
    "    return None if ap1 == 0 else (ap2 - ap1) / ap1 * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed259554",
   "metadata": {},
   "source": [
    "## 4. Carga y Preparación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e9ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Carga el dataset y convierte las métricas a formato numérico\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    for m in METRICS_BASE:\n",
    "        for suf in (\"AP1\", \"AP2\"):\n",
    "            col = f\"{m}_{suf}\"\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "# Cargar datos\n",
    "df = load_dataset(CSV_PATH)\n",
    "print(f\"Dataset cargado: {df.shape[0]} estudiantes\")\n",
    "print(f\"\\nPrimeras columnas: {list(df.columns[:10])}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83371666",
   "metadata": {},
   "source": [
    "## 5. Análisis Estadístico por Métrica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6fbf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_metric(df: pd.DataFrame, metric: str) -> MetricResult:\n",
    "    \"\"\"Realiza análisis estadístico pareado para una métrica\"\"\"\n",
    "    col_ap1 = f\"{metric}_AP1\"\n",
    "    col_ap2 = f\"{metric}_AP2\"\n",
    "    \n",
    "    # Verificar que existan las columnas\n",
    "    if col_ap1 not in df.columns or col_ap2 not in df.columns:\n",
    "        return MetricResult(\n",
    "            metric, \"NA\", 0, np.nan, np.nan, np.nan, None, np.nan, None, \n",
    "            infer_direction(metric), \"NA\", None, None\n",
    "        )\n",
    "    \n",
    "    # Preparar datos pareados\n",
    "    data = df[[col_ap1, col_ap2]].dropna()\n",
    "    a = data[col_ap1].values\n",
    "    b = data[col_ap2].values\n",
    "    n = len(data)\n",
    "    \n",
    "    # Verificar tamaño mínimo de muestra\n",
    "    if n < 3:\n",
    "        return MetricResult(\n",
    "            metric, \"Insuficiente\", n, float(\"nan\"), float(\"nan\"), float(\"nan\"),\n",
    "            None, float(\"nan\"), None, infer_direction(metric), \"NA\", None, None\n",
    "        )\n",
    "    \n",
    "    # Test de normalidad en las diferencias\n",
    "    diffs = b - a\n",
    "    try:\n",
    "        _, p_norm = stats.shapiro(diffs)\n",
    "    except Exception:\n",
    "        p_norm = None\n",
    "    \n",
    "    # Seleccionar prueba estadística\n",
    "    if p_norm is not None and p_norm > 0.05:\n",
    "        _, p_val = stats.ttest_rel(a, b, nan_policy='omit')\n",
    "        test_used = \"paired_t\"\n",
    "    else:\n",
    "        if np.allclose(diffs, 0):\n",
    "            p_val = 1.0\n",
    "            test_used = \"wilcoxon_allzero\"\n",
    "        else:\n",
    "            try:\n",
    "                _, p_val = stats.wilcoxon(a, b, zero_method='wilcox', alternative='two-sided')\n",
    "                test_used = \"wilcoxon\"\n",
    "            except ValueError:\n",
    "                p_val = 1.0\n",
    "                test_used = \"wilcoxon_error\"\n",
    "    \n",
    "    # Calcular estadísticos\n",
    "    mean_ap1 = float(np.mean(a))\n",
    "    mean_ap2 = float(np.mean(b))\n",
    "    delta = mean_ap2 - mean_ap1\n",
    "    pct = safe_pct_change(mean_ap1, mean_ap2)\n",
    "    d = cohen_d_paired(a, b)\n",
    "    magnitude = classify_effect_size(d)\n",
    "    improved = compute_improvement(metric, mean_ap1, mean_ap2)\n",
    "    \n",
    "    return MetricResult(\n",
    "        metric, test_used, n, mean_ap1, mean_ap2, delta, pct, p_val, d,\n",
    "        magnitude, infer_direction(metric), improved, p_norm\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca66050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ejecuta el análisis para todas las métricas y aplica corrección FDR\"\"\"\n",
    "    res = [analyze_metric(df, m) for m in METRICS_BASE]\n",
    "    res_df = pd.DataFrame([r.to_dict() for r in res])\n",
    "    \n",
    "    # Aplicar corrección FDR\n",
    "    mask = res_df[\"p_value\"].notna()\n",
    "    pvals = res_df.loc[mask, \"p_value\"].values\n",
    "    \n",
    "    if len(pvals) > 0:\n",
    "        rejected, p_corr, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n",
    "        res_df.loc[mask, \"p_value_fdr\"] = p_corr\n",
    "        res_df.loc[mask, \"significant_raw\"] = res_df.loc[mask, \"p_value\"] < 0.05\n",
    "        res_df.loc[mask, \"significant_fdr\"] = rejected\n",
    "    \n",
    "    return res_df\n",
    "\n",
    "# Ejecutar análisis\n",
    "print(\"Ejecutando análisis estadístico...\")\n",
    "res_df = run_analysis(df)\n",
    "print(\"✓ Análisis completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3b4ed9",
   "metadata": {},
   "source": [
    "## 6. Resultados del Análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d507b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar por p-valor corregido\n",
    "res_sorted = res_df.sort_values(\"p_value_fdr\") if \"p_value_fdr\" in res_df.columns else res_df.sort_values(\"p_value\")\n",
    "\n",
    "# Mostrar resultados principales\n",
    "cols_show = [\n",
    "    \"metric\", \"n_paired\", \"mean_ap1\", \"mean_ap2\", \"delta_ap2_minus_ap1\", \n",
    "    \"pct_change\", \"test_used\", \"p_value\", \"p_value_fdr\", \n",
    "    \"effect_size_d\", \"effect_magnitude\", \"improved\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN DE MÉTRICAS (ordenadas por p-valor corregido)\")\n",
    "print(\"=\"*80)\n",
    "display(res_sorted[cols_show])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e6084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen estadístico\n",
    "sig = res_sorted[res_sorted.significant_fdr == True]\n",
    "improved = sig[sig.improved == 'Yes']\n",
    "worsened = sig[sig.improved == 'No']\n",
    "neutral = sig[sig.improved == 'Neutral']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN EJECUTIVO\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total de métricas analizadas: {len(res_sorted)}\")\n",
    "print(f\"Métricas con cambios significativos (FDR ≤ 0.05): {len(sig)}\")\n",
    "print(f\"\\n  → Mejoras significativas: {len(improved)}\")\n",
    "if len(improved) > 0:\n",
    "    print(f\"     {', '.join(improved.metric.tolist())}\")\n",
    "print(f\"\\n  → Deterioros significativos: {len(worsened)}\")\n",
    "if len(worsened) > 0:\n",
    "    print(f\"     {', '.join(worsened.metric.tolist())}\")\n",
    "print(f\"\\n  → Cambios neutrales: {len(neutral)}\")\n",
    "if len(neutral) > 0:\n",
    "    print(f\"     {', '.join(neutral.metric.tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66fd568",
   "metadata": {},
   "source": [
    "## 7. Guardar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d932171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear directorio de salida\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Guardar CSVs\n",
    "out_raw = os.path.join(OUTPUT_DIR, \"resultados_metricas.csv\")\n",
    "out_fdr = os.path.join(OUTPUT_DIR, \"resultados_metricas_fdr.csv\")\n",
    "\n",
    "res_df.to_csv(out_raw, index=False)\n",
    "res_sorted.to_csv(out_fdr, index=False)\n",
    "\n",
    "print(f\"✓ Resultados guardados:\")\n",
    "print(f\"  - {out_raw}\")\n",
    "print(f\"  - {out_fdr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2505a1dc",
   "metadata": {},
   "source": [
    "## 8. Visualizaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391e4cc2",
   "metadata": {},
   "source": [
    "### 8.1 Boxplots Comparativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06447043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplots(df: pd.DataFrame, outdir: str, metrics: List[str]):\n",
    "    \"\"\"Genera boxplots comparativos para todas las métricas\"\"\"\n",
    "    rows = []\n",
    "    for m in metrics:\n",
    "        c1, c2 = f\"{m}_AP1\", f\"{m}_AP2\"\n",
    "        if c1 in df.columns and c2 in df.columns:\n",
    "            sub = df[[c1, c2]].copy()\n",
    "            sub.columns = [\"AP1\", \"AP2\"]\n",
    "            if sub.dropna().empty:\n",
    "                continue\n",
    "            long = sub.melt(var_name=\"asignatura\", value_name=\"valor\")\n",
    "            long[\"metric\"] = m\n",
    "            rows.append(long)\n",
    "    \n",
    "    if not rows:\n",
    "        return\n",
    "    \n",
    "    long_df = pd.concat(rows, ignore_index=True)\n",
    "    metrics_present = sorted(long_df[\"metric\"].unique())\n",
    "    n_metrics = len(metrics_present)\n",
    "    ncols = 4\n",
    "    nrows = int(np.ceil(n_metrics / ncols))\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(4*ncols, 4*nrows))\n",
    "    axes = np.atleast_2d(axes).reshape(nrows, ncols)\n",
    "    \n",
    "    for ax, metric in zip(axes.flat, metrics_present):\n",
    "        g = long_df[long_df.metric == metric]\n",
    "        sns.boxplot(data=g, x=\"asignatura\", y=\"valor\", ax=ax)\n",
    "        sns.stripplot(data=g, x=\"asignatura\", y=\"valor\", ax=ax, \n",
    "                     color=\"#555\", alpha=0.4, jitter=0.2, size=3)\n",
    "        ax.set_title(metric)\n",
    "    \n",
    "    # Desactivar ejes sobrantes\n",
    "    for ax in axes.flat[len(metrics_present):]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    out = os.path.join(outdir, \"fig_boxplots.png\")\n",
    "    plt.savefig(out, dpi=150)\n",
    "    print(f\"✓ Boxplots guardados: {out}\")\n",
    "    plt.show()\n",
    "\n",
    "plot_boxplots(df, OUTPUT_DIR, METRICS_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a2cb8f",
   "metadata": {},
   "source": [
    "### 8.2 Gráficos de Evolución Pareada (Spaghetti Plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spaghetti(df: pd.DataFrame, outdir: str, metrics: List[str], max_plots: int = 6):\n",
    "    \"\"\"Genera gráficos de evolución pareada para las métricas más relevantes\"\"\"\n",
    "    count = 0\n",
    "    for m in metrics:\n",
    "        if count >= max_plots:\n",
    "            break\n",
    "            \n",
    "        c1, c2 = f\"{m}_AP1\", f\"{m}_AP2\"\n",
    "        if c1 not in df.columns or c2 not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        sub = df[[c1, c2]].dropna()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "        x = [1, 2]\n",
    "        \n",
    "        # Líneas de conexión\n",
    "        for _, row in sub.iterrows():\n",
    "            ax.plot(x, [row[c1], row[c2]], color=\"#999\", alpha=0.5, linewidth=0.8)\n",
    "        \n",
    "        # Puntos\n",
    "        ax.scatter([1]*len(sub), sub[c1], color=\"#1f77b4\", label=\"AP1\", s=40, alpha=0.7)\n",
    "        ax.scatter([2]*len(sub), sub[c2], color=\"#ff7f0e\", label=\"AP2\", s=40, alpha=0.7)\n",
    "        \n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([\"AP1\", \"AP2\"])\n",
    "        ax.set_title(f\"Evolución pareada: {m}\", fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel(\"Valor\", fontsize=10)\n",
    "        ax.grid(alpha=0.3, linestyle='--')\n",
    "        ax.legend(frameon=False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        out = os.path.join(outdir, f\"fig_spaghetti_{m}.png\")\n",
    "        plt.savefig(out, dpi=130)\n",
    "        plt.show()\n",
    "        count += 1\n",
    "    \n",
    "    print(f\"\\n✓ {count} gráficos de evolución pareada generados\")\n",
    "\n",
    "# Generar para las 6 métricas más relevantes (las que mostraron cambios significativos)\n",
    "top_metrics = res_sorted.head(6).metric.tolist()\n",
    "plot_spaghetti(df, OUTPUT_DIR, top_metrics, max_plots=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaf0f60",
   "metadata": {},
   "source": [
    "### 8.3 Matriz de Correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1be3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_heatmap(df: pd.DataFrame, outdir: str, metrics: List[str]):\n",
    "    \"\"\"Genera matriz de correlaciones entre métricas AP1 y AP2\"\"\"\n",
    "    cols = []\n",
    "    for m in metrics:\n",
    "        for suf in (\"AP1\", \"AP2\"):\n",
    "            c = f\"{m}_{suf}\"\n",
    "            if c in df.columns:\n",
    "                cols.append(c)\n",
    "    \n",
    "    if not cols:\n",
    "        return\n",
    "    \n",
    "    corr_df = df[cols].copy()\n",
    "    if corr_df.empty:\n",
    "        return\n",
    "    \n",
    "    corr = corr_df.corr()\n",
    "    \n",
    "    fig_size = min(1 + 0.5 * len(corr.columns), 18)\n",
    "    plt.figure(figsize=(fig_size, fig_size))\n",
    "    \n",
    "    sns.heatmap(corr, cmap=\"coolwarm\", center=0, annot=False, \n",
    "                linewidths=0.3, cbar_kws={'label': 'Correlación'})\n",
    "    plt.title(\"Matriz de Correlaciones (AP1 & AP2)\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    out = os.path.join(outdir, \"fig_heatmap_correlaciones.png\")\n",
    "    plt.savefig(out, dpi=160)\n",
    "    print(f\"✓ Matriz de correlaciones guardada: {out}\")\n",
    "    plt.show()\n",
    "\n",
    "plot_correlation_heatmap(df, OUTPUT_DIR, METRICS_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f855c60d",
   "metadata": {},
   "source": [
    "## 9. Generación de Reportes en Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2cbf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_pct(x):\n",
    "    \"\"\"Formatea un valor como porcentaje\"\"\"\n",
    "    try:\n",
    "        if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "            return \"NA\"\n",
    "        return f\"{float(x):.1f}%\"\n",
    "    except Exception:\n",
    "        return \"NA\"\n",
    "\n",
    "def generate_markdown_report(res_df: pd.DataFrame, outdir: str, metrics: List[str], csv_path: str):\n",
    "    \"\"\"Genera reporte detallado en Markdown\"\"\"\n",
    "    ts = datetime.datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "    out_path = os.path.join(outdir, 'reporte_metricas.md')\n",
    "    \n",
    "    df = res_df[res_df.metric.isin(metrics)].copy()\n",
    "    sig = df[df.significant_fdr == True]\n",
    "    improved_sig = sig[sig.improved == 'Yes']\n",
    "    worsened_sig = sig[sig.improved == 'No']\n",
    "    \n",
    "    df['abs_d'] = df['effect_size_d'].abs()\n",
    "    top_effect = df.sort_values('abs_d', ascending=False).head(5)\n",
    "    \n",
    "    lines = []\n",
    "    lines.append(f\"# Reporte Estadístico de Métricas AP1 vs AP2\\n\")\n",
    "    lines.append(f\"Generado: {ts}\\n\")\n",
    "    lines.append(f\"Fuente CSV: `{csv_path}`\\n\")\n",
    "    lines.append(\"## Resumen Global\\n\")\n",
    "    \n",
    "    total = len(df)\n",
    "    sig_n = sig.shape[0]\n",
    "    lines.append(f\"Se analizaron {total} métricas. {sig_n} resultaron significativas tras corrección FDR (α=0.05).\\n\")\n",
    "    \n",
    "    if improved_sig.shape[0] > 0:\n",
    "        lines.append(f\"- Mejoras significativas: {improved_sig.shape[0]} -> {', '.join(improved_sig.metric)}\")\n",
    "    if worsened_sig.shape[0] > 0:\n",
    "        lines.append(f\"- Deterioros significativos: {worsened_sig.shape[0]} -> {', '.join(worsened_sig.metric)}\")\n",
    "    \n",
    "    neutrals = sig[sig.improved == 'Neutral']\n",
    "    if neutrals.shape[0] > 0:\n",
    "        lines.append(f\"- Cambios significativos pero neutros (contexto): {neutrals.shape[0]} -> {', '.join(neutrals.metric)}\")\n",
    "    \n",
    "    lines.append(\"\\n## Principales Cambios (Top |d|)\\n\")\n",
    "    for _, r in top_effect.iterrows():\n",
    "        direction = '↓' if r.direction == 'lower_better' and r.mean_ap2 < r.mean_ap1 else ('↑' if r.direction == 'higher_better' and r.mean_ap2 > r.mean_ap1 else '↔')\n",
    "        lines.append(f\"- {r.metric}: d={r.effect_size_d:.3f} ({r.effect_magnitude}), p_FDR={r.p_value_fdr if not pd.isna(r.p_value_fdr) else r.p_value:.3g}, {direction} cambio relativo {format_pct(r.pct_change)} (AP1={r.mean_ap1:.3g}, AP2={r.mean_ap2:.3g}) -> Improved={r.improved}\")\n",
    "    \n",
    "    lines.append(\"\\n## Tabla Detallada\\n\")\n",
    "    show_cols = [\"metric\", \"mean_ap1\", \"mean_ap2\", \"pct_change\", \"test_used\", \"p_value\", \"p_value_fdr\", \"effect_size_d\", \"effect_magnitude\", \"improved\"]\n",
    "    header = '|' + '|'.join(show_cols) + '|'\n",
    "    sep = '|' + '|'.join(['---'] * len(show_cols)) + '|'\n",
    "    lines.append(header)\n",
    "    lines.append(sep)\n",
    "    \n",
    "    for _, r in df.sort_values('p_value_fdr').iterrows():\n",
    "        def safe(v, fmt=None):\n",
    "            if v is None or (isinstance(v, float) and pd.isna(v)):\n",
    "                return 'NA'\n",
    "            try:\n",
    "                return fmt.format(v) if fmt else str(v)\n",
    "            except Exception:\n",
    "                return str(v)\n",
    "        \n",
    "        lines.append('|' + '|'.join([\n",
    "            r.metric,\n",
    "            safe(r.mean_ap1, \"{:.3g}\"),\n",
    "            safe(r.mean_ap2, \"{:.3g}\"),\n",
    "            format_pct(r.pct_change),\n",
    "            r.test_used if isinstance(r.test_used, str) else 'NA',\n",
    "            safe(r.p_value, \"{:.3g}\"),\n",
    "            safe(r.p_value_fdr, \"{:.3g}\"),\n",
    "            safe(r.effect_size_d, \"{:.3g}\"),\n",
    "            r.effect_magnitude if isinstance(r.effect_magnitude, str) else 'NA',\n",
    "            r.improved if isinstance(r.improved, str) else 'NA'\n",
    "        ]) + '|')\n",
    "    \n",
    "    lines.append(\"\\n## Interpretación General\\n\")\n",
    "    if improved_sig.shape[0] > 0:\n",
    "        lines.append(f\"Las métricas con mejoras significativas muestran evidencia de impacto positivo (ej. {', '.join(improved_sig.metric[:3])}{'...' if improved_sig.shape[0] > 3 else ''}).\")\n",
    "    if worsened_sig.shape[0] > 0:\n",
    "        lines.append(f\"Atención: algunas métricas empeoraron significativamente (ej. {', '.join(worsened_sig.metric[:3])}).\")\n",
    "    lines.append(\"Los tamaños de efecto clasificados como medianos indican cambios sustanciales prácticos; revisar contexto pedagógico.\")\n",
    "    \n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(lines))\n",
    "    \n",
    "    return out_path\n",
    "\n",
    "# Generar reporte\n",
    "md_path = generate_markdown_report(res_sorted, OUTPUT_DIR, METRICS_BASE, CSV_PATH)\n",
    "print(f\"✓ Reporte Markdown generado: {md_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d95e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_executive_summary(res_df: pd.DataFrame, outdir: str):\n",
    "    \"\"\"Genera resumen ejecutivo\"\"\"\n",
    "    path = os.path.join(outdir, 'resumen_ejecutivo.md')\n",
    "    sig = res_df[res_df.significant_fdr == True]\n",
    "    improvements = ', '.join(sig[sig.improved == 'Yes'].metric.tolist()[:4])\n",
    "    deterioro = ', '.join(sig[sig.improved == 'No'].metric.tolist()[:4])\n",
    "    \n",
    "    lines = [\n",
    "        \"# Resumen Ejecutivo\",\n",
    "        \"\",\n",
    "        \"## Claves\",\n",
    "        f\"Cambios significativos tras FDR: {len(sig)}\",\n",
    "        f\"Mejoras: {improvements if improvements else 'Ninguna'}\",\n",
    "        f\"Deterioros: {deterioro if deterioro else 'Ninguno'}\",\n",
    "        \"\",\n",
    "        \"## Visuales\"\n",
    "    ]\n",
    "    \n",
    "    for img in [\"fig_boxplots.png\", \"fig_heatmap_correlaciones.png\"]:\n",
    "        if os.path.exists(os.path.join(outdir, img)):\n",
    "            lines.append(f\"![{img}]({img})\")\n",
    "    \n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(lines))\n",
    "    \n",
    "    return path\n",
    "\n",
    "# Generar resumen ejecutivo\n",
    "exec_path = generate_executive_summary(res_sorted, OUTPUT_DIR)\n",
    "print(f\"✓ Resumen ejecutivo generado: {exec_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06267374",
   "metadata": {},
   "source": [
    "## 10. Conclusiones\n",
    "\n",
    "El análisis ha completado exitosamente. Los archivos generados incluyen:\n",
    "\n",
    "- **CSVs de resultados**: `resultados_metricas.csv` y `resultados_metricas_fdr.csv`\n",
    "- **Visualizaciones**: Boxplots, gráficos de evolución pareada y matriz de correlaciones\n",
    "- **Reportes**: Reporte detallado y resumen ejecutivo en Markdown\n",
    "\n",
    "Revisa los resultados en el directorio `outputs/`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
