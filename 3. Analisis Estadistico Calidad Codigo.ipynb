{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f563dac2",
   "metadata": {},
   "source": [
    "## 1. üì¶ Instalaci√≥n de Dependencias\n",
    "\n",
    "Instalamos todas las librer√≠as necesarias para el an√°lisis estad√≠stico avanzado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578c9866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci√≥n de librer√≠as para an√°lisis estad√≠stico avanzado\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Instala un paquete usando pip si no est√° disponible\"\"\"\n",
    "    try:\n",
    "        __import__(package.split('==')[0])\n",
    "        print(f\"‚úÖ {package} ya est√° instalado\")\n",
    "    except ImportError:\n",
    "        print(f\"üì¶ Instalando {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"‚úÖ {package} instalado correctamente\")\n",
    "\n",
    "# Lista de paquetes necesarios\n",
    "packages = [\n",
    "    \"pandas>=1.5.0\",\n",
    "    \"numpy>=1.21.0\", \n",
    "    \"matplotlib>=3.5.0\",\n",
    "    \"seaborn>=0.11.0\",\n",
    "    \"plotly>=5.0.0\",\n",
    "    \"scipy>=1.9.0\",\n",
    "    \"scikit-learn>=1.1.0\",\n",
    "    \"statsmodels>=0.13.0\",\n",
    "    \"pingouin>=0.5.0\",  # Para an√°lisis estad√≠sticos avanzados\n",
    "    \"kaleido\",  # Para exportar gr√°ficos de plotly\n",
    "    \"jupyter-dash\",  # Para dashboards interactivos\n",
    "    \"umap-learn\",  # Para reducci√≥n de dimensionalidad\n",
    "    \"yellowbrick\",  # Para visualizaciones de ML\n",
    "    \"missingno\"  # Para an√°lisis de datos faltantes\n",
    "]\n",
    "\n",
    "print(\"üöÄ Iniciando instalaci√≥n de dependencias...\")\n",
    "for package in packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\nüéâ Todas las dependencias han sido instaladas correctamente!\")\n",
    "print(\"üìã Reinicia el kernel si es necesario para cargar las nuevas librer√≠as.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddfc04b",
   "metadata": {},
   "source": [
    "## 2. üìö Importaci√≥n de Librer√≠as\n",
    "\n",
    "Importamos todas las librer√≠as necesarias para el an√°lisis estad√≠stico, visualizaci√≥n y machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aa029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librer√≠as fundamentales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualizaci√≥n\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# An√°lisis estad√≠stico\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import shapiro, kstest, mannwhitneyu, wilcoxon, ttest_rel, ttest_ind\n",
    "from scipy.stats import pearsonr, spearmanr, chi2_contingency\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "import pingouin as pg\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, silhouette_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import umap\n",
    "\n",
    "# Visualizaci√≥n de ML\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "from yellowbrick.features import RadViz, ParallelCoordinates\n",
    "\n",
    "# Utilidades\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "import missingno as msno\n",
    "from itertools import combinations\n",
    "from functools import partial\n",
    "\n",
    "# Configuraciones de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Configuraciones de plotly\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# Colores personalizados para visualizaciones\n",
    "COLORS = {\n",
    "    'AP1': '#FF6B6B',    # Rojo suave\n",
    "    'AP2': '#4ECDC4',    # Verde azulado\n",
    "    'male': '#3498DB',   # Azul\n",
    "    'female': '#E74C3C', # Rojo\n",
    "    'primary': '#2C3E50',\n",
    "    'secondary': '#95A5A6',\n",
    "    'success': '#27AE60',\n",
    "    'warning': '#F39C12',\n",
    "    'danger': '#E74C3C'\n",
    "}\n",
    "\n",
    "print(\"üìö Librer√≠as importadas correctamente\")\n",
    "print(f\"üêº Pandas: {pd.__version__}\")\n",
    "print(f\"üî¢ NumPy: {np.__version__}\")\n",
    "print(f\"üìä Matplotlib: {plt.matplotlib.__version__}\")\n",
    "print(f\"üé® Seaborn: {sns.__version__}\")\n",
    "print(\"‚úÖ Configuraciones aplicadas\")\n",
    "\n",
    "# Configurar semilla para reproducibilidad\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45df3681",
   "metadata": {},
   "source": [
    "## 3. üìä Carga y Preparaci√≥n de Datos\n",
    "\n",
    "Cargamos los datasets de m√©tricas de SonarCloud y datos de issues para el an√°lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6898574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para cargar datos con manejo de errores\n",
    "def load_data_safely(file_path, description=\"archivo\"):\n",
    "    \"\"\"Carga un archivo CSV con manejo seguro de errores\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "            print(f\"‚úÖ {description} cargado: {file_path}\")\n",
    "            print(f\"   üìä Dimensiones: {df.shape}\")\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"‚ùå Archivo no encontrado: {file_path}\")\n",
    "            return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cargando {description}: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def download_data_from_github(url, description=\"archivo\"):\n",
    "    \"\"\"Descarga datos directamente desde GitHub con manejo de errores\"\"\"\n",
    "    try:\n",
    "        print(f\"üåê Descargando {description} desde GitHub...\")\n",
    "        df = pd.read_csv(url, encoding='utf-8-sig')\n",
    "        print(f\"‚úÖ {description} descargado exitosamente\")\n",
    "        print(f\"   üìä Dimensiones: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error descargando {description}: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Cargar dataset principal de m√©tricas desde GitHub\n",
    "print(\"üîÑ Cargando datasets...\")\n",
    "github_url = \"https://raw.githubusercontent.com/TesisEnel/Recopilacion_Datos_CalidadCodigo/refs/heads/main/Estudiantes_2023-2024_con_metricas_sonarcloud.csv\"\n",
    "\n",
    "# Intentar descargar desde GitHub primero\n",
    "df_metricas = download_data_from_github(\n",
    "    github_url,\n",
    "    'Dataset de m√©tricas de SonarCloud'\n",
    ")\n",
    "\n",
    "# Si falla la descarga, intentar cargar desde archivo local como respaldo\n",
    "if df_metricas.empty:\n",
    "    print(\"‚ö†Ô∏è Descarga desde GitHub fall√≥. Intentando cargar archivo local...\")\n",
    "    df_metricas = load_data_safely(\n",
    "        '../data/Estudiantes_2023-2024_con_metricas_sonarcloud.csv',\n",
    "        'Dataset local de m√©tricas de SonarCloud'\n",
    "    )\n",
    "\n",
    "# Buscar y cargar el archivo de issues m√°s reciente\n",
    "data_dir = '../data'\n",
    "issues_files = []\n",
    "if os.path.exists(data_dir):\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.startswith('issues_detallados') and file.endswith('.csv'):\n",
    "            issues_files.append(file)\n",
    "\n",
    "# Cargar archivo de issues m√°s reciente o el latest\n",
    "if issues_files:\n",
    "    # Buscar primero el archivo 'latest'\n",
    "    latest_file = [f for f in issues_files if 'latest' in f]\n",
    "    if latest_file:\n",
    "        issues_file = latest_file[0]\n",
    "    else:\n",
    "        # Si no hay latest, tomar el m√°s reciente por fecha\n",
    "        issues_files.sort(reverse=True)\n",
    "        issues_file = issues_files[0]\n",
    "    \n",
    "    df_issues = load_data_safely(\n",
    "        f'{data_dir}/{issues_file}',\n",
    "        'Dataset de issues detallados'\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se encontraron archivos de issues. Se proceder√° solo con m√©tricas.\")\n",
    "    df_issues = pd.DataFrame()\n",
    "\n",
    "# Verificar carga exitosa\n",
    "if not df_metricas.empty:\n",
    "    print(f\"\\nüìã Dataset de m√©tricas:\")\n",
    "    print(f\"   üë• Estudiantes: {df_metricas['Id'].nunique()}\")\n",
    "    print(f\"   üìä Columnas: {len(df_metricas.columns)}\")\n",
    "    print(f\"   üìÖ Semestres: {df_metricas['Semestre'].unique().tolist()}\")\n",
    "    \n",
    "    # Mostrar primeras columnas de m√©tricas\n",
    "    print(f\"\\nüîç Primeras 5 filas de m√©tricas:\")\n",
    "    display(df_metricas[['Id', 'Estudiante', 'Sexo', 'Semestre', 'bugs_AP1', 'bugs_AP2', \n",
    "                        'code_smells_AP1', 'code_smells_AP2', 'ncloc_AP1', 'ncloc_AP2']].head())\n",
    "\n",
    "if not df_issues.empty:\n",
    "    print(f\"\\nüìã Dataset de issues:\")\n",
    "    print(f\"   üêõ Total issues: {len(df_issues)}\")\n",
    "    print(f\"   üë• Estudiantes: {df_issues['student_id'].nunique()}\")\n",
    "    print(f\"   üìÅ Proyectos: {df_issues['project_key'].nunique()}\")\n",
    "    \n",
    "    # Distribuci√≥n por asignaci√≥n\n",
    "    if 'assignment' in df_issues.columns:\n",
    "        print(f\"   üìö Por asignaci√≥n:\")\n",
    "        for assignment in df_issues['assignment'].value_counts().items():\n",
    "            print(f\"      {assignment[0]}: {assignment[1]} issues\")\n",
    "    \n",
    "    print(f\"\\nüîç Muestra de issues:\")\n",
    "    display(df_issues[['student_id', 'assignment', 'type', 'severity', 'rule']].head())\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se cargaron datos de issues - se continuar√° con m√©tricas √∫nicamente\")\n",
    "\n",
    "print(f\"\\n‚úÖ Carga de datos completada\")\n",
    "print(f\"üìä Total de datasets cargados: {1 if not df_metricas.empty else 0} + {1 if not df_issues.empty else 0}\")\n",
    "print(f\"üåê Fuente de datos: {'GitHub (actualizada)' if not df_metricas.empty else 'Local/Error'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf77cae4",
   "metadata": {},
   "source": [
    "### 3.1 Preprocesamiento y Limpieza de Datos\n",
    "\n",
    "Preparamos los datos para el an√°lisis estad√≠stico, incluyendo limpieza, transformaciones y creaci√≥n de variables derivadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0330c7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_metricas.empty:\n",
    "    print(\"üîÑ Iniciando preprocesamiento de datos...\")\n",
    "    \n",
    "    # Crear una copia para trabajar\n",
    "    df = df_metricas.copy()\n",
    "    \n",
    "    # 1. LIMPIEZA B√ÅSICA\n",
    "    print(\"\\n1Ô∏è‚É£ Limpieza b√°sica de datos\")\n",
    "    \n",
    "    # Renombrar columnas para mayor claridad\n",
    "    df['estudiante_id'] = df['Id']\n",
    "    df['nombre'] = df['Estudiante']\n",
    "    df['genero'] = df['Sexo'].map({1: 'Masculino', 2: 'Femenino'})\n",
    "    df['semestre'] = df['Semestre']\n",
    "    \n",
    "    print(f\"   ‚úÖ Renombrado de columnas completado\")\n",
    "    print(f\"   üë• Distribuci√≥n por g√©nero: {df['genero'].value_counts().to_dict()}\")\n",
    "    print(f\"   üìÖ Distribuci√≥n por semestre: {df['semestre'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # 2. IDENTIFICAR M√âTRICAS DE CALIDAD\n",
    "    print(\"\\n2Ô∏è‚É£ Identificaci√≥n de m√©tricas de calidad\")\n",
    "    \n",
    "    # M√©tricas principales de SonarCloud\n",
    "    metricas_calidad = {\n",
    "        'bugs': ['bugs_AP1', 'bugs_AP2'],\n",
    "        'vulnerabilities': ['vulnerabilities_AP1', 'vulnerabilities_AP2'],\n",
    "        'security_hotspots': ['security_hotspots_AP1', 'security_hotspots_AP2'],\n",
    "        'code_smells': ['code_smells_AP1', 'code_smells_AP2'],\n",
    "        'technical_debt': ['technical_debt_AP1', 'technical_debt_AP2'],\n",
    "        'complexity': ['complexity_AP1', 'complexity_AP2'],\n",
    "        'cognitive_complexity': ['cognitive_complexity_AP1', 'cognitive_complexity_AP2'],\n",
    "        'ncloc': ['ncloc_AP1', 'ncloc_AP2'],\n",
    "        'duplicated_lines_density': ['duplicated_lines_density_AP1', 'duplicated_lines_density_AP2'],\n",
    "        'coverage': ['coverage_AP1', 'coverage_AP2'],\n",
    "        'comment_lines_density': ['comment_lines_density_AP1', 'comment_lines_density_AP2'],\n",
    "        'open_issues': ['open_issues_AP1', 'open_issues_AP2']\n",
    "    }\n",
    "    \n",
    "    # Verificar qu√© m√©tricas est√°n disponibles\n",
    "    metricas_disponibles = {}\n",
    "    for metrica, columnas in metricas_calidad.items():\n",
    "        cols_existentes = [col for col in columnas if col in df.columns]\n",
    "        if cols_existentes:\n",
    "            metricas_disponibles[metrica] = cols_existentes\n",
    "            print(f\"   ‚úÖ {metrica}: {len(cols_existentes)} columnas disponibles\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå {metrica}: No disponible\")\n",
    "    \n",
    "    # 3. MANEJO DE DATOS FALTANTES\n",
    "    print(f\"\\n3Ô∏è‚É£ An√°lisis de datos faltantes\")\n",
    "    \n",
    "    # Analizar patrones de datos faltantes\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_metrics = missing_data[missing_data > 0]\n",
    "    \n",
    "    if len(missing_metrics) > 0:\n",
    "        print(f\"   üìä Columnas con datos faltantes:\")\n",
    "        for col, count in missing_metrics.items():\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"      {col}: {count} ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ No se encontraron datos faltantes\")\n",
    "    \n",
    "    # 4. CREAR VARIABLES DERIVADAS\n",
    "    print(f\"\\n4Ô∏è‚É£ Creaci√≥n de variables derivadas\")\n",
    "    \n",
    "    # Calcular m√©tricas de mejora (diferencia AP2 - AP1)\n",
    "    for metrica, columnas in metricas_disponibles.items():\n",
    "        if len(columnas) == 2:  # Si tenemos ambas asignaciones\n",
    "            ap1_col, ap2_col = columnas\n",
    "            \n",
    "            # Mejora absoluta\n",
    "            df[f'{metrica}_mejora_abs'] = df[ap2_col] - df[ap1_col]\n",
    "            \n",
    "            # Mejora relativa (solo para m√©tricas donde m√°s bajo es mejor)\n",
    "            if metrica in ['bugs', 'vulnerabilities', 'security_hotspots', 'code_smells']:\n",
    "                # Para estas m√©tricas, reducci√≥n es mejora\n",
    "                df[f'{metrica}_mejora_rel'] = ((df[ap1_col] - df[ap2_col]) / (df[ap1_col] + 1)) * 100\n",
    "            elif metrica in ['coverage', 'comment_lines_density']:\n",
    "                # Para estas m√©tricas, aumento es mejora\n",
    "                df[f'{metrica}_mejora_rel'] = ((df[ap2_col] - df[ap1_col]) / (df[ap1_col] + 1)) * 100\n",
    "            \n",
    "            print(f\"   ‚úÖ Variables de mejora creadas para {metrica}\")\n",
    "    \n",
    "    # 5. NORMALIZACI√ìN POR TAMA√ëO DE C√ìDIGO\n",
    "    print(f\"\\n5Ô∏è‚É£ Normalizaci√≥n por tama√±o de c√≥digo\")\n",
    "    \n",
    "    # Crear m√©tricas normalizadas por NCLOC (l√≠neas de c√≥digo)\n",
    "    if 'ncloc' in metricas_disponibles:\n",
    "        for metrica in ['bugs', 'vulnerabilities', 'code_smells']:\n",
    "            if metrica in metricas_disponibles:\n",
    "                for asignacion in ['AP1', 'AP2']:\n",
    "                    metrica_col = f'{metrica}_{asignacion}'\n",
    "                    ncloc_col = f'ncloc_{asignacion}'\n",
    "                    \n",
    "                    if metrica_col in df.columns and ncloc_col in df.columns:\n",
    "                        # Densidad de defectos por 1000 l√≠neas de c√≥digo\n",
    "                        df[f'{metrica}_density_{asignacion}'] = (df[metrica_col] / (df[ncloc_col] + 1)) * 1000\n",
    "                        \n",
    "                print(f\"   ‚úÖ Densidad calculada para {metrica}\")\n",
    "    \n",
    "    # 6. CREAR √çNDICES COMPUESTOS\n",
    "    print(f\"\\n6Ô∏è‚É£ Creaci√≥n de √≠ndices compuestos de calidad\")\n",
    "    \n",
    "    # √çndice de calidad general (menor es mejor)\n",
    "    for asignacion in ['AP1', 'AP2']:\n",
    "        quality_components = []\n",
    "        \n",
    "        # Componentes del √≠ndice (normalizados)\n",
    "        if f'bugs_{asignacion}' in df.columns and f'ncloc_{asignacion}' in df.columns:\n",
    "            df[f'bugs_norm_{asignacion}'] = df[f'bugs_{asignacion}'] / (df[f'ncloc_{asignacion}'] / 1000 + 1)\n",
    "            quality_components.append(f'bugs_norm_{asignacion}')\n",
    "        \n",
    "        if f'vulnerabilities_{asignacion}' in df.columns and f'ncloc_{asignacion}' in df.columns:\n",
    "            df[f'vuln_norm_{asignacion}'] = df[f'vulnerabilities_{asignacion}'] / (df[f'ncloc_{asignacion}'] / 1000 + 1)\n",
    "            quality_components.append(f'vuln_norm_{asignacion}')\n",
    "        \n",
    "        if f'code_smells_{asignacion}' in df.columns and f'ncloc_{asignacion}' in df.columns:\n",
    "            df[f'smells_norm_{asignacion}'] = df[f'code_smells_{asignacion}'] / (df[f'ncloc_{asignacion}'] / 1000 + 1)\n",
    "            quality_components.append(f'smells_norm_{asignacion}')\n",
    "        \n",
    "        # Calcular √≠ndice compuesto\n",
    "        if quality_components:\n",
    "            df[f'quality_index_{asignacion}'] = df[quality_components].sum(axis=1)\n",
    "            print(f\"   ‚úÖ √çndice de calidad calculado para {asignacion}\")\n",
    "    \n",
    "    # 7. IDENTIFICAR ESTUDIANTES CON AMBOS PROYECTOS\n",
    "    print(f\"\\n7Ô∏è‚É£ Identificaci√≥n de estudiantes con datos pareados\")\n",
    "    \n",
    "    # Filtrar estudiantes que tienen datos en ambas asignaciones\n",
    "    estudiantes_completos = []\n",
    "    for metrica in ['bugs', 'code_smells', 'ncloc']:\n",
    "        if metrica in metricas_disponibles and len(metricas_disponibles[metrica]) == 2:\n",
    "            ap1_col, ap2_col = metricas_disponibles[metrica]\n",
    "            mask_completo = df[ap1_col].notna() & df[ap2_col].notna()\n",
    "            estudiantes_con_metrica = df[mask_completo]['estudiante_id'].tolist()\n",
    "            estudiantes_completos.extend(estudiantes_con_metrica)\n",
    "    \n",
    "    # Estudiantes que aparecen en todas las m√©tricas principales\n",
    "    estudiantes_pareados = list(set(estudiantes_completos))\n",
    "    df['datos_pareados'] = df['estudiante_id'].isin(estudiantes_pareados)\n",
    "    \n",
    "    print(f\"   üë• Total estudiantes: {len(df)}\")\n",
    "    print(f\"   üë• Estudiantes con datos pareados: {df['datos_pareados'].sum()}\")\n",
    "    print(f\"   üë• Estudiantes solo con AP1 o AP2: {(~df['datos_pareados']).sum()}\")\n",
    "    \n",
    "    # 8. RESUMEN FINAL\n",
    "    print(f\"\\n‚úÖ Preprocesamiento completado\")\n",
    "    print(f\"üìä Dataset final: {df.shape}\")\n",
    "    print(f\"üìã M√©tricas de calidad disponibles: {len(metricas_disponibles)}\")\n",
    "    print(f\"üìà Variables derivadas creadas: {len([col for col in df.columns if 'mejora' in col or 'density' in col or 'quality_index' in col])}\")\n",
    "    \n",
    "    # Mostrar estad√≠sticas b√°sicas\n",
    "    print(f\"\\nüìä Estad√≠sticas b√°sicas de m√©tricas principales:\")\n",
    "    main_metrics = ['bugs_AP1', 'bugs_AP2', 'code_smells_AP1', 'code_smells_AP2', 'ncloc_AP1', 'ncloc_AP2']\n",
    "    available_metrics = [col for col in main_metrics if col in df.columns]\n",
    "    if available_metrics:\n",
    "        display(df[available_metrics].describe())\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No se puede proceder sin datos de m√©tricas\")\n",
    "    df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d191366",
   "metadata": {},
   "source": [
    "## 4. üîç An√°lisis Exploratorio de Datos (EDA)\n",
    "\n",
    "En esta secci√≥n realizamos un an√°lisis exploratorio comprehensivo de las m√©tricas de calidad de c√≥digo, incluyendo distribuciones, patrones y relaciones entre variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1971e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones para an√°lisis exploratorio\n",
    "def create_distribution_plots(data, metrics, title_prefix=\"Distribuci√≥n\"):\n",
    "    \"\"\"Crea gr√°ficos de distribuci√≥n para m√∫ltiples m√©tricas\"\"\"\n",
    "    n_metrics = len(metrics)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_metrics + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        if metric in data.columns:\n",
    "            ax = axes[i]\n",
    "            # Histograma con curva de densidad\n",
    "            data[metric].hist(bins=30, alpha=0.7, ax=ax, color=COLORS['primary'])\n",
    "            data[metric].plot.density(ax=ax, color=COLORS['danger'], linewidth=2)\n",
    "            ax.set_title(f'{title_prefix}: {metric}')\n",
    "            ax.set_xlabel('Valor')\n",
    "            ax.set_ylabel('Frecuencia / Densidad')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Estad√≠sticas en el gr√°fico\n",
    "            mean_val = data[metric].mean()\n",
    "            median_val = data[metric].median()\n",
    "            ax.axvline(mean_val, color='red', linestyle='--', label=f'Media: {mean_val:.2f}')\n",
    "            ax.axvline(median_val, color='green', linestyle='--', label=f'Mediana: {median_val:.2f}')\n",
    "            ax.legend()\n",
    "        else:\n",
    "            axes[i].text(0.5, 0.5, f'M√©trica {metric}\\\\nno disponible', \n",
    "                        ha='center', va='center', transform=axes[i].transAxes)\n",
    "            axes[i].set_title(f'{metric} - No disponible')\n",
    "    \n",
    "    # Ocultar ejes sobrantes\n",
    "    for i in range(len(metrics), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_comparison_boxplots(data, metrics, assignment_cols=['AP1', 'AP2']):\n",
    "    \"\"\"Crea boxplots comparativos entre asignaciones\"\"\"\n",
    "    n_metrics = len(metrics)\n",
    "    n_cols = 2\n",
    "    n_rows = (n_metrics + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 6 * n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Preparar datos para boxplot\n",
    "        metric_data = []\n",
    "        labels = []\n",
    "        \n",
    "        for assignment in assignment_cols:\n",
    "            col_name = f'{metric}_{assignment}'\n",
    "            if col_name in data.columns:\n",
    "                values = data[col_name].dropna()\n",
    "                metric_data.append(values)\n",
    "                labels.append(f'{metric}\\\\n{assignment}')\n",
    "        \n",
    "        if metric_data:\n",
    "            bp = ax.boxplot(metric_data, labels=labels, patch_artist=True)\n",
    "            \n",
    "            # Colorear boxplots\n",
    "            colors = [COLORS['AP1'], COLORS['AP2']]\n",
    "            for patch, color in zip(bp['boxes'], colors[:len(bp['boxes'])]):\n",
    "                patch.set_facecolor(color)\n",
    "                patch.set_alpha(0.7)\n",
    "            \n",
    "            ax.set_title(f'Comparaci√≥n {metric}: AP1 vs AP2')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Agregar estad√≠sticas\n",
    "            for j, values in enumerate(metric_data):\n",
    "                median_val = values.median()\n",
    "                mean_val = values.mean()\n",
    "                ax.text(j+1, median_val, f'Med: {median_val:.1f}', \n",
    "                       ha='center', va='bottom', fontsize=9)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, f'Datos no\\\\ndisponibles', \n",
    "                   ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title(f'{metric} - Sin datos')\n",
    "    \n",
    "    # Ocultar ejes sobrantes\n",
    "    for i in range(len(metrics), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Ejecutar an√°lisis exploratorio si tenemos datos\n",
    "if not df.empty:\n",
    "    print(\"üîç Iniciando An√°lisis Exploratorio de Datos...\")\n",
    "    \n",
    "    # 1. ESTAD√çSTICAS DESCRIPTIVAS GENERALES\n",
    "    print(\"\\\\n1Ô∏è‚É£ Estad√≠sticas Descriptivas Generales\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"üë• Total de estudiantes: {len(df)}\")\n",
    "    print(f\"üìä Distribuci√≥n por g√©nero:\")\n",
    "    print(df['genero'].value_counts())\n",
    "    print(f\"\\\\nüìÖ Distribuci√≥n por semestre:\")\n",
    "    print(df['semestre'].value_counts())\n",
    "    \n",
    "    # 2. AN√ÅLISIS DE M√âTRICAS PRINCIPALES\n",
    "    print(\"\\\\n2Ô∏è‚É£ An√°lisis de M√©tricas Principales de Calidad\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # M√©tricas clave para analizar\n",
    "    main_quality_metrics = ['bugs', 'code_smells', 'vulnerabilities', 'ncloc', 'complexity']\n",
    "    \n",
    "    # Estad√≠sticas por asignaci√≥n\n",
    "    for assignment in ['AP1', 'AP2']:\n",
    "        print(f\"\\\\nüìö Estad√≠sticas para {assignment}:\")\n",
    "        assignment_metrics = [f'{metric}_{assignment}' for metric in main_quality_metrics]\n",
    "        available_assignment_metrics = [col for col in assignment_metrics if col in df.columns]\n",
    "        \n",
    "        if available_assignment_metrics:\n",
    "            stats_df = df[available_assignment_metrics].describe()\n",
    "            display(stats_df)\n",
    "            \n",
    "            # Identificar outliers\n",
    "            print(f\"\\\\nüéØ Outliers en {assignment} (valores > Q3 + 1.5*IQR):\")\n",
    "            for col in available_assignment_metrics:\n",
    "                q1 = df[col].quantile(0.25)\n",
    "                q3 = df[col].quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                outlier_threshold = q3 + 1.5 * iqr\n",
    "                outliers = df[df[col] > outlier_threshold][col]\n",
    "                if len(outliers) > 0:\n",
    "                    print(f\"   {col}: {len(outliers)} outliers (max: {outliers.max():.1f})\")\n",
    "    \n",
    "    # 3. DISTRIBUCIONES DE M√âTRICAS CLAVE\n",
    "    print(\"\\\\n3Ô∏è‚É£ Visualizaci√≥n de Distribuciones\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Gr√°ficos de distribuci√≥n para AP1\n",
    "    ap1_metrics = [f'{metric}_AP1' for metric in main_quality_metrics if f'{metric}_AP1' in df.columns]\n",
    "    if ap1_metrics:\n",
    "        print(\"üìä Distribuciones en AP1:\")\n",
    "        create_distribution_plots(df, ap1_metrics, \"Distribuci√≥n AP1\")\n",
    "    \n",
    "    # Gr√°ficos de distribuci√≥n para AP2  \n",
    "    ap2_metrics = [f'{metric}_AP2' for metric in main_quality_metrics if f'{metric}_AP2' in df.columns]\n",
    "    if ap2_metrics:\n",
    "        print(\"üìä Distribuciones en AP2:\")\n",
    "        create_distribution_plots(df, ap2_metrics, \"Distribuci√≥n AP2\")\n",
    "    \n",
    "    # 4. COMPARACIONES ENTRE ASIGNACIONES\n",
    "    print(\"\\\\n4Ô∏è‚É£ Comparaciones entre AP1 y AP2\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Boxplots comparativos\n",
    "    available_base_metrics = [metric for metric in main_quality_metrics \n",
    "                             if f'{metric}_AP1' in df.columns and f'{metric}_AP2' in df.columns]\n",
    "    \n",
    "    if available_base_metrics:\n",
    "        print(\"üìä Boxplots comparativos AP1 vs AP2:\")\n",
    "        create_comparison_boxplots(df, available_base_metrics)\n",
    "    \n",
    "    # 5. AN√ÅLISIS DE CORRELACIONES\n",
    "    print(\"\\\\n5Ô∏è‚É£ An√°lisis de Correlaciones\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Matriz de correlaci√≥n para m√©tricas de AP1\n",
    "    ap1_cols = [col for col in df.columns if col.endswith('_AP1') and df[col].dtype in ['int64', 'float64']]\n",
    "    if len(ap1_cols) > 1:\n",
    "        print(\"üìä Matriz de correlaci√≥n - AP1:\")\n",
    "        corr_matrix_ap1 = df[ap1_cols].corr()\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(corr_matrix_ap1, annot=True, cmap='RdBu_r', center=0, \n",
    "                   square=True, fmt='.2f', cbar_kws={'label': 'Correlaci√≥n'})\n",
    "        plt.title('Matriz de Correlaci√≥n - M√©tricas AP1')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Matriz de correlaci√≥n para m√©tricas de AP2\n",
    "    ap2_cols = [col for col in df.columns if col.endswith('_AP2') and df[col].dtype in ['int64', 'float64']]\n",
    "    if len(ap2_cols) > 1:\n",
    "        print(\"üìä Matriz de correlaci√≥n - AP2:\")\n",
    "        corr_matrix_ap2 = df[ap2_cols].corr()\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(corr_matrix_ap2, annot=True, cmap='RdBu_r', center=0, \n",
    "                   square=True, fmt='.2f', cbar_kws={'label': 'Correlaci√≥n'})\n",
    "        plt.title('Matriz de Correlaci√≥n - M√©tricas AP2')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\\\n‚úÖ An√°lisis Exploratorio de Datos completado\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No se puede realizar EDA sin datos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c5fa72",
   "metadata": {},
   "source": [
    "# üìä An√°lisis Estad√≠stico Avanzado de Calidad de C√≥digo en Proyectos Estudiantiles\n",
    "\n",
    "## üéØ Objetivo Principal\n",
    "\n",
    "Desarrollar un an√°lisis estad√≠stico riguroso y comprehensivo que determine si existe una **mejora estad√≠sticamente significativa** en la calidad del c√≥digo entre **Programaci√≥n Aplicada I (AP1)** y **Programaci√≥n Aplicada II (AP2)**, identificando patrones, factores de influencia y recomendaciones pedag√≥gicas.\n",
    "\n",
    "## üìã Datos Disponibles\n",
    "\n",
    "- ‚úÖ **60 estudiantes** con proyectos reales de programaci√≥n\n",
    "- ‚úÖ **M√©tricas de SonarCloud**: bugs, vulnerabilidades, code smells, deuda t√©cnica, complejidad, etc.\n",
    "- ‚úÖ **Issues detallados**: tipos, severidades, reglas violadas, ubicaciones espec√≠ficas\n",
    "- ‚úÖ **Datos demogr√°ficos**: g√©nero, semestre, repositorios originales\n",
    "- ‚úÖ **Proyectos diversos**: sistemas de ventas, apps m√≥viles, e-commerce, etc.\n",
    "\n",
    "## üî¨ Metodolog√≠a\n",
    "\n",
    "1. **An√°lisis Exploratorio de Datos (EDA)**\n",
    "2. **Pruebas de Hip√≥tesis M√∫ltiples**\n",
    "3. **An√°lisis Multivariado y Machine Learning**\n",
    "4. **Visualizaciones Avanzadas**\n",
    "5. **Recomendaciones Pedag√≥gicas**\n",
    "\n",
    "---\n",
    "*An√°lisis desarrollado para investigaci√≥n en calidad de software educativa - Tesis Aplicada 2*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
