{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2346d715",
   "metadata": {},
   "source": [
    "# Extracci√≥n de Issues Detallados desde SonarCloud\n",
    "\n",
    "Este notebook est√° dise√±ado para extraer todos los issues (problemas) detallados de los proyectos de SonarCloud de los estudiantes, proporcionando un an√°lisis profundo de la calidad del c√≥digo a nivel de issue individual.\n",
    "\n",
    "## Objetivos:\n",
    "1. **Cargar datos** de proyectos de estudiantes desde el CSV con columnas Sonar_Ap1 y Sonar_Ap2\n",
    "2. **Extraer issues detallados** desde SonarCloud API para cada proyecto utilizando `/api/issues/search`\n",
    "3. **Procesar y categorizar** issues por severidad, tipo y regla aplicada\n",
    "4. **Generar an√°lisis** de patrones de calidad de c√≥digo por estudiante y assignment\n",
    "5. **Exportar resultados** para an√°lisis estad√≠sticos posteriores\n",
    "\n",
    "## Informaci√≥n Extra√≠da por Issue:\n",
    "- **Identificaci√≥n**: Issue key, project key, regla aplicada\n",
    "- **Clasificaci√≥n**: Severidad (CRITICAL, MAJOR, MINOR, etc.), tipo (BUG, VULNERABILITY, CODE_SMELL)\n",
    "- **Localizaci√≥n**: Archivo/componente afectado, l√≠nea de c√≥digo espec√≠fica\n",
    "- **Descripci√≥n**: Mensaje detallado del problema detectado\n",
    "- **Contexto**: Informaci√≥n del estudiante y assignment (AP1/AP2)\n",
    "\n",
    "## Diferencia con el Notebook Anterior:\n",
    "- **Enfoque**: Issues individuales vs. m√©tricas agregadas\n",
    "- **Granularidad**: Nivel de l√≠nea de c√≥digo vs. nivel de proyecto\n",
    "- **An√°lisis**: Patrones espec√≠ficos de problemas vs. estad√≠sticas generales\n",
    "- **Volumen**: Mayor cantidad de datos detallados por proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci√≥n de dependencias (ejecutar solo si es necesario)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Instalar paquete si no est√° disponible\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"‚úÖ {package} ya est√° instalado\")\n",
    "    except ImportError:\n",
    "        print(f\"üîÑ Instalando {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"‚úÖ {package} instalado exitosamente\")\n",
    "\n",
    "# Lista de paquetes requeridos\n",
    "required_packages = ['pandas', 'requests']\n",
    "\n",
    "print(\"üîß Verificando e instalando dependencias...\")\n",
    "for package in required_packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"‚úÖ Todas las dependencias est√°n listas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d495d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librer√≠as necesarias\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import base64\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "import warnings\n",
    "import logging\n",
    "from functools import wraps\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")\n",
    "print(\"üìù Pandas version:\", pd.__version__)\n",
    "print(\"üåê Requests disponible para API calls\")\n",
    "print(\"üïê Datetime y logging configurados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058943dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones utilitarias para optimizar el c√≥digo\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def timer_decorator(func):\n",
    "    \"\"\"Decorador para medir tiempo de ejecuci√≥n de funciones\"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"‚è±Ô∏è  {func.__name__} ejecutado en {end_time - start_time:.2f} segundos\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def clean_message_for_csv(message):\n",
    "    \"\"\"Limpiar mensaje para formato CSV\"\"\"\n",
    "    if not message:\n",
    "        return \"\"\n",
    "    # Escapar comillas dobles duplic√°ndolas y envolver en comillas\n",
    "    cleaned = message.replace('\"', '\"\"')\n",
    "    return f'\"{cleaned}\"'\n",
    "\n",
    "def safe_divide(numerator, denominator):\n",
    "    \"\"\"Divisi√≥n segura que evita divisi√≥n por cero\"\"\"\n",
    "    return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "def format_percentage(value, total):\n",
    "    \"\"\"Formatear porcentaje de manera consistente\"\"\"\n",
    "    return f\"{safe_divide(value, total) * 100:.1f}%\"\n",
    "\n",
    "def print_section_header(title, char=\"=\", width=50):\n",
    "    \"\"\"Imprimir headers de secci√≥n consistentes\"\"\"\n",
    "    print(f\"\\n{title}\")\n",
    "    print(char * width)\n",
    "\n",
    "print(\"üõ†Ô∏è  Funciones utilitarias cargadas\")\n",
    "print(\"üìä Timer, limpieza de datos y formateo disponibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfe7e58",
   "metadata": {},
   "source": [
    "## 1. Configuraci√≥n de la API de SonarCloud\n",
    "\n",
    "Configuramos los par√°metros necesarios para conectarnos a SonarCloud y extraer issues detallados:\n",
    "\n",
    "- **Token de autenticaci√≥n**: Credenciales para acceder a la API de SonarCloud\n",
    "- **Headers de autenticaci√≥n**: Configuraci√≥n b√°sica con base64 encoding\n",
    "- **Organizaci√≥n**: Clave de la organizaci√≥n en SonarCloud (`tesisenel`)\n",
    "- **Endpoints**: URLs para b√∫squeda de proyectos e issues\n",
    "\n",
    "### Diferencias con Extracci√≥n de M√©tricas:\n",
    "- **Endpoint principal**: `/api/issues/search` (vs `/api/measures/component`)\n",
    "- **Paginaci√≥n**: Manejo de grandes vol√∫menes de issues por proyecto\n",
    "- **Filtros**: Capacidad de filtrar por tipo de issue, severidad, estado\n",
    "- **Detalle**: Informaci√≥n espec√≠fica de ubicaci√≥n (archivo, l√≠nea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5b1a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de SonarCloud API para extracci√≥n de issues\n",
    "from functools import lru_cache\n",
    "\n",
    "# Configuraci√≥n de autenticaci√≥n (usando el mismo token del notebook anterior)\n",
    "SONAR_TOKEN = \"cc64d7ea652e603cacbc87bbb9c7b550efee7353\"\n",
    "SONAR_BASE_URL = \"https://sonarcloud.io/api\"\n",
    "SONAR_ORGANIZATION = \"tesisenel\"\n",
    "\n",
    "# Cache para headers de autenticaci√≥n\n",
    "@lru_cache(maxsize=1)\n",
    "def get_auth_headers():\n",
    "    \"\"\"Crear headers de autenticaci√≥n para SonarCloud API (con cach√©)\"\"\"\n",
    "    auth_string = f\"{SONAR_TOKEN}:\"\n",
    "    auth_bytes = auth_string.encode('ascii')\n",
    "    auth_b64 = base64.b64encode(auth_bytes).decode('ascii')\n",
    "    \n",
    "    return {\n",
    "        'Authorization': f'Basic {auth_b64}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "# Configuraci√≥n de par√°metros de extracci√≥n\n",
    "ISSUES_CONFIG = {\n",
    "    'page_size': 500,  # M√°ximo permitido por SonarCloud\n",
    "    'max_retries': 3,\n",
    "    'retry_delay': 5.0,\n",
    "    'batch_delay': 2.0,\n",
    "    'timeout': 30\n",
    "}\n",
    "\n",
    "# Tipos de issues que vamos a extraer\n",
    "ISSUE_TYPES = ['BUG', 'VULNERABILITY', 'CODE_SMELL']\n",
    "SEVERITIES = ['BLOCKER', 'CRITICAL', 'MAJOR', 'MINOR', 'INFO']\n",
    "\n",
    "print(\"üîê Configuraci√≥n de autenticaci√≥n preparada\")\n",
    "print(f\"üè¢ Organizaci√≥n: {SONAR_ORGANIZATION}\")\n",
    "print(f\"üìä Page size: {ISSUES_CONFIG['page_size']} issues por p√°gina\")\n",
    "print(f\"üîÑ Reintentos: {ISSUES_CONFIG['max_retries']} m√°ximo\")\n",
    "print(\"‚úÖ Headers de API configurados con cach√©\")\n",
    "print(\"‚ö†Ô∏è  Token expuesto temporalmente para pruebas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6ef8dd",
   "metadata": {},
   "source": [
    "## 2. Carga de Datos de Estudiantes\n",
    "\n",
    "Cargamos el mismo archivo CSV utilizado en el notebook anterior, que contiene la informaci√≥n de los estudiantes incluyendo las columnas `Sonar_Ap1` y `Sonar_Ap2` con las claves de los proyectos en SonarCloud.\n",
    "\n",
    "### Reutilizaci√≥n de Funciones:\n",
    "Utilizaremos la misma funci√≥n `extract_project_keys()` para extraer las claves de proyecto, manteniendo consistencia con el an√°lisis de m√©tricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80835084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos de estudiantes (mismo CSV del notebook anterior)\n",
    "CSV_PATH = \"https://raw.githubusercontent.com/TesisEnel/Recopilacion_Datos_CalidadCodigo/refs/heads/main/Estudiantes_2023-2024.csv\"\n",
    "\n",
    "try:\n",
    "    # Cargar el CSV con informaci√≥n de estudiantes\n",
    "    df_estudiantes = pd.read_csv(CSV_PATH)\n",
    "    \n",
    "    print(\"üìÅ Datos de estudiantes cargados exitosamente\")\n",
    "    print(f\"üë• N√∫mero de estudiantes: {len(df_estudiantes)}\")\n",
    "    print(f\"üìä Columnas disponibles: {list(df_estudiantes.columns)}\")\n",
    "    \n",
    "    # Verificar columnas de SonarCloud\n",
    "    sonar_columns = [col for col in df_estudiantes.columns if 'Sonar' in col]\n",
    "    print(f\"\\nüéØ Columnas de SonarCloud encontradas: {sonar_columns}\")\n",
    "    \n",
    "    # Contar proyectos no vac√≠os\n",
    "    sonar_ap1_count = df_estudiantes['Sonar_Ap1'].notna().sum()\n",
    "    sonar_ap2_count = df_estudiantes['Sonar_Ap2'].notna().sum()\n",
    "    \n",
    "    print(f\"üìà Proyectos Sonar_Ap1 disponibles: {sonar_ap1_count}\")\n",
    "    print(f\"üìà Proyectos Sonar_Ap2 disponibles: {sonar_ap2_count}\")\n",
    "    print(f\"üìà Total de proyectos para an√°lisis de issues: {sonar_ap1_count + sonar_ap2_count}\")\n",
    "    \n",
    "    # Mostrar muestra de datos\n",
    "    print(f\"\\nüîç Primeras 3 filas:\")\n",
    "    display(df_estudiantes[['ID', 'Estudiante', 'Sonar_Ap1', 'Sonar_Ap2']].head(3))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: No se encontr√≥ el archivo CSV\")\n",
    "    print(f\"üìç Buscando en: {CSV_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al cargar datos: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0a414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer project keys de SonarCloud (reutilizando funci√≥n del notebook anterior)\n",
    "def extract_project_keys(df):\n",
    "    \"\"\"\n",
    "    Extraer todas las claves de proyecto de SonarCloud del DataFrame\n",
    "    \"\"\"\n",
    "    project_keys = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        student_id = row.get('ID', f'Student_{index}')\n",
    "        nombre = row.get('Estudiante', 'Unknown')\n",
    "        \n",
    "        # Procesar Sonar_Ap1\n",
    "        if pd.notna(row['Sonar_Ap1']) and row['Sonar_Ap1'].strip():\n",
    "            project_keys.append({\n",
    "                'student_id': student_id,\n",
    "                'nombre': nombre,\n",
    "                'project_key': row['Sonar_Ap1'].strip(),\n",
    "                'assignment': 'AP1',\n",
    "                'row_index': index\n",
    "            })\n",
    "        \n",
    "        # Procesar Sonar_Ap2\n",
    "        if pd.notna(row['Sonar_Ap2']) and row['Sonar_Ap2'].strip():\n",
    "            project_keys.append({\n",
    "                'student_id': student_id,\n",
    "                'nombre': nombre,\n",
    "                'project_key': row['Sonar_Ap2'].strip(),\n",
    "                'assignment': 'AP2',\n",
    "                'row_index': index\n",
    "            })\n",
    "    \n",
    "    return project_keys\n",
    "\n",
    "# Extraer project keys\n",
    "project_list = extract_project_keys(df_estudiantes)\n",
    "\n",
    "print(f\"üîë Total de project keys extra√≠dos: {len(project_list)}\")\n",
    "\n",
    "# Mostrar estad√≠sticas por assignment\n",
    "ap1_count = len([p for p in project_list if p['assignment'] == 'AP1'])\n",
    "ap2_count = len([p for p in project_list if p['assignment'] == 'AP2'])\n",
    "\n",
    "print(f\"üìä Proyectos AP1: {ap1_count}\")\n",
    "print(f\"üìä Proyectos AP2: {ap2_count}\")\n",
    "\n",
    "# Mostrar algunos ejemplos\n",
    "print(\"\\nüîç Primeros 5 project keys para an√°lisis de issues:\")\n",
    "for i, project in enumerate(project_list[:5]):\n",
    "    print(f\"  {i+1}. {project['nombre']} - {project['assignment']}: {project['project_key']}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Proyectos listos para extracci√≥n de issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8401c9e",
   "metadata": {},
   "source": [
    "## 3. Implementaci√≥n de Funciones de Extracci√≥n de Issues\n",
    "\n",
    "Implementamos las funciones principales para extraer issues detallados de SonarCloud utilizando el endpoint `/api/issues/search`.\n",
    "\n",
    "### Funciones implementadas:\n",
    "\n",
    "#### `fetch_project_issues(project_key)`\n",
    "- **Prop√≥sito**: Obtener todos los issues de un proyecto espec√≠fico desde SonarCloud\n",
    "- **Endpoint**: `/api/issues/search`\n",
    "- **Caracter√≠sticas**:\n",
    "  - ‚è±Ô∏è **Timing con decorador**: Mide tiempo de ejecuci√≥n\n",
    "  - üîÑ **Paginaci√≥n autom√°tica**: Maneja m√∫ltiples p√°ginas de resultados\n",
    "  - üõ°Ô∏è **Manejo robusto de errores**: Captura errores HTTP, conexi√≥n y timeout\n",
    "  - üìä **Extracci√≥n completa**: Obtiene todos los campos relevantes del issue\n",
    "  - üìù **Logging estructurado**: Registra progreso y errores\n",
    "\n",
    "#### `batch_fetch_issues(project_list)`\n",
    "- **Prop√≥sito**: Procesar m√∫ltiples proyectos con control de rate limiting\n",
    "- **Caracter√≠sticas**:\n",
    "  - ‚ö° **Procesamiento secuencial**: Evita sobrecargar la API\n",
    "  - üìä **Progreso en tiempo real**: Indicadores de avance por proyecto\n",
    "  - üéØ **Estad√≠sticas detalladas**: Conteo de issues por proyecto\n",
    "  - ‚è≥ **Rate limiting**: Delays configurables entre proyectos\n",
    "  - üõ°Ô∏è **Tolerancia a fallos**: Contin√∫a aunque fallen proyectos individuales\n",
    "\n",
    "#### Funciones auxiliares:\n",
    "- `parse_issue_data()`: Extrae y estructura datos del issue\n",
    "- `create_issue_error_response()`: Respuestas de error estandarizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones para extraer issues de SonarCloud API\n",
    "\n",
    "def parse_issue_data(issue, project_info):\n",
    "    \"\"\"Extraer y estructurar datos de un issue individual\"\"\"\n",
    "    return {\n",
    "        # Informaci√≥n del proyecto y estudiante\n",
    "        'student_id': project_info['student_id'],\n",
    "        'nombre': project_info['nombre'],\n",
    "        'assignment': project_info['assignment'],\n",
    "        'row_index': project_info['row_index'],\n",
    "        'project_key': project_info['project_key'],\n",
    "        \n",
    "        # Informaci√≥n del issue\n",
    "        'issue_key': issue.get('key', ''),\n",
    "        'rule': issue.get('rule', ''),\n",
    "        'severity': issue.get('severity', ''),\n",
    "        'type': issue.get('type', ''),\n",
    "        'message': issue.get('message', ''),\n",
    "        'component': issue.get('component', ''),\n",
    "        'line': issue.get('line', 0),\n",
    "        'status': issue.get('status', ''),\n",
    "        'creation_date': issue.get('creationDate', ''),\n",
    "        'update_date': issue.get('updateDate', ''),\n",
    "        'effort': issue.get('effort', ''),\n",
    "        'debt': issue.get('debt', ''),\n",
    "        'tags': ','.join(issue.get('tags', [])) if issue.get('tags') else ''\n",
    "    }\n",
    "\n",
    "def create_issue_error_response(project_key, status, error_info=None):\n",
    "    \"\"\"Crear respuesta de error estandarizada para issues\"\"\"\n",
    "    response = {\n",
    "        'project_key': project_key, \n",
    "        'status': status,\n",
    "        'issues_count': 0,\n",
    "        'issues': []\n",
    "    }\n",
    "    if error_info:\n",
    "        response['error'] = error_info\n",
    "    return response\n",
    "\n",
    "@timer_decorator\n",
    "def fetch_project_issues(project_info: Dict, retries: int = 3, delay: float = 5.0) -> Dict:\n",
    "    \"\"\"\n",
    "    Obtener todos los issues de un proyecto espec√≠fico desde SonarCloud con paginaci√≥n\n",
    "    \"\"\"\n",
    "    project_key = project_info['project_key']\n",
    "    all_issues = []\n",
    "    page = 1\n",
    "    has_more_pages = True\n",
    "    \n",
    "    while has_more_pages:\n",
    "        url = f\"{SONAR_BASE_URL}/issues/search\"\n",
    "        params = {\n",
    "            'componentKeys': project_key,\n",
    "            'p': page,\n",
    "            'ps': ISSUES_CONFIG['page_size']\n",
    "        }\n",
    "        \n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = requests.get(\n",
    "                    url, \n",
    "                    params=params, \n",
    "                    headers=get_auth_headers(), \n",
    "                    timeout=ISSUES_CONFIG['timeout']\n",
    "                )\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    \n",
    "                    # Extraer issues de esta p√°gina\n",
    "                    if 'issues' in data and data['issues']:\n",
    "                        page_issues = [\n",
    "                            parse_issue_data(issue, project_info) \n",
    "                            for issue in data['issues']\n",
    "                        ]\n",
    "                        all_issues.extend(page_issues)\n",
    "                    \n",
    "                    # Verificar si hay m√°s p√°ginas\n",
    "                    paging = data.get('paging', {})\n",
    "                    total = paging.get('total', 0)\n",
    "                    page_size = paging.get('pageSize', ISSUES_CONFIG['page_size'])\n",
    "                    \n",
    "                    has_more_pages = (page * page_size) < total\n",
    "                    if has_more_pages:\n",
    "                        page += 1\n",
    "                        print(f\"    üìÑ P√°gina {page-1} procesada, {len(page_issues)} issues encontrados\")\n",
    "                    else:\n",
    "                        print(f\"    üìÑ P√°gina {page} procesada (final), {len(page_issues)} issues encontrados\")\n",
    "                    \n",
    "                    break  # Salir del loop de reintentos\n",
    "                    \n",
    "                elif response.status_code == 401:\n",
    "                    logger.warning(f\"Authentication error for project {project_key}\")\n",
    "                    if attempt == retries - 1:\n",
    "                        return create_issue_error_response(project_key, 'authentication_error', response.status_code)\n",
    "                else:\n",
    "                    logger.warning(f\"HTTP error {response.status_code} for project {project_key}\")\n",
    "                    if attempt == retries - 1:\n",
    "                        return create_issue_error_response(project_key, f'http_error_{response.status_code}', response.status_code)\n",
    "                \n",
    "                time.sleep(delay)\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.error(f\"Connection error for project {project_key}: {e}\")\n",
    "                if attempt == retries - 1:\n",
    "                    return create_issue_error_response(project_key, 'connection_error', str(e))\n",
    "                time.sleep(delay)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Unexpected error for project {project_key}: {e}\")\n",
    "                if attempt == retries - 1:\n",
    "                    return create_issue_error_response(project_key, 'unexpected_error', str(e))\n",
    "                time.sleep(delay)\n",
    "    \n",
    "    return {\n",
    "        'project_key': project_key,\n",
    "        'status': 'success',\n",
    "        'issues_count': len(all_issues),\n",
    "        'issues': all_issues\n",
    "    }\n",
    "\n",
    "@timer_decorator  \n",
    "def batch_fetch_issues(project_list: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Obtener issues para m√∫ltiples proyectos con control de rate limiting\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_projects = len(project_list)\n",
    "    total_issues = 0\n",
    "    \n",
    "    print_section_header(f\"üöÄ Extracci√≥n de issues para {total_projects} proyectos\")\n",
    "    \n",
    "    for i, project in enumerate(project_list, 1):\n",
    "        project_key = project['project_key']\n",
    "        print(f\"\\nüì¶ Proyecto {i}/{total_projects}: {project['nombre']} - {project['assignment']}\")\n",
    "        print(f\"üîÑ Procesando: {project_key}\")\n",
    "        \n",
    "        # Extraer issues del proyecto\n",
    "        result = fetch_project_issues(project)\n",
    "        \n",
    "        # Agregar informaci√≥n del estudiante al resultado\n",
    "        result.update({\n",
    "            'student_id': project['student_id'],\n",
    "            'nombre': project['nombre'],\n",
    "            'assignment': project['assignment'],\n",
    "            'row_index': project['row_index']\n",
    "        })\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # Mostrar resultado\n",
    "        if result['status'] == 'success':\n",
    "            issues_count = result['issues_count']\n",
    "            total_issues += issues_count\n",
    "            print(f\"    ‚úÖ {issues_count} issues extra√≠dos exitosamente\")\n",
    "        else:\n",
    "            print(f\"    ‚ùå Error: {result.get('status', 'unknown')}\")\n",
    "        \n",
    "        # Progress update\n",
    "        progress = i / total_projects * 100\n",
    "        print(f\"    üìä Progreso: {progress:.1f}% ({i}/{total_projects})\")\n",
    "        \n",
    "        # Delay entre proyectos para respetar rate limits\n",
    "        if i < total_projects:\n",
    "            print(f\"    ‚è≥ Esperando {ISSUES_CONFIG['batch_delay']}s...\")\n",
    "            time.sleep(ISSUES_CONFIG['batch_delay'])\n",
    "    \n",
    "    # Estad√≠sticas finales\n",
    "    successful = [r for r in results if r.get('status') == 'success']\n",
    "    print_section_header(\"üìà Estad√≠sticas de Extracci√≥n de Issues\")\n",
    "    print(f\"‚úÖ Proyectos exitosos: {len(successful)} ({format_percentage(len(successful), total_projects)})\")\n",
    "    print(f\"‚ùå Proyectos fallidos: {total_projects - len(successful)} ({format_percentage(total_projects - len(successful), total_projects)})\")\n",
    "    print(f\"üìä Total de issues extra√≠dos: {total_issues}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"üõ†Ô∏è  Funciones de extracci√≥n de issues configuradas\")\n",
    "print(\"üì° Listo para extraer issues detallados con paginaci√≥n autom√°tica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bc58ea",
   "metadata": {},
   "source": [
    "## 4. Ejecuci√≥n del Proceso de Extracci√≥n de Issues\n",
    "\n",
    "Ejecutamos el proceso de extracci√≥n de issues para todos los proyectos de estudiantes. Este proceso puede tomar considerablemente m√°s tiempo que la extracci√≥n de m√©tricas debido al volumen de datos por proyecto.\n",
    "\n",
    "### Configuraci√≥n del proceso:\n",
    "- **Paginaci√≥n**: 500 issues por p√°gina (m√°ximo de SonarCloud)\n",
    "- **Rate limiting**: 2.0 segundos entre proyectos\n",
    "- **Reintentos**: 3 intentos por proyecto en caso de errores\n",
    "- **Timeout**: 30 segundos por request\n",
    "\n",
    "### Monitoreo incluido:\n",
    "- ‚úÖ **Progreso por proyecto**: Estudiante, assignment y project key\n",
    "- üìÑ **Progreso por p√°gina**: N√∫mero de issues por p√°gina procesada\n",
    "- üìä **Contadores en tiempo real**: Issues extra√≠dos por proyecto\n",
    "- ‚è±Ô∏è **Tiempo de ejecuci√≥n**: Por proyecto y total\n",
    "- üîç **Detecci√≥n de errores**: Logging de errores espec√≠ficos\n",
    "\n",
    "> **Nota**: La extracci√≥n puede tomar varios minutos dependiendo de la cantidad de issues por proyecto. Proyectos con muchos issues requerir√°n m√∫ltiples p√°ginas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar extracci√≥n de issues\n",
    "print(\"üöÄ Iniciando proceso de extracci√≥n de issues detallados...\")\n",
    "print(f\"üìä Total de proyectos a procesar: {len(project_list)}\")\n",
    "print(f\"‚öôÔ∏è  Configuraci√≥n: {ISSUES_CONFIG['page_size']} issues por p√°gina, {ISSUES_CONFIG['batch_delay']}s entre proyectos\")\n",
    "\n",
    "# Ejecutar extracci√≥n\n",
    "issues_results = batch_fetch_issues(project_list)\n",
    "\n",
    "print(\"\\nüìà Resultados de la extracci√≥n de issues:\")\n",
    "print(f\"‚úÖ Total de proyectos procesados: {len(issues_results)}\")\n",
    "\n",
    "# Analizar resultados\n",
    "successful_extractions = [r for r in issues_results if r.get('status') == 'success']\n",
    "failed_extractions = [r for r in issues_results if r.get('status') != 'success']\n",
    "\n",
    "print(f\"‚úÖ Extracciones exitosas: {len(successful_extractions)}\")\n",
    "print(f\"‚ùå Extracciones fallidas: {len(failed_extractions)}\")\n",
    "\n",
    "# Calcular totales de issues\n",
    "total_issues = sum(r.get('issues_count', 0) for r in successful_extractions)\n",
    "print(f\"üìä Total de issues extra√≠dos: {total_issues}\")\n",
    "\n",
    "if failed_extractions:\n",
    "    print(\"\\n‚ö†Ô∏è  Proyectos con errores:\")\n",
    "    for failed in failed_extractions[:5]:  # Mostrar solo los primeros 5\n",
    "        print(f\"  - {failed['project_key']}: {failed.get('status', 'unknown error')}\")\n",
    "\n",
    "# Mostrar estad√≠sticas por assignment\n",
    "ap1_projects = [r for r in successful_extractions if r.get('assignment') == 'AP1']\n",
    "ap2_projects = [r for r in successful_extractions if r.get('assignment') == 'AP2']\n",
    "\n",
    "ap1_issues = sum(r.get('issues_count', 0) for r in ap1_projects)\n",
    "ap2_issues = sum(r.get('issues_count', 0) for r in ap2_projects)\n",
    "\n",
    "print(f\"\\nüìä Distribuci√≥n por assignment:\")\n",
    "print(f\"  AP1: {len(ap1_projects)} proyectos, {ap1_issues} issues\")\n",
    "print(f\"  AP2: {len(ap2_projects)} proyectos, {ap2_issues} issues\")\n",
    "\n",
    "# Mostrar ejemplo de issues extra√≠dos\n",
    "if successful_extractions and total_issues > 0:\n",
    "    # Buscar el primer proyecto con issues\n",
    "    example_project = None\n",
    "    for project in successful_extractions:\n",
    "        if project.get('issues_count', 0) > 0:\n",
    "            example_project = project\n",
    "            break\n",
    "    \n",
    "    if example_project:\n",
    "        print(f\"\\nüîç Ejemplo de issues extra√≠dos (proyecto: {example_project['project_key']}):\")\n",
    "        print(f\"Estudiante: {example_project['nombre']}\")\n",
    "        print(f\"Assignment: {example_project['assignment']}\")\n",
    "        print(f\"Total de issues: {example_project['issues_count']}\")\n",
    "        \n",
    "        # Mostrar primeros 3 issues\n",
    "        sample_issues = example_project['issues'][:3]\n",
    "        for i, issue in enumerate(sample_issues, 1):\n",
    "            print(f\"\\n  Issue {i}:\")\n",
    "            print(f\"    Tipo: {issue['type']}\")\n",
    "            print(f\"    Severidad: {issue['severity']}\")\n",
    "            print(f\"    Regla: {issue['rule']}\")\n",
    "            print(f\"    L√≠nea: {issue['line']}\")\n",
    "            print(f\"    Mensaje: {issue['message'][:100]}{'...' if len(issue['message']) > 100 else ''}\")\n",
    "\n",
    "print(\"\\n‚úÖ Extracci√≥n de issues completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc662b",
   "metadata": {},
   "source": [
    "## 5. Procesamiento y Estructuraci√≥n de Datos de Issues\n",
    "\n",
    "Convertimos los resultados de la extracci√≥n en DataFrames estructurados para facilitar el an√°lisis. A diferencia de las m√©tricas, los issues requieren un procesamiento m√°s complejo debido a su naturaleza detallada.\n",
    "\n",
    "### Procesamiento realizado:\n",
    "1. **Aplanamiento de datos**: Convertir estructura anidada en DataFrame plano\n",
    "2. **Limpieza de mensajes**: Preparar mensajes para formato CSV\n",
    "3. **Categorizaci√≥n**: Agrupar por tipo, severidad y regla\n",
    "4. **Validaci√≥n**: Verificar integridad y completitud de datos\n",
    "\n",
    "### DataFrames generados:\n",
    "- **`df_all_issues`**: Todos los issues individuales con informaci√≥n completa\n",
    "- **`df_issues_summary`**: Resumen agregado por proyecto\n",
    "- **`df_issues_by_type`**: An√°lisis por tipo de issue (BUG, VULNERABILITY, CODE_SMELL)\n",
    "- **`df_issues_by_severity`**: An√°lisis por severidad (CRITICAL, MAJOR, MINOR, etc.)\n",
    "\n",
    "### Variables de an√°lisis:\n",
    "- Total de issues por estudiante y assignment\n",
    "- Distribuci√≥n por categor√≠as de calidad\n",
    "- An√°lisis de reglas m√°s frecuentes\n",
    "- Patrones de ubicaci√≥n de problemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8a0e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar y estructurar los datos extra√≠dos de issues\n",
    "print(\"üîÑ Procesando datos de issues extra√≠dos...\")\n",
    "\n",
    "# 1. Crear DataFrame con todos los issues individuales\n",
    "all_issues_data = []\n",
    "\n",
    "for project_result in successful_extractions:\n",
    "    if project_result.get('issues'):\n",
    "        all_issues_data.extend(project_result['issues'])\n",
    "\n",
    "if all_issues_data:\n",
    "    df_all_issues = pd.DataFrame(all_issues_data)\n",
    "    print(f\"üìä DataFrame de issues creado con {len(df_all_issues)} issues individuales\")\n",
    "    \n",
    "    # Mostrar informaci√≥n b√°sica del DataFrame\n",
    "    print(f\"üìã Columnas disponibles: {len(df_all_issues.columns)}\")\n",
    "    print(f\"üë• Estudiantes √∫nicos: {df_all_issues['student_id'].nunique()}\")\n",
    "    print(f\"üéØ Proyectos √∫nicos: {df_all_issues['project_key'].nunique()}\")\n",
    "    \n",
    "    # 2. Crear resumen por proyecto\n",
    "    df_issues_summary = df_all_issues.groupby([\n",
    "        'student_id', 'nombre', 'project_key', 'assignment'\n",
    "    ]).agg({\n",
    "        'issue_key': 'count',\n",
    "        'type': lambda x: x.value_counts().to_dict(),\n",
    "        'severity': lambda x: x.value_counts().to_dict()\n",
    "    }).reset_index()\n",
    "    \n",
    "    df_issues_summary.columns = ['student_id', 'nombre', 'project_key', 'assignment', \n",
    "                                'total_issues', 'types_distribution', 'severity_distribution']\n",
    "    \n",
    "    print(f\"üìà Resumen por proyecto creado con {len(df_issues_summary)} proyectos\")\n",
    "    \n",
    "    # 3. An√°lisis por tipo de issue\n",
    "    type_analysis = df_all_issues.groupby(['assignment', 'type']).size().reset_index(name='count')\n",
    "    type_pivot = type_analysis.pivot(index='assignment', columns='type', values='count').fillna(0)\n",
    "    \n",
    "    print(f\"\\nüìä Distribuci√≥n por tipo de issue:\")\n",
    "    for assignment in ['AP1', 'AP2']:\n",
    "        if assignment in type_pivot.index:\n",
    "            print(f\"  {assignment}:\")\n",
    "            for issue_type in type_pivot.columns:\n",
    "                count = int(type_pivot.loc[assignment, issue_type])\n",
    "                if count > 0:\n",
    "                    print(f\"    {issue_type}: {count}\")\n",
    "    \n",
    "    # 4. An√°lisis por severidad\n",
    "    severity_analysis = df_all_issues.groupby(['assignment', 'severity']).size().reset_index(name='count')\n",
    "    severity_pivot = severity_analysis.pivot(index='assignment', columns='severity', values='count').fillna(0)\n",
    "    \n",
    "    print(f\"\\nüö® Distribuci√≥n por severidad:\")\n",
    "    for assignment in ['AP1', 'AP2']:\n",
    "        if assignment in severity_pivot.index:\n",
    "            print(f\"  {assignment}:\")\n",
    "            for severity in severity_pivot.columns:\n",
    "                count = int(severity_pivot.loc[assignment, severity])\n",
    "                if count > 0:\n",
    "                    print(f\"    {severity}: {count}\")\n",
    "    \n",
    "    # 5. Top 10 reglas m√°s frecuentes\n",
    "    top_rules = df_all_issues['rule'].value_counts().head(10)\n",
    "    print(f\"\\nüìã Top 10 reglas m√°s frecuentes:\")\n",
    "    for i, (rule, count) in enumerate(top_rules.items(), 1):\n",
    "        print(f\"  {i}. {rule}: {count} issues\")\n",
    "    \n",
    "    # 6. An√°lisis de estudiantes con m√°s issues\n",
    "    student_issues = df_all_issues.groupby(['student_id', 'nombre', 'assignment']).size().reset_index(name='issues_count')\n",
    "    top_students = student_issues.nlargest(10, 'issues_count')\n",
    "    \n",
    "    print(f\"\\nüë• Top 10 estudiantes con m√°s issues:\")\n",
    "    for _, student in top_students.iterrows():\n",
    "        print(f\"  {student['nombre']} ({student['assignment']}): {student['issues_count']} issues\")\n",
    "    \n",
    "    # 7. Mostrar muestra del DataFrame principal\n",
    "    print(f\"\\nüîç Primeras 3 filas del dataset de issues:\")\n",
    "    display(df_all_issues[['nombre', 'assignment', 'project_key', 'type', 'severity', 'rule', 'line', 'message']].head(3))\n",
    "    \n",
    "    # 8. Crear DataFrames espec√≠ficos para an√°lisis\n",
    "    df_issues_by_type = df_all_issues.groupby(['student_id', 'nombre', 'assignment', 'type']).size().reset_index(name='count')\n",
    "    df_issues_by_severity = df_all_issues.groupby(['student_id', 'nombre', 'assignment', 'severity']).size().reset_index(name='count')\n",
    "    \n",
    "    print(f\"\\n‚úÖ DataFrames de an√°lisis creados:\")\n",
    "    print(f\"  üìä Issues por tipo: {len(df_issues_by_type)} filas\")\n",
    "    print(f\"  üö® Issues por severidad: {len(df_issues_by_severity)} filas\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No se encontraron issues para procesar\")\n",
    "    df_all_issues = pd.DataFrame()\n",
    "    df_issues_summary = pd.DataFrame()\n",
    "\n",
    "print(\"‚úÖ Procesamiento de datos de issues completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b291abf",
   "metadata": {},
   "source": [
    "## 6. An√°lisis Detallado por Categor√≠as\n",
    "\n",
    "Esta secci√≥n proporciona an√°lisis profundos de los issues extra√≠dos organizados por diferentes categor√≠as:\n",
    "\n",
    "1. **An√°lisis por Tipo de Issue**: Distribuci√≥n de BUG, CODE_SMELL, VULNERABILITY, SECURITY_HOTSPOT\n",
    "2. **An√°lisis por Severidad**: Distribuci√≥n de CRITICAL, MAJOR, MINOR, INFO\n",
    "3. **An√°lisis por Regla**: Identificaci√≥n de las reglas m√°s violadas\n",
    "4. **An√°lisis Temporal**: Comparaci√≥n entre asignaciones (AP1 vs AP2)\n",
    "5. **An√°lisis Estad√≠stico**: M√©tricas descriptivas y correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76677b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis detallado por categor√≠as\n",
    "if not df_all_issues.empty:\n",
    "    print(\"üìä Iniciando an√°lisis detallado por categor√≠as...\")\n",
    "    \n",
    "    # 1. AN√ÅLISIS POR TIPO DE ISSUE\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìã AN√ÅLISIS POR TIPO DE ISSUE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Distribuci√≥n general por tipo\n",
    "    type_counts = df_all_issues['type'].value_counts()\n",
    "    total_issues = len(df_all_issues)\n",
    "    \n",
    "    print(f\"üìä Distribuci√≥n de tipos (Total: {total_issues} issues):\")\n",
    "    for issue_type, count in type_counts.items():\n",
    "        percentage = (count / total_issues) * 100\n",
    "        print(f\"  {issue_type}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # An√°lisis por asignaci√≥n\n",
    "    type_by_assignment = df_all_issues.groupby(['assignment', 'type']).size().unstack(fill_value=0)\n",
    "    print(f\"\\nüìà Distribuci√≥n por asignaci√≥n:\")\n",
    "    for assignment in type_by_assignment.index:\n",
    "        total_assignment = type_by_assignment.loc[assignment].sum()\n",
    "        print(f\"  {assignment} (Total: {total_assignment}):\")\n",
    "        for issue_type in type_by_assignment.columns:\n",
    "            count = type_by_assignment.loc[assignment, issue_type]\n",
    "            if count > 0:\n",
    "                percentage = (count / total_assignment) * 100\n",
    "                print(f\"    {issue_type}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # 2. AN√ÅLISIS POR SEVERIDAD\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üö® AN√ÅLISIS POR SEVERIDAD\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Distribuci√≥n general por severidad\n",
    "    severity_counts = df_all_issues['severity'].value_counts()\n",
    "    print(f\"üö® Distribuci√≥n de severidades:\")\n",
    "    for severity, count in severity_counts.items():\n",
    "        percentage = (count / total_issues) * 100\n",
    "        print(f\"  {severity}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # An√°lisis por asignaci√≥n\n",
    "    severity_by_assignment = df_all_issues.groupby(['assignment', 'severity']).size().unstack(fill_value=0)\n",
    "    print(f\"\\nüìà Distribuci√≥n por asignaci√≥n:\")\n",
    "    for assignment in severity_by_assignment.index:\n",
    "        total_assignment = severity_by_assignment.loc[assignment].sum()\n",
    "        print(f\"  {assignment} (Total: {total_assignment}):\")\n",
    "        for severity in severity_by_assignment.columns:\n",
    "            count = severity_by_assignment.loc[assignment, severity]\n",
    "            if count > 0:\n",
    "                percentage = (count / total_assignment) * 100\n",
    "                print(f\"    {severity}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # 3. AN√ÅLISIS POR REGLAS M√ÅS FRECUENTES\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìã AN√ÅLISIS POR REGLAS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Top 15 reglas m√°s frecuentes\n",
    "    top_rules = df_all_issues['rule'].value_counts().head(15)\n",
    "    print(f\"üìã Top 15 reglas m√°s violadas:\")\n",
    "    for i, (rule, count) in enumerate(top_rules.items(), 1):\n",
    "        percentage = (count / total_issues) * 100\n",
    "        print(f\"  {i:2d}. {rule}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # An√°lisis de reglas por tipo de issue\n",
    "    print(f\"\\nüîç Distribuci√≥n de reglas por tipo:\")\n",
    "    rules_by_type = df_all_issues.groupby(['type', 'rule']).size().reset_index(name='count')\n",
    "    for issue_type in df_all_issues['type'].unique():\n",
    "        top_rules_type = rules_by_type[rules_by_type['type'] == issue_type].nlargest(5, 'count')\n",
    "        print(f\"  {issue_type}:\")\n",
    "        for _, row in top_rules_type.iterrows():\n",
    "            print(f\"    {row['rule']}: {row['count']}\")\n",
    "    \n",
    "    # 4. AN√ÅLISIS ESTAD√çSTICO DESCRIPTIVO\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä AN√ÅLISIS ESTAD√çSTICO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Issues por estudiante\n",
    "    issues_per_student = df_all_issues.groupby(['student_id', 'nombre', 'assignment']).size().reset_index(name='issues_count')\n",
    "    \n",
    "    print(f\"üë• Estad√≠sticas de issues por estudiante:\")\n",
    "    for assignment in ['AP1', 'AP2']:\n",
    "        assignment_data = issues_per_student[issues_per_student['assignment'] == assignment]['issues_count']\n",
    "        if not assignment_data.empty:\n",
    "            print(f\"  {assignment}:\")\n",
    "            print(f\"    Promedio: {assignment_data.mean():.1f}\")\n",
    "            print(f\"    Mediana: {assignment_data.median():.1f}\")\n",
    "            print(f\"    Desv. Est√°ndar: {assignment_data.std():.1f}\")\n",
    "            print(f\"    M√≠nimo: {assignment_data.min()}\")\n",
    "            print(f\"    M√°ximo: {assignment_data.max()}\")\n",
    "    \n",
    "    # Issues por proyecto\n",
    "    issues_per_project = df_all_issues.groupby(['project_key', 'assignment']).size().reset_index(name='issues_count')\n",
    "    \n",
    "    print(f\"\\nüìÅ Estad√≠sticas de issues por proyecto:\")\n",
    "    for assignment in ['AP1', 'AP2']:\n",
    "        assignment_data = issues_per_project[issues_per_project['assignment'] == assignment]['issues_count']\n",
    "        if not assignment_data.empty:\n",
    "            print(f\"  {assignment}:\")\n",
    "            print(f\"    Promedio: {assignment_data.mean():.1f}\")\n",
    "            print(f\"    Mediana: {assignment_data.median():.1f}\")\n",
    "            print(f\"    Proyectos totales: {len(assignment_data)}\")\n",
    "    \n",
    "    # 5. COMPARACI√ìN ENTRE ASIGNACIONES\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîÑ COMPARACI√ìN AP1 vs AP2\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    ap1_issues = df_all_issues[df_all_issues['assignment'] == 'AP1']\n",
    "    ap2_issues = df_all_issues[df_all_issues['assignment'] == 'AP2']\n",
    "    \n",
    "    print(f\"üìä Resumen comparativo:\")\n",
    "    print(f\"  AP1: {len(ap1_issues)} issues en {ap1_issues['project_key'].nunique()} proyectos\")\n",
    "    print(f\"  AP2: {len(ap2_issues)} issues en {ap2_issues['project_key'].nunique()} proyectos\")\n",
    "    \n",
    "    if len(ap1_issues) > 0 and len(ap2_issues) > 0:\n",
    "        print(f\"  Promedio issues/proyecto:\")\n",
    "        print(f\"    AP1: {len(ap1_issues) / ap1_issues['project_key'].nunique():.1f}\")\n",
    "        print(f\"    AP2: {len(ap2_issues) / ap2_issues['project_key'].nunique():.1f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ An√°lisis detallado completado\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No hay datos de issues disponibles para el an√°lisis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b840ad",
   "metadata": {},
   "source": [
    "## 7. Exportar Datos de Issues\n",
    "\n",
    "Esta secci√≥n se encarga de exportar los datos extra√≠dos y procesados a archivos CSV para su uso posterior:\n",
    "\n",
    "1. **Issues Individuales**: Archivo con todos los issues detallados\n",
    "2. **Resumen por Proyecto**: Consolidado con conteos y distribuciones\n",
    "3. **An√°lisis por Tipo**: Distribuci√≥n de tipos de issues por estudiante/proyecto\n",
    "4. **An√°lisis por Severidad**: Distribuci√≥n de severidades por estudiante/proyecto\n",
    "5. **Metadatos de Extracci√≥n**: Informaci√≥n sobre el proceso de extracci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e90057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar datos de issues a archivos CSV\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Crear directorio data si no existe\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "# Timestamp para archivos\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "if not df_all_issues.empty:\n",
    "    print(\"üìÅ Exportando datos de issues...\")\n",
    "    \n",
    "    try:\n",
    "        # 1. EXPORTAR ISSUES INDIVIDUALES COMPLETOS\n",
    "        issues_filename = f'../data/issues_detallados_{timestamp}.csv'\n",
    "        df_all_issues.to_csv(issues_filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"‚úÖ Issues individuales exportados: {issues_filename}\")\n",
    "        print(f\"   üìä {len(df_all_issues)} issues de {df_all_issues['student_id'].nunique()} estudiantes\")\n",
    "        \n",
    "        # 2. EXPORTAR RESUMEN POR PROYECTO\n",
    "        if not df_issues_summary.empty:\n",
    "            summary_filename = f'../data/issues_resumen_proyecto_{timestamp}.csv'\n",
    "            df_issues_summary.to_csv(summary_filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\"‚úÖ Resumen por proyecto exportado: {summary_filename}\")\n",
    "            print(f\"   üìä {len(df_issues_summary)} proyectos\")\n",
    "        \n",
    "        # 3. EXPORTAR AN√ÅLISIS POR TIPO\n",
    "        if 'df_issues_by_type' in locals():\n",
    "            type_filename = f'../data/issues_por_tipo_{timestamp}.csv'\n",
    "            df_issues_by_type.to_csv(type_filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\"‚úÖ An√°lisis por tipo exportado: {type_filename}\")\n",
    "            print(f\"   üìä {len(df_issues_by_type)} registros\")\n",
    "        \n",
    "        # 4. EXPORTAR AN√ÅLISIS POR SEVERIDAD\n",
    "        if 'df_issues_by_severity' in locals():\n",
    "            severity_filename = f'../data/issues_por_severidad_{timestamp}.csv'\n",
    "            df_issues_by_severity.to_csv(severity_filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\"‚úÖ An√°lisis por severidad exportado: {severity_filename}\")\n",
    "            print(f\"   üìä {len(df_issues_by_severity)} registros\")\n",
    "        \n",
    "        # 5. CREAR ARCHIVO DE ESTAD√çSTICAS RESUMIDAS\n",
    "        stats_data = []\n",
    "        \n",
    "        # Estad√≠sticas generales\n",
    "        stats_data.append({\n",
    "            'categoria': 'General',\n",
    "            'metrica': 'Total Issues',\n",
    "            'valor': len(df_all_issues),\n",
    "            'descripcion': 'N√∫mero total de issues extra√≠dos'\n",
    "        })\n",
    "        \n",
    "        stats_data.append({\n",
    "            'categoria': 'General',\n",
    "            'metrica': 'Estudiantes √önicos',\n",
    "            'valor': df_all_issues['student_id'].nunique(),\n",
    "            'descripcion': 'N√∫mero de estudiantes con issues'\n",
    "        })\n",
    "        \n",
    "        stats_data.append({\n",
    "            'categoria': 'General',\n",
    "            'metrica': 'Proyectos √önicos',\n",
    "            'valor': df_all_issues['project_key'].nunique(),\n",
    "            'descripcion': 'N√∫mero de proyectos con issues'\n",
    "        })\n",
    "        \n",
    "        # Estad√≠sticas por asignaci√≥n\n",
    "        for assignment in ['AP1', 'AP2']:\n",
    "            assignment_issues = df_all_issues[df_all_issues['assignment'] == assignment]\n",
    "            if not assignment_issues.empty:\n",
    "                stats_data.append({\n",
    "                    'categoria': assignment,\n",
    "                    'metrica': 'Total Issues',\n",
    "                    'valor': len(assignment_issues),\n",
    "                    'descripcion': f'Issues en {assignment}'\n",
    "                })\n",
    "                \n",
    "                stats_data.append({\n",
    "                    'categoria': assignment,\n",
    "                    'metrica': 'Proyectos',\n",
    "                    'valor': assignment_issues['project_key'].nunique(),\n",
    "                    'descripcion': f'Proyectos con issues en {assignment}'\n",
    "                })\n",
    "                \n",
    "                stats_data.append({\n",
    "                    'categoria': assignment,\n",
    "                    'metrica': 'Promedio Issues/Proyecto',\n",
    "                    'valor': round(len(assignment_issues) / assignment_issues['project_key'].nunique(), 2),\n",
    "                    'descripcion': f'Promedio de issues por proyecto en {assignment}'\n",
    "                })\n",
    "        \n",
    "        # Estad√≠sticas por tipo de issue\n",
    "        type_counts = df_all_issues['type'].value_counts()\n",
    "        for issue_type, count in type_counts.items():\n",
    "            stats_data.append({\n",
    "                'categoria': 'Tipos',\n",
    "                'metrica': issue_type,\n",
    "                'valor': count,\n",
    "                'descripcion': f'Issues de tipo {issue_type}'\n",
    "            })\n",
    "        \n",
    "        # Estad√≠sticas por severidad\n",
    "        severity_counts = df_all_issues['severity'].value_counts()\n",
    "        for severity, count in severity_counts.items():\n",
    "            stats_data.append({\n",
    "                'categoria': 'Severidad',\n",
    "                'metrica': severity,\n",
    "                'valor': count,\n",
    "                'descripcion': f'Issues con severidad {severity}'\n",
    "            })\n",
    "        \n",
    "        # Exportar estad√≠sticas\n",
    "        df_stats = pd.DataFrame(stats_data)\n",
    "        stats_filename = f'../data/issues_estadisticas_{timestamp}.csv'\n",
    "        df_stats.to_csv(stats_filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"‚úÖ Estad√≠sticas exportadas: {stats_filename}\")\n",
    "        print(f\"   üìä {len(df_stats)} m√©tricas estad√≠sticas\")\n",
    "        \n",
    "        # 6. CREAR ARCHIVO DE METADATOS\n",
    "        metadata = {\n",
    "            'fecha_extraccion': datetime.now().isoformat(),\n",
    "            'total_estudiantes_procesados': len(df_estudiantes) if 'df_estudiantes' in locals() else 0,\n",
    "            'total_proyectos_intentados': len(successful_extractions) + len(failed_extractions) if 'successful_extractions' in locals() and 'failed_extractions' in locals() else 0,\n",
    "            'total_proyectos_exitosos': len(successful_extractions) if 'successful_extractions' in locals() else 0,\n",
    "            'total_proyectos_fallidos': len(failed_extractions) if 'failed_extractions' in locals() else 0,\n",
    "            'total_issues_extraidos': len(df_all_issues),\n",
    "            'estudiantes_con_issues': df_all_issues['student_id'].nunique(),\n",
    "            'proyectos_con_issues': df_all_issues['project_key'].nunique(),\n",
    "            'archivos_generados': [\n",
    "                issues_filename,\n",
    "                summary_filename if not df_issues_summary.empty else None,\n",
    "                type_filename if 'df_issues_by_type' in locals() else None,\n",
    "                severity_filename if 'df_issues_by_severity' in locals() else None,\n",
    "                stats_filename\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Filtrar archivos None\n",
    "        metadata['archivos_generados'] = [f for f in metadata['archivos_generados'] if f is not None]\n",
    "        \n",
    "        metadata_filename = f'../data/issues_metadata_{timestamp}.json'\n",
    "        import json\n",
    "        with open(metadata_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"‚úÖ Metadatos exportados: {metadata_filename}\")\n",
    "        \n",
    "        # RESUMEN FINAL\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"üìã RESUMEN DE EXPORTACI√ìN\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"üïí Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"üìä Issues exportados: {len(df_all_issues)}\")\n",
    "        print(f\"üë• Estudiantes: {df_all_issues['student_id'].nunique()}\")\n",
    "        print(f\"üìÅ Proyectos: {df_all_issues['project_key'].nunique()}\")\n",
    "        print(f\"üìÑ Archivos generados: {len(metadata['archivos_generados'])}\")\n",
    "        print(f\"\\nüìÅ Archivos guardados en: ../data/\")\n",
    "        for archivo in metadata['archivos_generados']:\n",
    "            print(f\"   ‚Ä¢ {os.path.basename(archivo)}\")\n",
    "        \n",
    "        # Tambi√©n crear una versi√≥n simplificada sin timestamp para facilitar el acceso\n",
    "        main_filename = '../data/issues_detallados_latest.csv'\n",
    "        df_all_issues.to_csv(main_filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n‚úÖ Archivo principal actualizado: {main_filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error durante la exportaci√≥n: {str(e)}\")\n",
    "        logging.error(f\"Error en exportaci√≥n: {str(e)}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No hay datos de issues para exportar\")\n",
    "\n",
    "print(\"\\nüéâ Proceso de extracci√≥n de issues completado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ea57c",
   "metadata": {},
   "source": [
    "## üìã Resumen del Notebook\n",
    "\n",
    "Este notebook ha completado exitosamente la extracci√≥n y an√°lisis de issues de SonarCloud para los proyectos de estudiantes. \n",
    "\n",
    "### ‚úÖ Procesos Realizados:\n",
    "\n",
    "1. **Configuraci√≥n**: Setup de APIs y credenciales de SonarCloud\n",
    "2. **Carga de Datos**: Lectura de informaci√≥n de estudiantes y proyectos\n",
    "3. **Extracci√≥n**: Obtenci√≥n de issues detallados usando la API de SonarCloud\n",
    "4. **Procesamiento**: Estructuraci√≥n y an√°lisis de los datos extra√≠dos\n",
    "5. **An√°lisis**: Categorizaci√≥n por tipo, severidad, reglas y comparaciones\n",
    "6. **Exportaci√≥n**: Generaci√≥n de m√∫ltiples archivos CSV para an√°lisis posterior\n",
    "\n",
    "### üìä Datos Generados:\n",
    "\n",
    "- **Issues Detallados**: Cada issue individual con toda su informaci√≥n\n",
    "- **Resumen por Proyecto**: Consolidaci√≥n de issues por proyecto y estudiante  \n",
    "- **An√°lisis por Categor√≠as**: Distribuciones por tipo y severidad\n",
    "- **Estad√≠sticas**: M√©tricas descriptivas y comparativas\n",
    "- **Metadatos**: Informaci√≥n del proceso de extracci√≥n\n",
    "\n",
    "### üîÑ Pr√≥ximos Pasos:\n",
    "\n",
    "1. Utilizar los archivos CSV generados para an√°lisis estad√≠sticos avanzados\n",
    "2. Combinar con datos de m√©tricas de calidad para an√°lisis integral\n",
    "3. Generar visualizaciones y reportes de calidad de software\n",
    "4. Identificar patrones y tendencias en los tipos de issues m√°s comunes\n",
    "\n",
    "---\n",
    "*Notebook creado para el an√°lisis de calidad de software en proyectos estudiantiles - Tesis Aplicada 2*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
